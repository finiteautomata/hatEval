{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM  + ElMO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instancias de entrenamiento: 4500\n",
      "Instancias de desarrollo: 500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(2019)\n",
    "np.random.seed(2019)\n",
    "tf.random.set_random_seed(2019)\n",
    "random.seed(2019)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "df_dev = pd.read_table(\"../../../data/es/dev_es.tsv\", index_col=\"id\", quoting=csv.QUOTE_NONE)\n",
    "df_train = pd.read_table(\"../../../data/es/train_es.tsv\", index_col=\"id\", quoting=csv.QUOTE_NONE)\n",
    "df_test = pd.read_table(\"../../../data/es/reference_es.tsv\", header=None, \n",
    "                        names=[\"text\", \"HS\", \"TR\", \"AG\"], quoting=csv.QUOTE_NONE)\n",
    "\n",
    "\n",
    "text_train, y_train = df_train[\"text\"], df_train[\"HS\"]\n",
    "text_dev, y_dev = df_dev[\"text\"], df_dev[\"HS\"]\n",
    "text_test, y_test = df_test[\"text\"], df_test[\"HS\"]\n",
    "\n",
    "print(\"Instancias de entrenamiento: {}\".format(len(df_train)))\n",
    "print(\"Instancias de desarrollo: {}\".format(len(df_dev)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tengo que hacer dos cosas:\n",
    "\n",
    "- Primero, convertir los tweets a secuencias de texto\n",
    "- Luego, paddear las secuencias a cierta longitud (Keras necesita esto para poder paralelizar c√°lculo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "max_length = 30\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    if len(tokens) >= max_length:\n",
    "        tokens = tokens[:max_length]\n",
    "    else:\n",
    "        tokens = tokens + [''] * (max_length - len(tokens))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "text_train = [preprocess_tweet(tweet) for tweet in df_train[\"text\"].values]\n",
    "text_dev = [preprocess_tweet(tweet) for tweet in df_dev[\"text\"].values]\n",
    "text_test = [preprocess_tweet(tweet) for tweet in df_test[\"text\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from elmoformanylangs import Embedder\n",
    "\n",
    "e = Embedder(\"../../../models/elmo/es/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carguemos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['easyjet', 'quiere', 'duplicar', 'el', 'n√∫mero', 'de', 'mujeres', 'piloto', \"'\", 'ver√°s', 't√∫', 'para', 'aparcar', 'el', 'avi√≥n', '..', 'http://t.co/46NuLkm09x', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(text_train[0])\n",
    "\n",
    "\n",
    "X_train = np.array(e.sents2elmo(text_train))\n",
    "X_dev = np.array(e.sents2elmo(text_dev))\n",
    "X_test = np.array(e.sents2elmo(text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4500, 30, 1024), (500, 30, 1024), (1600, 30, 1024), (4500,), (500,), (1600,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_dev.shape, X_test.shape, y_train.shape, y_dev.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 256)               1312768   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,378,817\n",
      "Trainable params: 1,378,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/28\n",
      "4500/4500 [==============================] - 3s 703us/step - loss: 0.7262 - acc: 0.5442 - val_loss: 0.6815 - val_acc: 0.5560\n",
      "Epoch 2/28\n",
      "4500/4500 [==============================] - 2s 363us/step - loss: 0.6861 - acc: 0.5678 - val_loss: 0.6780 - val_acc: 0.5600\n",
      "Epoch 3/28\n",
      "4500/4500 [==============================] - 2s 371us/step - loss: 0.6734 - acc: 0.5876 - val_loss: 0.6696 - val_acc: 0.5800\n",
      "Epoch 4/28\n",
      "4500/4500 [==============================] - 2s 372us/step - loss: 0.6680 - acc: 0.5947 - val_loss: 0.6671 - val_acc: 0.5940\n",
      "Epoch 5/28\n",
      "4500/4500 [==============================] - 2s 368us/step - loss: 0.6561 - acc: 0.6189 - val_loss: 0.6590 - val_acc: 0.6080\n",
      "Epoch 6/28\n",
      "4500/4500 [==============================] - 2s 368us/step - loss: 0.6524 - acc: 0.6200 - val_loss: 0.6512 - val_acc: 0.6280\n",
      "Epoch 7/28\n",
      "4500/4500 [==============================] - 2s 367us/step - loss: 0.6305 - acc: 0.6529 - val_loss: 0.6268 - val_acc: 0.6400\n",
      "Epoch 8/28\n",
      "4500/4500 [==============================] - 2s 364us/step - loss: 0.6140 - acc: 0.6744 - val_loss: 0.6018 - val_acc: 0.6860\n",
      "Epoch 9/28\n",
      "4500/4500 [==============================] - 2s 366us/step - loss: 0.5860 - acc: 0.6933 - val_loss: 0.5775 - val_acc: 0.7200\n",
      "Epoch 10/28\n",
      "4500/4500 [==============================] - 2s 365us/step - loss: 0.5687 - acc: 0.7178 - val_loss: 0.5596 - val_acc: 0.7220\n",
      "Epoch 11/28\n",
      "4500/4500 [==============================] - 2s 367us/step - loss: 0.5444 - acc: 0.7356 - val_loss: 0.5660 - val_acc: 0.7040\n",
      "Epoch 12/28\n",
      "4500/4500 [==============================] - 2s 366us/step - loss: 0.5307 - acc: 0.7507 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 13/28\n",
      "4500/4500 [==============================] - 2s 366us/step - loss: 0.5110 - acc: 0.7576 - val_loss: 0.5090 - val_acc: 0.7680\n",
      "Epoch 14/28\n",
      "4500/4500 [==============================] - 2s 367us/step - loss: 0.4996 - acc: 0.7671 - val_loss: 0.4983 - val_acc: 0.7720\n",
      "Epoch 15/28\n",
      "4500/4500 [==============================] - 2s 365us/step - loss: 0.4841 - acc: 0.7778 - val_loss: 0.4886 - val_acc: 0.7720\n",
      "Epoch 16/28\n",
      "4500/4500 [==============================] - 2s 366us/step - loss: 0.4770 - acc: 0.7889 - val_loss: 0.4857 - val_acc: 0.7700\n",
      "Epoch 17/28\n",
      "4500/4500 [==============================] - 2s 368us/step - loss: 0.4520 - acc: 0.8016 - val_loss: 0.4852 - val_acc: 0.7620\n",
      "Epoch 18/28\n",
      "4500/4500 [==============================] - 2s 365us/step - loss: 0.4486 - acc: 0.8027 - val_loss: 0.4712 - val_acc: 0.7760\n",
      "Epoch 19/28\n",
      "4500/4500 [==============================] - 2s 368us/step - loss: 0.4270 - acc: 0.8216 - val_loss: 0.4653 - val_acc: 0.7800\n",
      "Epoch 20/28\n",
      "4500/4500 [==============================] - 2s 371us/step - loss: 0.4214 - acc: 0.8200 - val_loss: 0.4611 - val_acc: 0.7860\n",
      "Epoch 21/28\n",
      "4500/4500 [==============================] - 2s 366us/step - loss: 0.4121 - acc: 0.8249 - val_loss: 0.4632 - val_acc: 0.7820\n",
      "Epoch 22/28\n",
      "4500/4500 [==============================] - 2s 362us/step - loss: 0.4034 - acc: 0.8289 - val_loss: 0.4608 - val_acc: 0.7840\n",
      "Epoch 23/28\n",
      "4500/4500 [==============================] - 2s 362us/step - loss: 0.3948 - acc: 0.8371 - val_loss: 0.4585 - val_acc: 0.7820\n",
      "Epoch 24/28\n",
      "4500/4500 [==============================] - 2s 369us/step - loss: 0.3847 - acc: 0.8427 - val_loss: 0.4591 - val_acc: 0.7960\n",
      "Epoch 25/28\n",
      "4500/4500 [==============================] - 2s 368us/step - loss: 0.3744 - acc: 0.8482 - val_loss: 0.4565 - val_acc: 0.7900\n",
      "Epoch 26/28\n",
      "4500/4500 [==============================] - 2s 364us/step - loss: 0.3619 - acc: 0.8567 - val_loss: 0.4570 - val_acc: 0.7920\n",
      "Epoch 27/28\n",
      "4500/4500 [==============================] - 2s 368us/step - loss: 0.3601 - acc: 0.8591 - val_loss: 0.4637 - val_acc: 0.7840\n",
      "Epoch 28/28\n",
      "4500/4500 [==============================] - 2s 364us/step - loss: 0.3519 - acc: 0.8609 - val_loss: 0.4577 - val_acc: 0.7880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1c442acda0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, CuDNNLSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "embedding_dim = 1024\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.0005,\n",
    "    \"decay\": 0.01\n",
    "}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(256, input_shape=(max_length, embedding_dim)))\n",
    "model.add(Dropout(0.75))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=28, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluaci√≥n sobre dev\n",
      "500/500 [==============================] - 0s 182us/step\n",
      "Loss           : 0.4577\n",
      "Accuracy       : 0.7880\n",
      "Precision(1)   : 0.7843\n",
      "Precision(1)   : 0.7905\n",
      "Precision(avg) : 0.7874\n",
      "\n",
      "Recall(1)      : 0.7207\n",
      "Recall(0)      : 0.8417\n",
      "Recall(avg)    : 0.7812\n",
      "\n",
      "F1(1)          : 0.7512\n",
      "F1(0)          : 0.8153\n",
      "F1(avg)        : 0.7833\n",
      "\n",
      "\n",
      "Evaluaci√≥n sobre test\n",
      "1600/1600 [==============================] - 0s 170us/step\n",
      "Loss           : 0.6080\n",
      "Accuracy       : 0.7181\n",
      "Precision(1)   : 0.6530\n",
      "Precision(1)   : 0.7666\n",
      "Precision(avg) : 0.7098\n",
      "\n",
      "Recall(1)      : 0.6758\n",
      "Recall(0)      : 0.7479\n",
      "Recall(avg)    : 0.7118\n",
      "\n",
      "F1(1)          : 0.6642\n",
      "F1(0)          : 0.7571\n",
      "F1(avg)        : 0.7107\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "\n",
    "print(\"Evaluaci√≥n sobre dev\")\n",
    "print_evaluation(model, X_dev, y_dev)\n",
    "print(\"\\n\\nEvaluaci√≥n sobre test\")\n",
    "print_evaluation(model, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/35\n",
      "4500/4500 [==============================] - 3s 580us/step - loss: 0.8301 - acc: 0.5471 - val_loss: 0.6550 - val_acc: 0.6340\n",
      "Epoch 2/35\n",
      "4500/4500 [==============================] - 2s 435us/step - loss: 0.7146 - acc: 0.5949 - val_loss: 0.6507 - val_acc: 0.6340\n",
      "Epoch 3/35\n",
      "4500/4500 [==============================] - 2s 440us/step - loss: 0.6782 - acc: 0.6193 - val_loss: 0.6403 - val_acc: 0.6660\n",
      "Epoch 4/35\n",
      "4500/4500 [==============================] - 2s 441us/step - loss: 0.6513 - acc: 0.6389 - val_loss: 0.6284 - val_acc: 0.6620\n",
      "Epoch 5/35\n",
      "4500/4500 [==============================] - 2s 443us/step - loss: 0.6284 - acc: 0.6562 - val_loss: 0.6175 - val_acc: 0.6640\n",
      "Epoch 6/35\n",
      "4500/4500 [==============================] - 2s 439us/step - loss: 0.5995 - acc: 0.6847 - val_loss: 0.6022 - val_acc: 0.6740\n",
      "Epoch 7/35\n",
      "4500/4500 [==============================] - 2s 440us/step - loss: 0.5982 - acc: 0.6853 - val_loss: 0.5884 - val_acc: 0.6940\n",
      "Epoch 8/35\n",
      "4500/4500 [==============================] - 2s 440us/step - loss: 0.5656 - acc: 0.7111 - val_loss: 0.5688 - val_acc: 0.7240\n",
      "Epoch 9/35\n",
      "4500/4500 [==============================] - 2s 445us/step - loss: 0.5610 - acc: 0.7136 - val_loss: 0.5496 - val_acc: 0.7420\n",
      "Epoch 10/35\n",
      "4500/4500 [==============================] - 2s 444us/step - loss: 0.5387 - acc: 0.7349 - val_loss: 0.5329 - val_acc: 0.7440\n",
      "Epoch 11/35\n",
      "4500/4500 [==============================] - 2s 447us/step - loss: 0.5247 - acc: 0.7447 - val_loss: 0.5227 - val_acc: 0.7500\n",
      "Epoch 12/35\n",
      "4500/4500 [==============================] - 2s 443us/step - loss: 0.5117 - acc: 0.7562 - val_loss: 0.5095 - val_acc: 0.7640\n",
      "Epoch 13/35\n",
      "4500/4500 [==============================] - 2s 442us/step - loss: 0.4976 - acc: 0.7669 - val_loss: 0.5044 - val_acc: 0.7640\n",
      "Epoch 14/35\n",
      "4500/4500 [==============================] - 2s 443us/step - loss: 0.4917 - acc: 0.7724 - val_loss: 0.4944 - val_acc: 0.7740\n",
      "Epoch 15/35\n",
      "4500/4500 [==============================] - 2s 445us/step - loss: 0.4720 - acc: 0.7793 - val_loss: 0.4924 - val_acc: 0.7860\n",
      "Epoch 16/35\n",
      "4500/4500 [==============================] - 2s 439us/step - loss: 0.4736 - acc: 0.7838 - val_loss: 0.4797 - val_acc: 0.7760\n",
      "Epoch 17/35\n",
      "4500/4500 [==============================] - 2s 436us/step - loss: 0.4676 - acc: 0.7876 - val_loss: 0.4795 - val_acc: 0.7800\n",
      "Epoch 18/35\n",
      "4500/4500 [==============================] - 2s 445us/step - loss: 0.4516 - acc: 0.7940 - val_loss: 0.4721 - val_acc: 0.7800\n",
      "Epoch 19/35\n",
      "4500/4500 [==============================] - 2s 441us/step - loss: 0.4504 - acc: 0.7896 - val_loss: 0.4680 - val_acc: 0.7780\n",
      "Epoch 20/35\n",
      "4500/4500 [==============================] - 2s 435us/step - loss: 0.4387 - acc: 0.8018 - val_loss: 0.4626 - val_acc: 0.7780\n",
      "Epoch 21/35\n",
      "4500/4500 [==============================] - 2s 444us/step - loss: 0.4341 - acc: 0.8053 - val_loss: 0.4569 - val_acc: 0.7900\n",
      "Epoch 22/35\n",
      "4500/4500 [==============================] - 2s 435us/step - loss: 0.4373 - acc: 0.8036 - val_loss: 0.4566 - val_acc: 0.7900\n",
      "Epoch 23/35\n",
      "4500/4500 [==============================] - 2s 442us/step - loss: 0.4244 - acc: 0.8129 - val_loss: 0.4511 - val_acc: 0.7980\n",
      "Epoch 24/35\n",
      "4500/4500 [==============================] - 2s 441us/step - loss: 0.4091 - acc: 0.8187 - val_loss: 0.4487 - val_acc: 0.7940\n",
      "Epoch 25/35\n",
      "4500/4500 [==============================] - 2s 442us/step - loss: 0.4161 - acc: 0.8131 - val_loss: 0.4488 - val_acc: 0.8000\n",
      "Epoch 26/35\n",
      "4500/4500 [==============================] - 2s 442us/step - loss: 0.4097 - acc: 0.8158 - val_loss: 0.4495 - val_acc: 0.7880\n",
      "Epoch 27/35\n",
      "4500/4500 [==============================] - 2s 444us/step - loss: 0.4015 - acc: 0.8204 - val_loss: 0.4431 - val_acc: 0.7980\n",
      "Epoch 28/35\n",
      "4500/4500 [==============================] - 2s 442us/step - loss: 0.3999 - acc: 0.8182 - val_loss: 0.4440 - val_acc: 0.7980\n",
      "Epoch 29/35\n",
      "4500/4500 [==============================] - 2s 439us/step - loss: 0.3994 - acc: 0.8233 - val_loss: 0.4464 - val_acc: 0.7980\n",
      "Epoch 30/35\n",
      "4500/4500 [==============================] - 2s 441us/step - loss: 0.3864 - acc: 0.8273 - val_loss: 0.4396 - val_acc: 0.8060\n",
      "Epoch 31/35\n",
      "4500/4500 [==============================] - 2s 445us/step - loss: 0.3885 - acc: 0.8298 - val_loss: 0.4412 - val_acc: 0.7900\n",
      "Epoch 32/35\n",
      "4500/4500 [==============================] - 2s 440us/step - loss: 0.3800 - acc: 0.8373 - val_loss: 0.4398 - val_acc: 0.7960\n",
      "Epoch 33/35\n",
      "4500/4500 [==============================] - 2s 444us/step - loss: 0.3846 - acc: 0.8322 - val_loss: 0.4399 - val_acc: 0.7940\n",
      "Epoch 34/35\n",
      "4500/4500 [==============================] - 2s 448us/step - loss: 0.3821 - acc: 0.8347 - val_loss: 0.4376 - val_acc: 0.7960\n",
      "Epoch 35/35\n",
      "4500/4500 [==============================] - 2s 445us/step - loss: 0.3729 - acc: 0.8382 - val_loss: 0.4385 - val_acc: 0.8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1ab0477898>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import CuDNNGRU, Dropout, LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.0005,\n",
    "    \"decay\": 0.01\n",
    "}\n",
    "\n",
    "embedding_dim = 1024\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(CuDNNGRU(256), input_shape=(max_length, embedding_dim)))\n",
    "model.add(Dropout(0.80))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.55))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train.values, validation_data=(X_dev, y_dev), epochs=35, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Lambda, Flatten\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.003,\n",
    "    \"decay\": 0.00\n",
    "}\n",
    "\n",
    "embedding_dim = 1024\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(AveragePooling1D(pool_size=max_length, input_shape=(max_length, embedding_dim)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      "4500/4500 [==============================] - 3s 717us/step - loss: 0.6476 - acc: 0.6289 - val_loss: 0.6332 - val_acc: 0.6220\n",
      "Epoch 2/100\n",
      "4500/4500 [==============================] - 1s 207us/step - loss: 0.6111 - acc: 0.6740 - val_loss: 0.5975 - val_acc: 0.6820\n",
      "Epoch 3/100\n",
      "4500/4500 [==============================] - 1s 207us/step - loss: 0.6019 - acc: 0.6707 - val_loss: 0.5859 - val_acc: 0.6880\n",
      "Epoch 4/100\n",
      "4500/4500 [==============================] - 1s 207us/step - loss: 0.5935 - acc: 0.6818 - val_loss: 0.5699 - val_acc: 0.7000\n",
      "Epoch 5/100\n",
      "4500/4500 [==============================] - 1s 210us/step - loss: 0.5790 - acc: 0.6962 - val_loss: 0.5636 - val_acc: 0.7140\n",
      "Epoch 6/100\n",
      "4500/4500 [==============================] - 1s 211us/step - loss: 0.5785 - acc: 0.6936 - val_loss: 0.5659 - val_acc: 0.7080\n",
      "Epoch 7/100\n",
      "4500/4500 [==============================] - 1s 208us/step - loss: 0.5751 - acc: 0.6989 - val_loss: 0.5578 - val_acc: 0.7140\n",
      "Epoch 8/100\n",
      "4500/4500 [==============================] - 1s 212us/step - loss: 0.5737 - acc: 0.7080 - val_loss: 0.5706 - val_acc: 0.6900\n",
      "Epoch 9/100\n",
      "4500/4500 [==============================] - 1s 211us/step - loss: 0.5731 - acc: 0.7016 - val_loss: 0.5474 - val_acc: 0.7500\n",
      "Epoch 10/100\n",
      "4500/4500 [==============================] - 1s 212us/step - loss: 0.5712 - acc: 0.7056 - val_loss: 0.5557 - val_acc: 0.7040\n",
      "Epoch 11/100\n",
      "4500/4500 [==============================] - 1s 213us/step - loss: 0.5809 - acc: 0.6953 - val_loss: 0.5593 - val_acc: 0.7100\n",
      "Epoch 12/100\n",
      "4500/4500 [==============================] - 1s 214us/step - loss: 0.5679 - acc: 0.7047 - val_loss: 0.5668 - val_acc: 0.7020\n",
      "Epoch 13/100\n",
      "4500/4500 [==============================] - 1s 210us/step - loss: 0.5719 - acc: 0.7064 - val_loss: 0.5535 - val_acc: 0.7300\n",
      "Epoch 14/100\n",
      "4500/4500 [==============================] - 1s 211us/step - loss: 0.5723 - acc: 0.7011 - val_loss: 0.5451 - val_acc: 0.7220\n",
      "Epoch 15/100\n",
      "4500/4500 [==============================] - 1s 212us/step - loss: 0.5850 - acc: 0.6887 - val_loss: 0.5501 - val_acc: 0.7420\n",
      "Epoch 16/100\n",
      "4500/4500 [==============================] - 1s 214us/step - loss: 0.5683 - acc: 0.7069 - val_loss: 0.5437 - val_acc: 0.7200\n",
      "Epoch 17/100\n",
      "4500/4500 [==============================] - 1s 211us/step - loss: 0.5765 - acc: 0.6976 - val_loss: 0.5513 - val_acc: 0.7240\n",
      "Epoch 18/100\n",
      "4500/4500 [==============================] - 1s 212us/step - loss: 0.5649 - acc: 0.7064 - val_loss: 0.5428 - val_acc: 0.7580\n",
      "Epoch 19/100\n",
      "4500/4500 [==============================] - 1s 213us/step - loss: 0.5747 - acc: 0.6938 - val_loss: 0.5437 - val_acc: 0.7540\n",
      "Epoch 20/100\n",
      "4500/4500 [==============================] - 1s 211us/step - loss: 0.5709 - acc: 0.7024 - val_loss: 0.5545 - val_acc: 0.7220\n",
      "Epoch 21/100\n",
      "4500/4500 [==============================] - 1s 209us/step - loss: 0.5764 - acc: 0.7009 - val_loss: 0.5489 - val_acc: 0.7260\n",
      "Epoch 22/100\n",
      "4500/4500 [==============================] - 1s 211us/step - loss: 0.5679 - acc: 0.7038 - val_loss: 0.5786 - val_acc: 0.6820\n",
      "Epoch 23/100\n",
      "4500/4500 [==============================] - 1s 210us/step - loss: 0.5849 - acc: 0.6931 - val_loss: 0.5698 - val_acc: 0.7000\n",
      "Epoch 24/100\n",
      "4500/4500 [==============================] - 1s 204us/step - loss: 0.5807 - acc: 0.7009 - val_loss: 0.5454 - val_acc: 0.7440\n",
      "Epoch 25/100\n",
      "4500/4500 [==============================] - 1s 213us/step - loss: 0.5934 - acc: 0.6978 - val_loss: 0.5460 - val_acc: 0.7460\n",
      "Epoch 26/100\n",
      "4500/4500 [==============================] - 1s 211us/step - loss: 0.5628 - acc: 0.7138 - val_loss: 0.5552 - val_acc: 0.7020\n",
      "Epoch 27/100\n",
      "4500/4500 [==============================] - 1s 205us/step - loss: 0.5712 - acc: 0.7116 - val_loss: 0.5477 - val_acc: 0.7340\n",
      "Epoch 28/100\n",
      "4500/4500 [==============================] - 1s 208us/step - loss: 0.5772 - acc: 0.7020 - val_loss: 0.5496 - val_acc: 0.7380\n",
      "Epoch 29/100\n",
      "4500/4500 [==============================] - 1s 210us/step - loss: 0.5739 - acc: 0.7027 - val_loss: 0.5557 - val_acc: 0.7300\n",
      "Epoch 30/100\n",
      "4500/4500 [==============================] - 1s 210us/step - loss: 0.5669 - acc: 0.7029 - val_loss: 0.5504 - val_acc: 0.7300\n",
      "Epoch 31/100\n",
      "4500/4500 [==============================] - 1s 209us/step - loss: 0.5690 - acc: 0.7051 - val_loss: 0.5580 - val_acc: 0.7120\n",
      "Epoch 32/100\n",
      "4500/4500 [==============================] - 1s 209us/step - loss: 0.5733 - acc: 0.6971 - val_loss: 0.5446 - val_acc: 0.7500\n",
      "Epoch 33/100\n",
      "4500/4500 [==============================] - 1s 208us/step - loss: 0.5747 - acc: 0.7053 - val_loss: 0.5825 - val_acc: 0.6720\n",
      "Epoch 34/100\n",
      "4500/4500 [==============================] - 1s 210us/step - loss: 0.5608 - acc: 0.7142 - val_loss: 0.5549 - val_acc: 0.7240\n",
      "Epoch 35/100\n",
      "4500/4500 [==============================] - 1s 207us/step - loss: 0.5677 - acc: 0.7069 - val_loss: 0.5522 - val_acc: 0.7180\n",
      "Epoch 36/100\n",
      "4500/4500 [==============================] - 1s 213us/step - loss: 0.5714 - acc: 0.7073 - val_loss: 0.5480 - val_acc: 0.7280\n",
      "Epoch 37/100\n",
      "4500/4500 [==============================] - 1s 209us/step - loss: 0.5758 - acc: 0.7049 - val_loss: 0.5434 - val_acc: 0.7520\n",
      "Epoch 38/100\n",
      "4500/4500 [==============================] - 1s 206us/step - loss: 0.5602 - acc: 0.7127 - val_loss: 0.5553 - val_acc: 0.7020\n",
      "Epoch 39/100\n",
      "4500/4500 [==============================] - 1s 212us/step - loss: 0.5676 - acc: 0.7096 - val_loss: 0.5824 - val_acc: 0.6780\n",
      "Epoch 40/100\n",
      "4500/4500 [==============================] - 1s 214us/step - loss: 0.5743 - acc: 0.7013 - val_loss: 0.5798 - val_acc: 0.6720\n",
      "Epoch 41/100\n",
      "4500/4500 [==============================] - 1s 213us/step - loss: 0.5699 - acc: 0.7089 - val_loss: 0.5369 - val_acc: 0.7480\n",
      "Epoch 42/100\n",
      "4500/4500 [==============================] - 1s 208us/step - loss: 0.5707 - acc: 0.7096 - val_loss: 0.5401 - val_acc: 0.7520\n",
      "Epoch 43/100\n",
      "4500/4500 [==============================] - 1s 207us/step - loss: 0.5618 - acc: 0.7144 - val_loss: 0.5400 - val_acc: 0.7460\n",
      "Epoch 44/100\n",
      "4500/4500 [==============================] - 1s 210us/step - loss: 0.5693 - acc: 0.7107 - val_loss: 0.5467 - val_acc: 0.7340\n",
      "Epoch 45/100\n",
      "4500/4500 [==============================] - 1s 212us/step - loss: 0.5750 - acc: 0.7071 - val_loss: 0.5511 - val_acc: 0.7020\n",
      "Epoch 46/100\n",
      "4500/4500 [==============================] - 1s 209us/step - loss: 0.5754 - acc: 0.6982 - val_loss: 0.5474 - val_acc: 0.7120\n",
      "Epoch 47/100\n",
      "4500/4500 [==============================] - 1s 214us/step - loss: 0.5672 - acc: 0.7024 - val_loss: 0.5436 - val_acc: 0.7420\n",
      "Epoch 48/100\n",
      "1376/4500 [========>.....................] - ETA: 0s - loss: 0.5679 - acc: 0.7078"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-fd661d89de7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/hateval/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/hateval/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/hateval/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/hateval/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/hateval/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/hateval/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/hateval/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/hateval/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/hateval/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/hateval/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train.values, validation_data=(X_dev, y_dev), epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluaci√≥n sobre dev\n",
      "500/500 [==============================] - 0s 161us/step\n",
      "Loss           : 0.5472\n",
      "Accuracy       : 0.7240\n",
      "Precision(1)   : 0.7658\n",
      "Precision(1)   : 0.7047\n",
      "Precision(avg) : 0.7353\n",
      "\n",
      "Recall(1)      : 0.5450\n",
      "Recall(0)      : 0.8669\n",
      "Recall(avg)    : 0.7060\n",
      "\n",
      "F1(1)          : 0.6368\n",
      "F1(0)          : 0.7774\n",
      "F1(avg)        : 0.7071\n",
      "\n",
      "\n",
      "Evaluaci√≥n sobre test\n",
      "1600/1600 [==============================] - 0s 131us/step\n",
      "Loss           : 0.6222\n",
      "Accuracy       : 0.6556\n",
      "Precision(1)   : 0.5986\n",
      "Precision(1)   : 0.6858\n",
      "Precision(avg) : 0.6422\n",
      "\n",
      "Recall(1)      : 0.5015\n",
      "Recall(0)      : 0.7638\n",
      "Recall(avg)    : 0.6327\n",
      "\n",
      "F1(1)          : 0.5458\n",
      "F1(0)          : 0.7227\n",
      "F1(avg)        : 0.6342\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "\n",
    "print(\"Evaluaci√≥n sobre dev\")\n",
    "print_evaluation(model, X_dev, y_dev)\n",
    "print(\"\\n\\nEvaluaci√≥n sobre test\")\n",
    "print_evaluation(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Vamos a ver los tweets con mayores errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_true</th>\n",
       "      <th>pred_false</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hs=1</th>\n",
       "      <td>175</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hs=0</th>\n",
       "      <td>53</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pred_true  pred_false\n",
       "real                       \n",
       "hs=1        175          47\n",
       "hs=0         53         225"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev[\"proba\"] = model.predict_proba(X_dev)\n",
    "\n",
    "\n",
    "true_positives = df_dev[(df_dev[\"HS\"] == 1) & (df_dev[\"proba\"] >= 0.5)].copy()\n",
    "true_negatives = df_dev[(df_dev[\"HS\"] == 0) & (df_dev[\"proba\"] < 0.5)].copy()\n",
    "\n",
    "false_positives = df_dev[(df_dev[\"HS\"] == 0) & (df_dev[\"proba\"] > 0.5)].copy()\n",
    "false_positives.sort_values(\"proba\", ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "false_negatives = df_dev[(df_dev[\"HS\"] == 1) & (df_dev[\"proba\"] < 0.5)].copy()\n",
    "false_negatives.sort_values(\"proba\", ascending=True, inplace=True)\n",
    "\n",
    "conf_matrix = pd.DataFrame([\n",
    "    {\"real\":\"hs=1\", \"pred_true\": len(true_positives), \"pred_false\": len(false_negatives)},\n",
    "    {\"real\":\"hs=0\", \"pred_true\": len(false_positives), \"pred_false\": len(true_negatives)}\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "conf_matrix.set_index(\"real\", inplace=True)\n",
    "\n",
    "conf_matrix[[\"pred_true\", \"pred_false\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Falsos Negativos\n",
    "\n",
    "Veamos los 20 falsos negativos en los cuales nuestro modelo se equivoca m√°s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21535</th>\n",
       "      <td>#VOX critica duramente a ‚Å¶@pablocasado_‚Å© ‚ÄúLes da la bienvenida como si fuera un cartel del welcome de #Carmena ‚Äú ¬°Expulsi√≥n inmediata de moromierdas!  https://t.co/HaySDibj2l</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.043542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21152</th>\n",
       "      <td>@NatyBurgos15 @Horaciogenta @PaolaPa05 C√°llate vos Black and bitch como la puta falsa abogada. Espero se pudra en la c√°rcel y devuelva todo. Ded√≠cate a estudiar en serio. La droga es mala. Cristin...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.053808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22560</th>\n",
       "      <td>@CarlaEUrena @FelipeCalderon Callateeeee !! Puta gallina cacarea !! Ni idea tienes de nada .</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.080199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23415</th>\n",
       "      <td>Exacto! #puralata #feminazis #HombreAbusado https://t.co/zTEvaftxcd</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.084722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21215</th>\n",
       "      <td>Que tal otro aporte an√≥nimo son de la puta de mi esposa, as√≠ los dej√≥ en navidad ense√±ando le los calzones al vecino rt y siguenos @TANGASUCIA @Tangasrobadas https://t.co/70ioUjveqY</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.092792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24925</th>\n",
       "      <td>La verdad que si, no veia la hora de llegar a los 5 mil amigosüòÇ ahora de a poco voy borrando esos indios indocumentadosüòÇ https://t.co/Xw8q4GLM8v</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.144993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24401</th>\n",
       "      <td>No habr√° que darles de lo 'nuestro'. Ellos se apropiar√°n de lo 'suyo'.  @Manuj_Hidalgo les dice ‚òùÔ∏è a quienes piensan q hay q jubilar a los mayores para q trabajen los j√≥venes o q los q vienen d fu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.151047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21072</th>\n",
       "      <td>Voy andando por la calle y me han dicho cosas en √°rabe ya que me estas acosando dime lo en espa√±ol para que te entienda, asique como yo no les entiendo me he cagado en su puta madre en √°rabe para ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.152465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24590</th>\n",
       "      <td>SOMOS MUCHO M√ÅS DE LO QUE SUE√ëAS SER PINCH√â SUDACA CAGADO, y si no te gusta dime donde nos topamos pa reventarnos el hocico 1:1 @amarchesin1 https://t.co/vHHPBKS1AK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.157015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22008</th>\n",
       "      <td>@Namurii @medicaster_xz @Yorha2d @KeloKlau_ Se merec√≠a de puta y no quiere que le metan la verga, jajaja no mamen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24488</th>\n",
       "      <td>Un indocumentado...habla y habla y no para y acaba hablando su ignorancia.Empezamos bien. https://t.co/SMRsqjtEBT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22168</th>\n",
       "      <td>Mi novia es como mi polla. S√© que est√° ah√≠ porque de vez en cuando se levanta. Una de mis pantalones y la otra de la cama.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.185569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24864</th>\n",
       "      <td>rosalia me podr√≠a decir sudaca i wouldn't give a fuck</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24481</th>\n",
       "      <td>En que se parece superman a una mujer lista: En que los dos son personajes de ciencia-ficci√≥n.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.193847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24431</th>\n",
       "      <td>Yo nunca le he dicho puta a una mujer pero socia t√∫ eres senda petardo üòÇüòÇ estas navidades no voy a comprar pirotecnias ya contigo vasta üòÇüòÇüòÇ</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.194956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24746</th>\n",
       "      <td>@rU_1970 muestra como teni esa vagina de puta caliente llena de moco wacha culia hija de la perra traga semen 146:146</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.199260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21887</th>\n",
       "      <td>@A3Noticias Esta tiene que ser una 'fake new' porque las mujeres son siempre las v√≠ctimas, no las hijas de puta maltratadoras. Eso o el chaval es un machista que se lo ha merecido (machete al mach...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.222684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21727</th>\n",
       "      <td>Espa√±a crea su primer 'campo de refugiados' en la bah√≠a de Algeciras https://t.co/l7Zmh2kvPN v√≠a @elmundoes Gran noticia para el contribuyente espa√±ol. Espa√±oles a pagar, a pagar. Las pensiones pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.224704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20337</th>\n",
       "      <td>Ni armar un complot para matar un presidente saben... #Sudacas..</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.236325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21339</th>\n",
       "      <td>La #Inmigraci√≥n, como sabemos, es mala, se ha demostrado en media #Europa. Pero no debemos caer en la tentaci√≥n de salpicar nuestra #soberan√≠a con extremismos, tenemos que defender lo nuestro, si,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.236573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                          text  \\\n",
       "id                                                                                                                                                                                                               \n",
       "21535                           #VOX critica duramente a ‚Å¶@pablocasado_‚Å© ‚ÄúLes da la bienvenida como si fuera un cartel del welcome de #Carmena ‚Äú ¬°Expulsi√≥n inmediata de moromierdas!  https://t.co/HaySDibj2l   \n",
       "21152  @NatyBurgos15 @Horaciogenta @PaolaPa05 C√°llate vos Black and bitch como la puta falsa abogada. Espero se pudra en la c√°rcel y devuelva todo. Ded√≠cate a estudiar en serio. La droga es mala. Cristin...   \n",
       "22560                                                                                                             @CarlaEUrena @FelipeCalderon Callateeeee !! Puta gallina cacarea !! Ni idea tienes de nada .   \n",
       "23415                                                                                                                                      Exacto! #puralata #feminazis #HombreAbusado https://t.co/zTEvaftxcd   \n",
       "21215                    Que tal otro aporte an√≥nimo son de la puta de mi esposa, as√≠ los dej√≥ en navidad ense√±ando le los calzones al vecino rt y siguenos @TANGASUCIA @Tangasrobadas https://t.co/70ioUjveqY   \n",
       "24925                                                         La verdad que si, no veia la hora de llegar a los 5 mil amigosüòÇ ahora de a poco voy borrando esos indios indocumentadosüòÇ https://t.co/Xw8q4GLM8v   \n",
       "24401  No habr√° que darles de lo 'nuestro'. Ellos se apropiar√°n de lo 'suyo'.  @Manuj_Hidalgo les dice ‚òùÔ∏è a quienes piensan q hay q jubilar a los mayores para q trabajen los j√≥venes o q los q vienen d fu...   \n",
       "21072  Voy andando por la calle y me han dicho cosas en √°rabe ya que me estas acosando dime lo en espa√±ol para que te entienda, asique como yo no les entiendo me he cagado en su puta madre en √°rabe para ...   \n",
       "24590                                     SOMOS MUCHO M√ÅS DE LO QUE SUE√ëAS SER PINCH√â SUDACA CAGADO, y si no te gusta dime donde nos topamos pa reventarnos el hocico 1:1 @amarchesin1 https://t.co/vHHPBKS1AK   \n",
       "22008                                                                                        @Namurii @medicaster_xz @Yorha2d @KeloKlau_ Se merec√≠a de puta y no quiere que le metan la verga, jajaja no mamen   \n",
       "24488                                                                                        Un indocumentado...habla y habla y no para y acaba hablando su ignorancia.Empezamos bien. https://t.co/SMRsqjtEBT   \n",
       "22168                                                                               Mi novia es como mi polla. S√© que est√° ah√≠ porque de vez en cuando se levanta. Una de mis pantalones y la otra de la cama.   \n",
       "24864                                                                                                                                                    rosalia me podr√≠a decir sudaca i wouldn't give a fuck   \n",
       "24481                                                                                                           En que se parece superman a una mujer lista: En que los dos son personajes de ciencia-ficci√≥n.   \n",
       "24431                                                              Yo nunca le he dicho puta a una mujer pero socia t√∫ eres senda petardo üòÇüòÇ estas navidades no voy a comprar pirotecnias ya contigo vasta üòÇüòÇüòÇ   \n",
       "24746                                                                                    @rU_1970 muestra como teni esa vagina de puta caliente llena de moco wacha culia hija de la perra traga semen 146:146   \n",
       "21887  @A3Noticias Esta tiene que ser una 'fake new' porque las mujeres son siempre las v√≠ctimas, no las hijas de puta maltratadoras. Eso o el chaval es un machista que se lo ha merecido (machete al mach...   \n",
       "21727  Espa√±a crea su primer 'campo de refugiados' en la bah√≠a de Algeciras https://t.co/l7Zmh2kvPN v√≠a @elmundoes Gran noticia para el contribuyente espa√±ol. Espa√±oles a pagar, a pagar. Las pensiones pe...   \n",
       "20337                                                                                                                                         Ni armar un complot para matar un presidente saben... #Sudacas..   \n",
       "21339  La #Inmigraci√≥n, como sabemos, es mala, se ha demostrado en media #Europa. Pero no debemos caer en la tentaci√≥n de salpicar nuestra #soberan√≠a con extremismos, tenemos que defender lo nuestro, si,...   \n",
       "\n",
       "       HS  TR  AG     proba  \n",
       "id                           \n",
       "21535   1   0   1  0.043542  \n",
       "21152   1   1   1  0.053808  \n",
       "22560   1   1   1  0.080199  \n",
       "23415   1   0   1  0.084722  \n",
       "21215   1   1   1  0.092792  \n",
       "24925   1   0   0  0.144993  \n",
       "24401   1   0   1  0.151047  \n",
       "21072   1   0   1  0.152465  \n",
       "24590   1   1   1  0.157015  \n",
       "22008   1   1   0  0.168723  \n",
       "24488   1   0   0  0.183900  \n",
       "22168   1   1   0  0.185569  \n",
       "24864   1   1   1  0.186539  \n",
       "24481   1   0   0  0.193847  \n",
       "24431   1   1   0  0.194956  \n",
       "24746   1   1   1  0.199260  \n",
       "21887   1   0   1  0.222684  \n",
       "21727   1   0   0  0.224704  \n",
       "20337   1   0   1  0.236325  \n",
       "21339   1   0   1  0.236573  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives.iloc[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Qu√© onda la longitud de la secuencia?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 734., 1109., 1019.,  655.,  386.,  315.,  210.,   57.,   13.,\n",
       "           2.]),\n",
       " array([ 2., 10., 18., 26., 34., 42., 50., 58., 66., 74., 82.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokens_dev = df_dev[\"text\"].apply(lambda t: len(tokenizer.tokenize(t)))\n",
    "tokens_train = df_train[\"text\"].apply(lambda t: len(tokenizer.tokenize(t)))\n",
    "\n",
    "plt.hist(tokens_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22517</th>\n",
       "      <td>-¬øeres poblana? -soy colombiana -¬øy por qu√© eres g√ºera? en Colombia TODAS son morenas, fui a Colombia 3 veces. -ok, ment√≠ üôÑ solo me pinto el cabello</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.971048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22416</th>\n",
       "      <td>@Pepe_ElEzequiel Puta!? Puta con las que vos te metes Esa mujer es un √°ngel enviado por Jesucristo nuestro se√±or para darnos esa paz que buscamos s√≥lo con verla</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.967200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21892</th>\n",
       "      <td>@saancisco Callate puta tu eres el traidor.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.963039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21497</th>\n",
       "      <td>Talvez quede como zorra pero te lo mereces Sociaüòé</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.947185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24172</th>\n",
       "      <td>Y los que no se denuncian... hagamos n√∫meros. El acoso a las mujeres es diario. No todos los hombres son acosadores, pero todas las mujeres han sido acosadas. https://t.co/jza9llhqWA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.945128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21255</th>\n",
       "      <td>Y t√∫ eres una maldita escoria con una repugnante inclinaci√≥n por los hombres.. ¬°Seres como t√∫ me producen n√°useas! https://t.co/TaHDcpZdnm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24159</th>\n",
       "      <td>@LeonDeJapon Jajajajajaja tu eres uno, diego jajajajajaja mentira perra perra perrraaaaaaaaaa de mierda jajaja ‚ô°</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.931898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24402</th>\n",
       "      <td>zorra eres tu que no te sientes hombre y no entiendes que toda mujer es bella</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.925799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21671</th>\n",
       "      <td>@danlntifada @fatinascidi @manolivizoso @Zurine3 @Jvillaver2 https://t.co/HG4ljNlwR3, co√±o entonces tu eres de lo que te guias por el antes de cristo y despues de cristo,cuando llegamos los Espa√±o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.921038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22115</th>\n",
       "      <td>Una chica no es una guarra, una zorra, una puta, una buscan por su manera de vestir.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.915731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22014</th>\n",
       "      <td>Ni t√∫ eres una zorra hambrienta ni yo un incorregible depredador.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.910154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20376</th>\n",
       "      <td>Zorra tu abuela, imb√©cil.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.890432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21050</th>\n",
       "      <td>@GvyMC @Wismichu Callate puta rataüòÇüòÇ ya estas llorando??</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.877848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22848</th>\n",
       "      <td>Pinche beb√© pendejo c√°llate el hocico, nom√°s te est√°n cortando el pelo. Puta chillona</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.856129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22938</th>\n",
       "      <td>Masculinidad fr√°gil es mi crush franc√©s ech√°ndose un clavado al cenote solo porque un grupo de √°rabes le dijo pussy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.855542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23320</th>\n",
       "      <td>@HenriFalconLara No digas nada que t√∫ eres c√≥mplice de lo que est√° pasando... y recuerda seguir√°s siendo la perra m√°s perra de Venezuela</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.854512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21966</th>\n",
       "      <td>esc√∫chame ya me joder√≠a jajajajajajajajajajajajajaja si pa ti ense√±ar es ser una guarra yo soy biennnn guarra, menos mal que no me respeto que si no... https://t.co/yt5fHp99b7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.848084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24209</th>\n",
       "      <td>¬øYa les he contado que si fuera rica/millonaria ser√≠a toda una PERRA?, Yisus me est√° ense√±ando humildad... Y no he aprendido mucho ldvdd.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.813778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22846</th>\n",
       "      <td>Si a mi se me ocurre ense√±ar por twitter una conversaci√≥n de whatsap con mi madre me manda a dormir a la caseta de mi perra.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.808360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22457</th>\n",
       "      <td>De Colombia tocar√° emigrar en pateras, como hacen los africanos subsaharianos.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.795402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                          text  \\\n",
       "id                                                                                                                                                                                                               \n",
       "22517                                                     -¬øeres poblana? -soy colombiana -¬øy por qu√© eres g√ºera? en Colombia TODAS son morenas, fui a Colombia 3 veces. -ok, ment√≠ üôÑ solo me pinto el cabello   \n",
       "22416                                         @Pepe_ElEzequiel Puta!? Puta con las que vos te metes Esa mujer es un √°ngel enviado por Jesucristo nuestro se√±or para darnos esa paz que buscamos s√≥lo con verla   \n",
       "21892                                                                                                                                                              @saancisco Callate puta tu eres el traidor.   \n",
       "21497                                                                                                                                                        Talvez quede como zorra pero te lo mereces Sociaüòé   \n",
       "24172                   Y los que no se denuncian... hagamos n√∫meros. El acoso a las mujeres es diario. No todos los hombres son acosadores, pero todas las mujeres han sido acosadas. https://t.co/jza9llhqWA   \n",
       "21255                                                               Y t√∫ eres una maldita escoria con una repugnante inclinaci√≥n por los hombres.. ¬°Seres como t√∫ me producen n√°useas! https://t.co/TaHDcpZdnm   \n",
       "24159                                                                                         @LeonDeJapon Jajajajajaja tu eres uno, diego jajajajajaja mentira perra perra perrraaaaaaaaaa de mierda jajaja ‚ô°   \n",
       "24402                                                                                                                            zorra eres tu que no te sientes hombre y no entiendes que toda mujer es bella   \n",
       "21671  @danlntifada @fatinascidi @manolivizoso @Zurine3 @Jvillaver2 https://t.co/HG4ljNlwR3, co√±o entonces tu eres de lo que te guias por el antes de cristo y despues de cristo,cuando llegamos los Espa√±o...   \n",
       "22115                                                                                                                     Una chica no es una guarra, una zorra, una puta, una buscan por su manera de vestir.   \n",
       "22014                                                                                                                                        Ni t√∫ eres una zorra hambrienta ni yo un incorregible depredador.   \n",
       "20376                                                                                                                                                                                Zorra tu abuela, imb√©cil.   \n",
       "21050                                                                                                                                                 @GvyMC @Wismichu Callate puta rataüòÇüòÇ ya estas llorando??   \n",
       "22848                                                                                                                    Pinche beb√© pendejo c√°llate el hocico, nom√°s te est√°n cortando el pelo. Puta chillona   \n",
       "22938                                                                                      Masculinidad fr√°gil es mi crush franc√©s ech√°ndose un clavado al cenote solo porque un grupo de √°rabes le dijo pussy   \n",
       "23320                                                                 @HenriFalconLara No digas nada que t√∫ eres c√≥mplice de lo que est√° pasando... y recuerda seguir√°s siendo la perra m√°s perra de Venezuela   \n",
       "21966                          esc√∫chame ya me joder√≠a jajajajajajajajajajajajajaja si pa ti ense√±ar es ser una guarra yo soy biennnn guarra, menos mal que no me respeto que si no... https://t.co/yt5fHp99b7   \n",
       "24209                                                                ¬øYa les he contado que si fuera rica/millonaria ser√≠a toda una PERRA?, Yisus me est√° ense√±ando humildad... Y no he aprendido mucho ldvdd.   \n",
       "22846                                                                             Si a mi se me ocurre ense√±ar por twitter una conversaci√≥n de whatsap con mi madre me manda a dormir a la caseta de mi perra.   \n",
       "22457                                                                                                                           De Colombia tocar√° emigrar en pateras, como hacen los africanos subsaharianos.   \n",
       "\n",
       "       HS  TR  AG     proba  \n",
       "id                           \n",
       "22517   0   0   0  0.971048  \n",
       "22416   0   0   0  0.967200  \n",
       "21892   0   0   0  0.963039  \n",
       "21497   0   0   0  0.947185  \n",
       "24172   0   0   0  0.945128  \n",
       "21255   0   0   0  0.943286  \n",
       "24159   0   0   0  0.931898  \n",
       "24402   0   0   0  0.925799  \n",
       "21671   0   0   0  0.921038  \n",
       "22115   0   0   0  0.915731  \n",
       "22014   0   0   0  0.910154  \n",
       "20376   0   0   0  0.890432  \n",
       "21050   0   0   0  0.877848  \n",
       "22848   0   0   0  0.856129  \n",
       "22938   0   0   0  0.855542  \n",
       "23320   0   0   0  0.854512  \n",
       "21966   0   0   0  0.848084  \n",
       "24209   0   0   0  0.813778  \n",
       "22846   0   0   0  0.808360  \n",
       "22457   0   0   0  0.795402  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 200)\n",
    "false_positives.iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['easyjet',\n",
       " 'quiere',\n",
       " 'duplicar',\n",
       " 'el',\n",
       " 'n√∫mero',\n",
       " 'de',\n",
       " 'mujeres',\n",
       " 'piloto',\n",
       " \"'\",\n",
       " 'ver√°s',\n",
       " 't√∫',\n",
       " 'para',\n",
       " 'aparcar',\n",
       " 'el',\n",
       " 'avi√≥n',\n",
       " '..',\n",
       " 'http://t.co/46NuLkm09x',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

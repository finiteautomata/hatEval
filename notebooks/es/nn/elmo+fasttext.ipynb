{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elmo + Embeddings\n",
    "\n",
    "Probemos si usando también los embeddings de fastText obtenemos algo razonable...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instancias de entrenamiento: 4500\n",
      "Instancias de desarrollo: 500\n",
      "Instancias de test: 1600\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(2019)\n",
    "np.random.seed(2019)\n",
    "tf.random.set_random_seed(2019)\n",
    "random.seed(2019)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "df_dev = pd.read_table(\"../../../data/es/dev_es.tsv\", index_col=\"id\", quoting=csv.QUOTE_NONE)\n",
    "df_train = pd.read_table(\"../../../data/es/train_es.tsv\", index_col=\"id\", quoting=csv.QUOTE_NONE)\n",
    "df_test = pd.read_table(\"../../../data/es/reference_es.tsv\", header=None, \n",
    "                        names=[\"text\", \"HS\", \"TR\", \"AG\"], quoting=csv.QUOTE_NONE)\n",
    "\n",
    "\n",
    "text_train, y_train = df_train[\"text\"], df_train[\"HS\"]\n",
    "text_dev, y_dev = df_dev[\"text\"], df_dev[\"HS\"]\n",
    "text_test, y_test = df_test[\"text\"], df_test[\"HS\"]\n",
    "\n",
    "print(\"Instancias de entrenamiento: {}\".format(len(df_train)))\n",
    "print(\"Instancias de desarrollo: {}\".format(len(df_dev)))\n",
    "print(\"Instancias de test: {}\".format(len(df_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText\n",
    "import os\n",
    "\n",
    "\n",
    "model = fastText.load_model(os.path.expanduser(\"../../../WordVectors/wiki.es.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_shape = model.get_word_vector(\"pepe\").shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tengo que hacer dos cosas:\n",
    "\n",
    "- Primero, convertir los tweets a secuencias de texto\n",
    "- Luego, paddear las secuencias a cierta longitud (Keras necesita esto para poder paralelizar cálculo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "max_length = 40\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    if len(tokens) >= max_length:\n",
    "        tokens = tokens[:max_length]\n",
    "    else:\n",
    "        tokens = tokens + [''] * (max_length - len(tokens))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokens_train = [preprocess_tweet(tweet) for tweet in df_train[\"text\"].values]\n",
    "tokens_dev = [preprocess_tweet(tweet) for tweet in df_dev[\"text\"].values]\n",
    "tokens_test = [preprocess_tweet(tweet) for tweet in df_test[\"text\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from elmoformanylangs import Embedder\n",
    "\n",
    "e = Embedder(\"../../../models/elmo/es/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carguemos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X_train = np.array(e.sents2elmo(tokens_train))\n",
    "X_dev = np.array(e.sents2elmo(tokens_dev))\n",
    "X_test = np.array(e.sents2elmo(tokens_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4500, 40, 300), (500, 40, 300), (1600, 40, 300))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embeddings(toks):\n",
    "    ret = []\n",
    "    \n",
    "    for tok in toks:\n",
    "        vec = model.get_word_vector(tok)\n",
    "        ret.append(vec)\n",
    "    return ret\n",
    "\n",
    "X_emb_train = np.array([get_embeddings(toks) for toks in tokens_train])\n",
    "X_emb_dev = np.array([get_embeddings(toks) for toks in tokens_dev])\n",
    "X_emb_test = np.array([get_embeddings(toks) for toks in tokens_test])\n",
    "\n",
    "X_emb_train.shape, X_emb_dev.shape, X_emb_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 40, 1024)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 40, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 40, 1324)     0           input_17[0][0]                   \n",
      "                                                                 input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 512)          3239936     concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 512)          0           bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            513         dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,240,449\n",
      "Trainable params: 3,240,449\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Dropout, CuDNNLSTM, CuDNNGRU, Input, Concatenate, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "embedding_dim = 1024\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.00025,\n",
    "    \"decay\": 0.01,\n",
    "}\n",
    "\n",
    "elmo_input = Input(shape=X_train[0].shape)\n",
    "emb_input = Input(shape=X_emb_train[0].shape)\n",
    "\n",
    "x = Concatenate()([elmo_input, emb_input])\n",
    "x = Bidirectional(CuDNNLSTM(256))(x)\n",
    "x = Dropout(0.50)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=[elmo_input, emb_input], outputs=[output])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 0.6292 - acc: 0.6553 - val_loss: 0.5911 - val_acc: 0.6820\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.68200, saving model to /tmp/lstm_model.h5\n",
      "Epoch 2/100\n",
      "4500/4500 [==============================] - 3s 658us/step - loss: 0.5279 - acc: 0.7380 - val_loss: 0.5362 - val_acc: 0.7300\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.68200 to 0.73000, saving model to /tmp/lstm_model.h5\n",
      "Epoch 3/100\n",
      "4500/4500 [==============================] - 3s 655us/step - loss: 0.4742 - acc: 0.7742 - val_loss: 0.5072 - val_acc: 0.7560\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.73000 to 0.75600, saving model to /tmp/lstm_model.h5\n",
      "Epoch 4/100\n",
      "4500/4500 [==============================] - 3s 648us/step - loss: 0.4231 - acc: 0.8071 - val_loss: 0.5170 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.75600\n",
      "Epoch 5/100\n",
      "4500/4500 [==============================] - 3s 662us/step - loss: 0.3938 - acc: 0.8333 - val_loss: 0.4707 - val_acc: 0.7820\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.75600 to 0.78200, saving model to /tmp/lstm_model.h5\n",
      "Epoch 6/100\n",
      "4500/4500 [==============================] - 3s 674us/step - loss: 0.3635 - acc: 0.8398 - val_loss: 0.4521 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.78200 to 0.80800, saving model to /tmp/lstm_model.h5\n",
      "Epoch 7/100\n",
      "4500/4500 [==============================] - 3s 607us/step - loss: 0.3407 - acc: 0.8622 - val_loss: 0.4538 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.80800\n",
      "Epoch 8/100\n",
      "4500/4500 [==============================] - 3s 675us/step - loss: 0.3136 - acc: 0.8722 - val_loss: 0.4551 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.80800\n",
      "Epoch 9/100\n",
      "4500/4500 [==============================] - 3s 636us/step - loss: 0.2955 - acc: 0.8838 - val_loss: 0.4586 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.80800\n",
      "Epoch 10/100\n",
      "4500/4500 [==============================] - 3s 621us/step - loss: 0.2768 - acc: 0.8922 - val_loss: 0.4614 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.80800 to 0.81000, saving model to /tmp/lstm_model.h5\n",
      "Epoch 11/100\n",
      "4500/4500 [==============================] - 3s 675us/step - loss: 0.2656 - acc: 0.8931 - val_loss: 0.4822 - val_acc: 0.7780\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.81000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6640a35dd8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpointer = ModelCheckpoint('/tmp/lstm_model.h5', save_best_only=True, monitor='val_acc', verbose=1)\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "\n",
    "model.fit([X_train, X_emb_train], y_train, \n",
    "          callbacks=[checkpointer, early_stopper],\n",
    "          validation_data=([X_dev, X_emb_dev], y_dev), epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biLSTM - Elmo+Embeddings -- \n",
      "\n",
      "\n",
      "Evaluación sobre dev\n",
      "500/500 [==============================] - 0s 332us/step\n",
      "Loss           : 0.4614\n",
      "Accuracy       : 0.8100\n",
      "Precision(1)   : 0.7679\n",
      "Precision(1)   : 0.8479\n",
      "Precision(avg) : 0.8079\n",
      "\n",
      "Recall(1)      : 0.8198\n",
      "Recall(0)      : 0.8022\n",
      "Recall(avg)    : 0.8110\n",
      "\n",
      "F1(1)          : 0.7930\n",
      "F1(0)          : 0.8244\n",
      "F1(avg)        : 0.8087\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "print(\"biLSTM - Elmo+Embeddings -- \\n\\n\")\n",
    "print(\"Evaluación sobre dev\")\n",
    "\n",
    "model.load_weights(checkpointer.filepath)\n",
    "\n",
    "print_evaluation(model, [X_dev, X_emb_dev], y_dev)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, 40, 1024)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           (None, 40, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 40, 1324)     0           input_21[0][0]                   \n",
      "                                                                 input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 512)          2429952     concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            513         dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 2,430,465\n",
      "Trainable params: 2,430,465\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 1024\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.0005,\n",
    "    \"decay\": 0.01,\n",
    "}\n",
    "\n",
    "elmo_input = Input(shape=X_train[0].shape)\n",
    "emb_input = Input(shape=X_emb_train[0].shape)\n",
    "\n",
    "x = Concatenate()([elmo_input, emb_input])\n",
    "x = Bidirectional(CuDNNGRU(256))(x)\n",
    "x = Dropout(0.65)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=[elmo_input, emb_input], outputs=[output])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      "4500/4500 [==============================] - 8s 2ms/step - loss: 0.7124 - acc: 0.6164 - val_loss: 0.5885 - val_acc: 0.6620\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.66200, saving model to /tmp/lstm_model.h5\n",
      "Epoch 2/100\n",
      "4500/4500 [==============================] - 3s 686us/step - loss: 0.5459 - acc: 0.7209 - val_loss: 0.5027 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.66200 to 0.74000, saving model to /tmp/lstm_model.h5\n",
      "Epoch 3/100\n",
      "4500/4500 [==============================] - 3s 689us/step - loss: 0.4522 - acc: 0.7882 - val_loss: 0.4682 - val_acc: 0.7780\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.74000 to 0.77800, saving model to /tmp/lstm_model.h5\n",
      "Epoch 4/100\n",
      "4500/4500 [==============================] - 3s 697us/step - loss: 0.3991 - acc: 0.8184 - val_loss: 0.4477 - val_acc: 0.7980\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.77800 to 0.79800, saving model to /tmp/lstm_model.h5\n",
      "Epoch 5/100\n",
      "4500/4500 [==============================] - 3s 607us/step - loss: 0.3611 - acc: 0.8427 - val_loss: 0.4328 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.79800 to 0.82200, saving model to /tmp/lstm_model.h5\n",
      "Epoch 6/100\n",
      "4500/4500 [==============================] - 3s 670us/step - loss: 0.3362 - acc: 0.8496 - val_loss: 0.4303 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.82200\n",
      "Epoch 7/100\n",
      "4500/4500 [==============================] - 3s 698us/step - loss: 0.3082 - acc: 0.8720 - val_loss: 0.4358 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.82200\n",
      "Epoch 8/100\n",
      "4500/4500 [==============================] - 3s 676us/step - loss: 0.2836 - acc: 0.8778 - val_loss: 0.4508 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.82200\n",
      "Epoch 9/100\n",
      "4500/4500 [==============================] - 3s 657us/step - loss: 0.2656 - acc: 0.8891 - val_loss: 0.4278 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.82200\n",
      "Epoch 10/100\n",
      "4500/4500 [==============================] - 3s 689us/step - loss: 0.2480 - acc: 0.9024 - val_loss: 0.4266 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.82200\n",
      "Epoch 11/100\n",
      "4500/4500 [==============================] - 3s 661us/step - loss: 0.2293 - acc: 0.9091 - val_loss: 0.4322 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.82200 to 0.82400, saving model to /tmp/lstm_model.h5\n",
      "Epoch 12/100\n",
      "4500/4500 [==============================] - 3s 571us/step - loss: 0.2087 - acc: 0.9231 - val_loss: 0.4332 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.82400 to 0.82400, saving model to /tmp/lstm_model.h5\n",
      "Epoch 13/100\n",
      "4500/4500 [==============================] - 3s 597us/step - loss: 0.1952 - acc: 0.9282 - val_loss: 0.4756 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.82400\n",
      "Epoch 14/100\n",
      "4500/4500 [==============================] - 3s 669us/step - loss: 0.1870 - acc: 0.9331 - val_loss: 0.4393 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.82400\n",
      "Epoch 15/100\n",
      "4500/4500 [==============================] - 3s 702us/step - loss: 0.1715 - acc: 0.9422 - val_loss: 0.4569 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.82400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6640800518>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpointer = ModelCheckpoint('/tmp/lstm_model.h5', save_best_only=True, monitor='val_acc', verbose=1)\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "\n",
    "model.fit([X_train, X_emb_train], y_train, \n",
    "          callbacks=[checkpointer, early_stopper],\n",
    "          validation_data=([X_dev, X_emb_dev], y_dev), epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biLSTM - Elmo+Embeddings -- \n",
      "\n",
      "\n",
      "Evaluación sobre dev\n",
      "500/500 [==============================] - 0s 378us/step\n",
      "Loss           : 0.4569\n",
      "Accuracy       : 0.8000\n",
      "Precision(1)   : 0.7723\n",
      "Precision(1)   : 0.8225\n",
      "Precision(avg) : 0.7974\n",
      "\n",
      "Recall(1)      : 0.7793\n",
      "Recall(0)      : 0.8165\n",
      "Recall(avg)    : 0.7979\n",
      "\n",
      "F1(1)          : 0.7758\n",
      "F1(0)          : 0.8195\n",
      "F1(avg)        : 0.7976\n",
      "\n",
      "\n",
      "Evaluación sobre test\n",
      "1600/1600 [==============================] - 1s 356us/step\n",
      "Loss           : 0.6611\n",
      "Accuracy       : 0.7381\n",
      "Precision(1)   : 0.6600\n",
      "Precision(1)   : 0.8076\n",
      "Precision(avg) : 0.7338\n",
      "\n",
      "Recall(1)      : 0.7530\n",
      "Recall(0)      : 0.7277\n",
      "Recall(avg)    : 0.7403\n",
      "\n",
      "F1(1)          : 0.7035\n",
      "F1(0)          : 0.7655\n",
      "F1(avg)        : 0.7345\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "print(\"biGRU - Elmo+Embeddings -- \\n\\n\")\n",
    "print(\"Evaluación sobre dev\")\n",
    "print_evaluation(model, [X_dev, X_emb_dev], y_dev)\n",
    "print(\"\\n\\nEvaluación sobre test\")\n",
    "print_evaluation(model, [X_test, X_emb_test], y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

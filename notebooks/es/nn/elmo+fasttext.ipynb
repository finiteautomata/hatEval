{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elmo + Embeddings\n",
    "\n",
    "Probemos si usando también los embeddings de fastText obtenemos algo razonable...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instancias de entrenamiento: 4500\n",
      "Instancias de desarrollo: 500\n",
      "Instancias de test: 1600\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(2019)\n",
    "np.random.seed(2019)\n",
    "tf.random.set_random_seed(2019)\n",
    "random.seed(2019)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "df_dev = pd.read_table(\"../../../data/es/dev_es.tsv\", index_col=\"id\", quoting=csv.QUOTE_NONE)\n",
    "df_train = pd.read_table(\"../../../data/es/train_es.tsv\", index_col=\"id\", quoting=csv.QUOTE_NONE)\n",
    "df_test = pd.read_table(\"../../../data/es/reference_es.tsv\", header=None, \n",
    "                        names=[\"text\", \"HS\", \"TR\", \"AG\"], quoting=csv.QUOTE_NONE)\n",
    "\n",
    "\n",
    "text_train, y_train = df_train[\"text\"], df_train[\"HS\"]\n",
    "text_dev, y_dev = df_dev[\"text\"], df_dev[\"HS\"]\n",
    "text_test, y_test = df_test[\"text\"], df_test[\"HS\"]\n",
    "\n",
    "print(\"Instancias de entrenamiento: {}\".format(len(df_train)))\n",
    "print(\"Instancias de desarrollo: {}\".format(len(df_dev)))\n",
    "print(\"Instancias de test: {}\".format(len(df_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText\n",
    "import os\n",
    "\n",
    "\n",
    "model = fastText.load_model(os.path.expanduser(\"~/WordVectors/UBA_w5_300.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_shape = model.get_word_vector(\"pepe\").shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6086544990539551"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine as cosine_distance\n",
    "\n",
    "def cos_similarity(v1, v2):\n",
    "    return 1 - cosine_distance(v1, v2)\n",
    "\n",
    "cos_similarity(model.get_word_vector(\"angau\"), model.get_word_vector(\"anga\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tengo que hacer dos cosas:\n",
    "\n",
    "- Primero, convertir los tweets a secuencias de texto\n",
    "- Luego, paddear las secuencias a cierta longitud (Keras necesita esto para poder paralelizar cálculo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "max_length = 40\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    if len(tokens) >= max_length:\n",
    "        tokens = tokens[:max_length]\n",
    "    else:\n",
    "        tokens = tokens + [''] * (max_length - len(tokens))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokens_train = [preprocess_tweet(tweet) for tweet in df_train[\"text\"].values]\n",
    "tokens_dev = [preprocess_tweet(tweet) for tweet in df_dev[\"text\"].values]\n",
    "tokens_test = [preprocess_tweet(tweet) for tweet in df_test[\"text\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from elmoformanylangs import Embedder\n",
    "\n",
    "e = Embedder(\"../../../models/elmo/es/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carguemos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents2elmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/3.6.5/envs/hateval/lib/python3.6/site-packages/elmoformanylangs/elmo.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "e.sents2elmo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X_train = np.array(e.sents2elmo(tokens_train))\n",
    "X_dev = np.array(e.sents2elmo(tokens_dev))\n",
    "X_test = np.array(e.sents2elmo(tokens_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4500, 30, 300), (500, 30, 300), (1600, 30, 300))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embeddings(toks):\n",
    "    ret = []\n",
    "    \n",
    "    for tok in toks:\n",
    "        vec = model.get_word_vector(tok)\n",
    "        ret.append(vec)\n",
    "    return ret\n",
    "\n",
    "X_emb_train = np.array([get_embeddings(toks) for toks in tokens_train])\n",
    "X_emb_dev = np.array([get_embeddings(toks) for toks in tokens_dev])\n",
    "X_emb_test = np.array([get_embeddings(toks) for toks in tokens_test])\n",
    "\n",
    "X_emb_train.shape, X_emb_dev.shape, X_emb_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 30, 1024)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, 30, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 30, 1324)     0           input_13[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 512)          3239936     concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 512)          0           bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 128)          65664       dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 128)          0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1)            129         dropout_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,305,729\n",
      "Trainable params: 3,305,729\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "4500/4500 [==============================] - 5s 1ms/step - loss: 0.8074 - acc: 0.5653 - val_loss: 0.6368 - val_acc: 0.6540\n",
      "Epoch 2/30\n",
      "4500/4500 [==============================] - 2s 555us/step - loss: 0.6602 - acc: 0.6393 - val_loss: 0.6148 - val_acc: 0.6620\n",
      "Epoch 3/30\n",
      "4500/4500 [==============================] - 3s 577us/step - loss: 0.6164 - acc: 0.6756 - val_loss: 0.5837 - val_acc: 0.7060\n",
      "Epoch 4/30\n",
      "4500/4500 [==============================] - 3s 563us/step - loss: 0.5856 - acc: 0.7002 - val_loss: 0.5638 - val_acc: 0.7120\n",
      "Epoch 5/30\n",
      "4500/4500 [==============================] - 3s 565us/step - loss: 0.5533 - acc: 0.7160 - val_loss: 0.5483 - val_acc: 0.7220\n",
      "Epoch 6/30\n",
      "4500/4500 [==============================] - 3s 566us/step - loss: 0.5316 - acc: 0.7444 - val_loss: 0.5176 - val_acc: 0.7540\n",
      "Epoch 7/30\n",
      "4500/4500 [==============================] - 3s 566us/step - loss: 0.5052 - acc: 0.7602 - val_loss: 0.5076 - val_acc: 0.7580\n",
      "Epoch 8/30\n",
      "4500/4500 [==============================] - 3s 571us/step - loss: 0.4873 - acc: 0.7682 - val_loss: 0.4898 - val_acc: 0.7640\n",
      "Epoch 9/30\n",
      "4500/4500 [==============================] - 3s 570us/step - loss: 0.4630 - acc: 0.7849 - val_loss: 0.4836 - val_acc: 0.7700\n",
      "Epoch 10/30\n",
      "4500/4500 [==============================] - 3s 569us/step - loss: 0.4475 - acc: 0.7996 - val_loss: 0.4706 - val_acc: 0.7800\n",
      "Epoch 11/30\n",
      "4500/4500 [==============================] - 3s 563us/step - loss: 0.4414 - acc: 0.8000 - val_loss: 0.4696 - val_acc: 0.7760\n",
      "Epoch 12/30\n",
      "4500/4500 [==============================] - 3s 569us/step - loss: 0.4220 - acc: 0.8102 - val_loss: 0.4551 - val_acc: 0.7920\n",
      "Epoch 13/30\n",
      "4500/4500 [==============================] - 3s 559us/step - loss: 0.4166 - acc: 0.8178 - val_loss: 0.4553 - val_acc: 0.7860\n",
      "Epoch 14/30\n",
      "4500/4500 [==============================] - 3s 563us/step - loss: 0.4126 - acc: 0.8193 - val_loss: 0.4526 - val_acc: 0.7940\n",
      "Epoch 15/30\n",
      "4500/4500 [==============================] - 3s 563us/step - loss: 0.3973 - acc: 0.8276 - val_loss: 0.4499 - val_acc: 0.8000\n",
      "Epoch 16/30\n",
      "4500/4500 [==============================] - 3s 584us/step - loss: 0.3902 - acc: 0.8338 - val_loss: 0.4536 - val_acc: 0.8000\n",
      "Epoch 17/30\n",
      "4500/4500 [==============================] - 3s 588us/step - loss: 0.3752 - acc: 0.8371 - val_loss: 0.4616 - val_acc: 0.7960\n",
      "Epoch 18/30\n",
      "4500/4500 [==============================] - 3s 571us/step - loss: 0.3586 - acc: 0.8489 - val_loss: 0.4459 - val_acc: 0.8140\n",
      "Epoch 19/30\n",
      "4500/4500 [==============================] - 3s 573us/step - loss: 0.3626 - acc: 0.8487 - val_loss: 0.4440 - val_acc: 0.8100\n",
      "Epoch 20/30\n",
      "4500/4500 [==============================] - 3s 569us/step - loss: 0.3568 - acc: 0.8484 - val_loss: 0.4425 - val_acc: 0.8140\n",
      "Epoch 21/30\n",
      "4500/4500 [==============================] - 3s 576us/step - loss: 0.3464 - acc: 0.8522 - val_loss: 0.4405 - val_acc: 0.8200\n",
      "Epoch 22/30\n",
      "4500/4500 [==============================] - 3s 560us/step - loss: 0.3353 - acc: 0.8622 - val_loss: 0.4365 - val_acc: 0.8160\n",
      "Epoch 23/30\n",
      "4500/4500 [==============================] - 3s 573us/step - loss: 0.3296 - acc: 0.8607 - val_loss: 0.4416 - val_acc: 0.8160\n",
      "Epoch 24/30\n",
      "4500/4500 [==============================] - 2s 550us/step - loss: 0.3227 - acc: 0.8693 - val_loss: 0.4363 - val_acc: 0.8120\n",
      "Epoch 25/30\n",
      "4500/4500 [==============================] - 3s 566us/step - loss: 0.3151 - acc: 0.8691 - val_loss: 0.4444 - val_acc: 0.8140\n",
      "Epoch 26/30\n",
      "4500/4500 [==============================] - 3s 566us/step - loss: 0.3112 - acc: 0.8756 - val_loss: 0.4370 - val_acc: 0.8100\n",
      "Epoch 27/30\n",
      "4500/4500 [==============================] - 3s 563us/step - loss: 0.3095 - acc: 0.8778 - val_loss: 0.4426 - val_acc: 0.8120\n",
      "Epoch 28/30\n",
      "4500/4500 [==============================] - 3s 563us/step - loss: 0.2925 - acc: 0.8793 - val_loss: 0.4440 - val_acc: 0.8020\n",
      "Epoch 29/30\n",
      "4500/4500 [==============================] - 3s 587us/step - loss: 0.2908 - acc: 0.8836 - val_loss: 0.4436 - val_acc: 0.8080\n",
      "Epoch 30/30\n",
      "4500/4500 [==============================] - 3s 562us/step - loss: 0.2829 - acc: 0.8887 - val_loss: 0.4560 - val_acc: 0.8220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc72f80c240>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Dropout, CuDNNLSTM, CuDNNGRU, Input, Concatenate, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "embedding_dim = 1024\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.0005,\n",
    "    \"decay\": 0.01,\n",
    "}\n",
    "\n",
    "elmo_input = Input(shape=X_train[0].shape)\n",
    "emb_input = Input(shape=X_emb_train[0].shape)\n",
    "\n",
    "x = Concatenate()([elmo_input, emb_input])\n",
    "x = Bidirectional(CuDNNLSTM(256))(x)\n",
    "x = Dropout(0.80)(x)\n",
    "x = Dense(128)(x)\n",
    "x = Dropout(0.55)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=[elmo_input, emb_input], outputs=[output])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit([X_train, X_emb_train], y_train, \n",
    "          validation_data=([X_dev, X_emb_dev], y_dev), epochs=30, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biLSTM - Elmo+Embeddings -- \n",
      "\n",
      "\n",
      "Evaluación sobre dev\n",
      "500/500 [==============================] - 0s 298us/step\n",
      "Loss           : 0.4560\n",
      "Accuracy       : 0.8220\n",
      "Precision(1)   : 0.8122\n",
      "Precision(1)   : 0.8293\n",
      "Precision(avg) : 0.8207\n",
      "\n",
      "Recall(1)      : 0.7793\n",
      "Recall(0)      : 0.8561\n",
      "Recall(avg)    : 0.8177\n",
      "\n",
      "F1(1)          : 0.7954\n",
      "F1(0)          : 0.8425\n",
      "F1(avg)        : 0.8189\n",
      "\n",
      "\n",
      "Evaluación sobre test\n",
      "1600/1600 [==============================] - 0s 286us/step\n",
      "Loss           : 0.6082\n",
      "Accuracy       : 0.7400\n",
      "Precision(1)   : 0.6699\n",
      "Precision(1)   : 0.7971\n",
      "Precision(avg) : 0.7335\n",
      "\n",
      "Recall(1)      : 0.7288\n",
      "Recall(0)      : 0.7479\n",
      "Recall(avg)    : 0.7383\n",
      "\n",
      "F1(1)          : 0.6981\n",
      "F1(0)          : 0.7717\n",
      "F1(avg)        : 0.7349\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "print(\"biLSTM - Elmo+Embeddings -- \\n\\n\")\n",
    "print(\"Evaluación sobre dev\")\n",
    "print_evaluation(model, [X_dev, X_emb_dev], y_dev)\n",
    "print(\"\\n\\nEvaluación sobre test\")\n",
    "print_evaluation(model, [X_test, X_emb_test], y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional GRU sin densa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_39 (InputLayer)           (None, 30, 1024)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_40 (InputLayer)           (None, 30, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 30, 1324)     0           input_39[0][0]                   \n",
      "                                                                 input_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_23 (Bidirectional (None, 512)          2429952     concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 512)          0           bidirectional_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 1)            513         dropout_31[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 2,430,465\n",
      "Trainable params: 2,430,465\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/12\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 0.7098 - acc: 0.6242 - val_loss: 0.5831 - val_acc: 0.6920\n",
      "Epoch 2/12\n",
      "4500/4500 [==============================] - 2s 451us/step - loss: 0.5390 - acc: 0.7362 - val_loss: 0.5204 - val_acc: 0.7380\n",
      "Epoch 3/12\n",
      "4500/4500 [==============================] - 2s 449us/step - loss: 0.4658 - acc: 0.7818 - val_loss: 0.4814 - val_acc: 0.7680\n",
      "Epoch 4/12\n",
      "4500/4500 [==============================] - 2s 504us/step - loss: 0.3997 - acc: 0.8182 - val_loss: 0.4507 - val_acc: 0.8080\n",
      "Epoch 5/12\n",
      "4500/4500 [==============================] - 2s 497us/step - loss: 0.3652 - acc: 0.8391 - val_loss: 0.4435 - val_acc: 0.7940\n",
      "Epoch 6/12\n",
      "4500/4500 [==============================] - 2s 510us/step - loss: 0.3299 - acc: 0.8598 - val_loss: 0.4541 - val_acc: 0.7920\n",
      "Epoch 7/12\n",
      "4500/4500 [==============================] - 2s 483us/step - loss: 0.3081 - acc: 0.8662 - val_loss: 0.4373 - val_acc: 0.8000\n",
      "Epoch 8/12\n",
      "4500/4500 [==============================] - 2s 514us/step - loss: 0.2818 - acc: 0.8849 - val_loss: 0.4236 - val_acc: 0.8160\n",
      "Epoch 9/12\n",
      "4500/4500 [==============================] - 2s 502us/step - loss: 0.2616 - acc: 0.8936 - val_loss: 0.4316 - val_acc: 0.8000\n",
      "Epoch 10/12\n",
      "4500/4500 [==============================] - 2s 508us/step - loss: 0.2474 - acc: 0.9016 - val_loss: 0.4268 - val_acc: 0.8100\n",
      "Epoch 11/12\n",
      "4500/4500 [==============================] - 2s 506us/step - loss: 0.2285 - acc: 0.9104 - val_loss: 0.4248 - val_acc: 0.8080\n",
      "Epoch 12/12\n",
      "4500/4500 [==============================] - 2s 513us/step - loss: 0.2122 - acc: 0.9216 - val_loss: 0.4459 - val_acc: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc3d38f9eb8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 1024\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.0005,\n",
    "    \"decay\": 0.01,\n",
    "}\n",
    "\n",
    "elmo_input = Input(shape=X_train[0].shape)\n",
    "emb_input = Input(shape=X_emb_train[0].shape)\n",
    "\n",
    "x = Concatenate()([elmo_input, emb_input])\n",
    "x = Bidirectional(CuDNNGRU(256))(x)\n",
    "x = Dropout(0.65)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=[elmo_input, emb_input], outputs=[output])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit([X_train, X_emb_train], y_train, \n",
    "          validation_data=([X_dev, X_emb_dev], y_dev), epochs=12, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biLSTM - Elmo+Embeddings -- \n",
      "\n",
      "\n",
      "Evaluación sobre dev\n",
      "500/500 [==============================] - 0s 224us/step\n",
      "Loss           : 0.4459\n",
      "Accuracy       : 0.8000\n",
      "Precision(1)   : 0.7933\n",
      "Precision(1)   : 0.8048\n",
      "Precision(avg) : 0.7990\n",
      "\n",
      "Recall(1)      : 0.7432\n",
      "Recall(0)      : 0.8453\n",
      "Recall(avg)    : 0.7943\n",
      "\n",
      "F1(1)          : 0.7674\n",
      "F1(0)          : 0.8246\n",
      "F1(avg)        : 0.7960\n",
      "\n",
      "\n",
      "Evaluación sobre test\n",
      "1600/1600 [==============================] - 0s 228us/step\n",
      "Loss           : 0.5869\n",
      "Accuracy       : 0.7481\n",
      "Precision(1)   : 0.6962\n",
      "Precision(1)   : 0.7841\n",
      "Precision(avg) : 0.7402\n",
      "\n",
      "Recall(1)      : 0.6909\n",
      "Recall(0)      : 0.7883\n",
      "Recall(avg)    : 0.7396\n",
      "\n",
      "F1(1)          : 0.6935\n",
      "F1(0)          : 0.7862\n",
      "F1(avg)        : 0.7399\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "print(\"biLSTM - Elmo+Embeddings -- \\n\\n\")\n",
    "print(\"Evaluación sobre dev\")\n",
    "print_evaluation(model, [X_dev, X_emb_dev], y_dev)\n",
    "print(\"\\n\\nEvaluación sobre test\")\n",
    "print_evaluation(model, [X_test, X_emb_test], y_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

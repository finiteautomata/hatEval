{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elmo + Embeddings\n",
    "\n",
    "Probemos si usando también los embeddings de fastText obtenemos algo razonable...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instancias de entrenamiento: 4500\n",
      "Instancias de desarrollo: 500\n",
      "Instancias de test: 1600\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(2019)\n",
    "np.random.seed(2019)\n",
    "tf.random.set_random_seed(2019)\n",
    "random.seed(2019)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "df_dev = pd.read_table(\"../../../data/es/dev_es.tsv\", index_col=\"id\", quoting=csv.QUOTE_NONE)\n",
    "df_train = pd.read_table(\"../../../data/es/train_es.tsv\", index_col=\"id\", quoting=csv.QUOTE_NONE)\n",
    "df_test = pd.read_table(\"../../../data/es/reference_es.tsv\", header=None, \n",
    "                        names=[\"text\", \"HS\", \"TR\", \"AG\"], quoting=csv.QUOTE_NONE)\n",
    "\n",
    "\n",
    "text_train, y_train = df_train[\"text\"], df_train[\"HS\"]\n",
    "text_dev, y_dev = df_dev[\"text\"], df_dev[\"HS\"]\n",
    "text_test, y_test = df_test[\"text\"], df_test[\"HS\"]\n",
    "\n",
    "print(\"Instancias de entrenamiento: {}\".format(len(df_train)))\n",
    "print(\"Instancias de desarrollo: {}\".format(len(df_dev)))\n",
    "print(\"Instancias de test: {}\".format(len(df_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_eyeball = df_dev.sample(n=300, random_state=20190911)\n",
    "text_eyeball, y_eyeball = df_eyeball[\"text\"], df_eyeball[\"HS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText\n",
    "import os\n",
    "\n",
    "\n",
    "model = fastText.load_model(os.path.expanduser(\"../../../WordVectors/UBA_w3_300.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tengo que hacer dos cosas:\n",
    "\n",
    "- Primero, convertir los tweets a secuencias de texto\n",
    "- Luego, paddear las secuencias a cierta longitud (Keras necesita esto para poder paralelizar cálculo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "max_length = 40\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    if len(tokens) >= max_length:\n",
    "        tokens = tokens[:max_length]\n",
    "    else:\n",
    "        tokens = tokens + [''] * (max_length - len(tokens))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokens_train = [preprocess_tweet(tweet) for tweet in df_train[\"text\"].values]\n",
    "tokens_dev = [preprocess_tweet(tweet) for tweet in df_dev[\"text\"].values]\n",
    "tokens_test = [preprocess_tweet(tweet) for tweet in df_test[\"text\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from elmoformanylangs import Embedder\n",
    "\n",
    "e = Embedder(\"../../../models/elmo/es/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carguemos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X_train = np.array(e.sents2elmo(tokens_train))\n",
    "X_dev = np.array(e.sents2elmo(tokens_dev))\n",
    "X_test = np.array(e.sents2elmo(tokens_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4500, 40, 300), (500, 40, 300), (1600, 40, 300))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embeddings(toks):\n",
    "    ret = []\n",
    "    \n",
    "    for tok in toks:\n",
    "        vec = model.get_word_vector(tok)\n",
    "        ret.append(vec)\n",
    "    return ret\n",
    "\n",
    "X_emb_train = np.array([get_embeddings(toks) for toks in tokens_train])\n",
    "X_emb_dev = np.array([get_embeddings(toks) for toks in tokens_dev])\n",
    "X_emb_test = np.array([get_embeddings(toks) for toks in tokens_test])\n",
    "\n",
    "X_emb_train.shape, X_emb_dev.shape, X_emb_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40, 1024)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 40, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 40, 1324)     0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 512)          3239936     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            513         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,240,449\n",
      "Trainable params: 3,240,449\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Dropout, CuDNNLSTM, CuDNNGRU, Input, Concatenate, Bidirectional, GlobalMaxPool1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "embedding_dim = 1024\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.00075,\n",
    "    \"decay\": 0.01,\n",
    "}\n",
    "\n",
    "elmo_input = Input(shape=X_train[0].shape)\n",
    "emb_input = Input(shape=X_emb_train[0].shape)\n",
    "\n",
    "x = Concatenate()([elmo_input, emb_input])\n",
    "x = Bidirectional(CuDNNLSTM(256))(x)\n",
    "x = Dropout(0.65)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=[elmo_input, emb_input], outputs=[output])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      "4500/4500 [==============================] - 6s 1ms/step - loss: 0.6433 - acc: 0.6456 - val_loss: 0.6010 - val_acc: 0.6580\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.65800, saving model to /tmp/lstm_model.h5\n",
      "Epoch 2/100\n",
      "4500/4500 [==============================] - 3s 557us/step - loss: 0.5106 - acc: 0.7502 - val_loss: 0.4981 - val_acc: 0.7840\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.65800 to 0.78400, saving model to /tmp/lstm_model.h5\n",
      "Epoch 3/100\n",
      "4500/4500 [==============================] - 2s 540us/step - loss: 0.4216 - acc: 0.8036 - val_loss: 0.4436 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.78400 to 0.80000, saving model to /tmp/lstm_model.h5\n",
      "Epoch 4/100\n",
      "4500/4500 [==============================] - 2s 551us/step - loss: 0.3687 - acc: 0.8382 - val_loss: 0.4183 - val_acc: 0.8260\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.80000 to 0.82600, saving model to /tmp/lstm_model.h5\n",
      "Epoch 5/100\n",
      "4500/4500 [==============================] - 2s 545us/step - loss: 0.3151 - acc: 0.8693 - val_loss: 0.4267 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.82600\n",
      "Epoch 6/100\n",
      "4500/4500 [==============================] - 2s 548us/step - loss: 0.2794 - acc: 0.8840 - val_loss: 0.4234 - val_acc: 0.8340\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.82600 to 0.83400, saving model to /tmp/lstm_model.h5\n",
      "Epoch 7/100\n",
      "4500/4500 [==============================] - 3s 560us/step - loss: 0.2403 - acc: 0.9058 - val_loss: 0.4361 - val_acc: 0.8380\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.83400 to 0.83800, saving model to /tmp/lstm_model.h5\n",
      "Epoch 8/100\n",
      "4500/4500 [==============================] - 2s 545us/step - loss: 0.2139 - acc: 0.9211 - val_loss: 0.4443 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.83800\n",
      "Epoch 9/100\n",
      "4500/4500 [==============================] - 2s 544us/step - loss: 0.1845 - acc: 0.9340 - val_loss: 0.4780 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.83800\n",
      "Epoch 10/100\n",
      "4500/4500 [==============================] - 2s 545us/step - loss: 0.1522 - acc: 0.9451 - val_loss: 0.5039 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.83800\n",
      "Epoch 11/100\n",
      "4500/4500 [==============================] - 2s 544us/step - loss: 0.1330 - acc: 0.9556 - val_loss: 0.4996 - val_acc: 0.8280\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.83800\n",
      "Epoch 12/100\n",
      "4500/4500 [==============================] - 3s 562us/step - loss: 0.1066 - acc: 0.9704 - val_loss: 0.5531 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.83800\n",
      "Epoch 13/100\n",
      "4500/4500 [==============================] - 2s 545us/step - loss: 0.0920 - acc: 0.9736 - val_loss: 0.5166 - val_acc: 0.8340\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.83800\n",
      "Epoch 14/100\n",
      "4500/4500 [==============================] - 3s 559us/step - loss: 0.0765 - acc: 0.9791 - val_loss: 0.5394 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.83800\n",
      "Epoch 15/100\n",
      "4500/4500 [==============================] - 2s 544us/step - loss: 0.0610 - acc: 0.9849 - val_loss: 0.5736 - val_acc: 0.8320\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.83800\n",
      "Epoch 16/100\n",
      "4500/4500 [==============================] - 2s 548us/step - loss: 0.0542 - acc: 0.9851 - val_loss: 0.6013 - val_acc: 0.8300\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.83800\n",
      "Epoch 17/100\n",
      "4500/4500 [==============================] - 2s 551us/step - loss: 0.0451 - acc: 0.9913 - val_loss: 0.6309 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.83800\n",
      "Epoch 18/100\n",
      "4500/4500 [==============================] - 2s 550us/step - loss: 0.0380 - acc: 0.9909 - val_loss: 0.6721 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.83800\n",
      "Epoch 19/100\n",
      "4500/4500 [==============================] - 3s 556us/step - loss: 0.0328 - acc: 0.9938 - val_loss: 0.6990 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.83800\n",
      "Epoch 20/100\n",
      "4500/4500 [==============================] - 2s 545us/step - loss: 0.0275 - acc: 0.9949 - val_loss: 0.7093 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.83800\n",
      "Epoch 21/100\n",
      "4500/4500 [==============================] - 2s 547us/step - loss: 0.0236 - acc: 0.9947 - val_loss: 0.7321 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.83800\n",
      "Epoch 22/100\n",
      "4500/4500 [==============================] - 3s 561us/step - loss: 0.0208 - acc: 0.9969 - val_loss: 0.7787 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.83800\n",
      "Epoch 23/100\n",
      "4500/4500 [==============================] - 2s 543us/step - loss: 0.0182 - acc: 0.9982 - val_loss: 0.8070 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.83800\n",
      "Epoch 24/100\n",
      "4500/4500 [==============================] - 3s 559us/step - loss: 0.0167 - acc: 0.9980 - val_loss: 0.8082 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.83800\n",
      "Epoch 25/100\n",
      "4500/4500 [==============================] - 2s 534us/step - loss: 0.0138 - acc: 0.9991 - val_loss: 0.8292 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.83800\n",
      "Epoch 26/100\n",
      "4500/4500 [==============================] - 2s 547us/step - loss: 0.0142 - acc: 0.9982 - val_loss: 0.8467 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.83800\n",
      "Epoch 27/100\n",
      "4500/4500 [==============================] - 2s 540us/step - loss: 0.0130 - acc: 0.9991 - val_loss: 0.8819 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.83800\n",
      "Epoch 28/100\n",
      "4500/4500 [==============================] - 2s 548us/step - loss: 0.0119 - acc: 0.9991 - val_loss: 0.8910 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.83800\n",
      "Epoch 29/100\n",
      "4500/4500 [==============================] - 2s 552us/step - loss: 0.0106 - acc: 0.9996 - val_loss: 0.9343 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.83800\n",
      "Epoch 30/100\n",
      "4500/4500 [==============================] - 2s 540us/step - loss: 0.0090 - acc: 0.9991 - val_loss: 0.9759 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.83800\n",
      "Epoch 31/100\n",
      "4500/4500 [==============================] - 2s 547us/step - loss: 0.0086 - acc: 0.9996 - val_loss: 0.9484 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.83800\n",
      "Epoch 32/100\n",
      "4500/4500 [==============================] - 2s 544us/step - loss: 0.0088 - acc: 0.9991 - val_loss: 0.9695 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.83800\n",
      "Epoch 33/100\n",
      "4500/4500 [==============================] - 2s 530us/step - loss: 0.0077 - acc: 0.9993 - val_loss: 0.9901 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.83800\n",
      "Epoch 34/100\n",
      "4500/4500 [==============================] - 3s 567us/step - loss: 0.0073 - acc: 0.9993 - val_loss: 0.9858 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.83800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f09a34da0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpointer = ModelCheckpoint('/tmp/lstm_model.h5', save_best_only=True, monitor='val_acc', verbose=1)\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=30)\n",
    "\n",
    "\n",
    "model.fit([X_train, X_emb_train], y_train, \n",
    "          callbacks=[checkpointer, early_stopper],\n",
    "          validation_data=([X_dev, X_emb_dev], y_dev), epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biLSTM - Elmo+Embeddings -- \n",
      "\n",
      "\n",
      "Evaluación sobre dev\n",
      "500/500 [==============================] - 0s 262us/step\n",
      "Loss           : 0.4361\n",
      "Accuracy       : 0.8380\n",
      "Precision(1)   : 0.8249\n",
      "Precision(1)   : 0.8481\n",
      "Precision(avg) : 0.8365\n",
      "\n",
      "Recall(1)      : 0.8063\n",
      "Recall(0)      : 0.8633\n",
      "Recall(avg)    : 0.8348\n",
      "\n",
      "F1(1)          : 0.8155\n",
      "F1(0)          : 0.8556\n",
      "F1(avg)        : 0.8356\n",
      "\n",
      "Evaluación sobre test\n",
      "1600/1600 [==============================] - 0s 261us/step\n",
      "Loss           : 0.6437\n",
      "Accuracy       : 0.7312\n",
      "Precision(1)   : 0.6554\n",
      "Precision(1)   : 0.7965\n",
      "Precision(avg) : 0.7260\n",
      "\n",
      "Recall(1)      : 0.7348\n",
      "Recall(0)      : 0.7287\n",
      "Recall(avg)    : 0.7318\n",
      "\n",
      "F1(1)          : 0.6929\n",
      "F1(0)          : 0.7611\n",
      "F1(avg)        : 0.7270\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "print(\"biLSTM - Elmo+Embeddings -- \\n\\n\")\n",
    "print(\"Evaluación sobre dev\")\n",
    "\n",
    "model.load_weights(checkpointer.filepath)\n",
    "\n",
    "print_evaluation(model, [X_dev, X_emb_dev], y_dev)\n",
    "print(\"\\nEvaluación sobre test\")\n",
    "\n",
    "print_evaluation(model, [X_test, X_emb_test], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 40, 1024)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 40, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 40, 1324)     0           input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 512)          2429952     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            513         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,430,465\n",
      "Trainable params: 2,430,465\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 1024\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.0075,\n",
    "    \"decay\": 0.01,\n",
    "}\n",
    "\n",
    "elmo_input = Input(shape=X_train[0].shape)\n",
    "emb_input = Input(shape=X_emb_train[0].shape)\n",
    "\n",
    "x = Concatenate()([elmo_input, emb_input])\n",
    "x = Bidirectional(CuDNNGRU(256))(x)\n",
    "x = Dropout(0.65)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=[elmo_input, emb_input], outputs=[output])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      "4500/4500 [==============================] - 3s 755us/step - loss: 0.8181 - acc: 0.5727 - val_loss: 0.6239 - val_acc: 0.6380\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.63800, saving model to /tmp/lstm_model.h5\n",
      "Epoch 2/100\n",
      "4500/4500 [==============================] - 2s 504us/step - loss: 0.6306 - acc: 0.6631 - val_loss: 0.5369 - val_acc: 0.7440\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.63800 to 0.74400, saving model to /tmp/lstm_model.h5\n",
      "Epoch 3/100\n",
      "4500/4500 [==============================] - 2s 527us/step - loss: 0.5390 - acc: 0.7369 - val_loss: 0.4965 - val_acc: 0.7660\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.74400 to 0.76600, saving model to /tmp/lstm_model.h5\n",
      "Epoch 4/100\n",
      "4500/4500 [==============================] - 2s 511us/step - loss: 0.5008 - acc: 0.7578 - val_loss: 0.4906 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76600\n",
      "Epoch 5/100\n",
      "4500/4500 [==============================] - 2s 524us/step - loss: 0.4773 - acc: 0.7824 - val_loss: 0.4667 - val_acc: 0.7680\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.76600 to 0.76800, saving model to /tmp/lstm_model.h5\n",
      "Epoch 6/100\n",
      "4500/4500 [==============================] - 2s 516us/step - loss: 0.4434 - acc: 0.8020 - val_loss: 0.4569 - val_acc: 0.7720\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.76800 to 0.77200, saving model to /tmp/lstm_model.h5\n",
      "Epoch 7/100\n",
      "4500/4500 [==============================] - 2s 514us/step - loss: 0.4317 - acc: 0.8051 - val_loss: 0.4467 - val_acc: 0.7900\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.77200 to 0.79000, saving model to /tmp/lstm_model.h5\n",
      "Epoch 8/100\n",
      "4500/4500 [==============================] - 2s 530us/step - loss: 0.4034 - acc: 0.8218 - val_loss: 0.4421 - val_acc: 0.7880\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.79000\n",
      "Epoch 9/100\n",
      "4500/4500 [==============================] - 2s 507us/step - loss: 0.3796 - acc: 0.8320 - val_loss: 0.4361 - val_acc: 0.7860\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79000\n",
      "Epoch 10/100\n",
      "4500/4500 [==============================] - 2s 520us/step - loss: 0.3603 - acc: 0.8473 - val_loss: 0.4391 - val_acc: 0.7740\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79000\n",
      "Epoch 11/100\n",
      "4500/4500 [==============================] - 2s 514us/step - loss: 0.3597 - acc: 0.8369 - val_loss: 0.4336 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.79000 to 0.80200, saving model to /tmp/lstm_model.h5\n",
      "Epoch 12/100\n",
      "4500/4500 [==============================] - 2s 511us/step - loss: 0.3242 - acc: 0.8676 - val_loss: 0.4234 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.80200 to 0.80800, saving model to /tmp/lstm_model.h5\n",
      "Epoch 13/100\n",
      "4500/4500 [==============================] - 2s 532us/step - loss: 0.2934 - acc: 0.8824 - val_loss: 0.4327 - val_acc: 0.7860\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.80800\n",
      "Epoch 14/100\n",
      "4500/4500 [==============================] - 2s 518us/step - loss: 0.3045 - acc: 0.8707 - val_loss: 0.4296 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.80800\n",
      "Epoch 15/100\n",
      "4500/4500 [==============================] - 2s 514us/step - loss: 0.2652 - acc: 0.8927 - val_loss: 0.4270 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.80800\n",
      "Epoch 16/100\n",
      "4500/4500 [==============================] - 2s 523us/step - loss: 0.2568 - acc: 0.8991 - val_loss: 0.4295 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.80800 to 0.81400, saving model to /tmp/lstm_model.h5\n",
      "Epoch 17/100\n",
      "4500/4500 [==============================] - 2s 514us/step - loss: 0.2400 - acc: 0.9096 - val_loss: 0.4278 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.81400\n",
      "Epoch 18/100\n",
      "4500/4500 [==============================] - 2s 526us/step - loss: 0.2325 - acc: 0.9071 - val_loss: 0.4398 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.81400\n",
      "Epoch 19/100\n",
      "4500/4500 [==============================] - 2s 516us/step - loss: 0.2053 - acc: 0.9222 - val_loss: 0.4316 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.81400\n",
      "Epoch 20/100\n",
      "4500/4500 [==============================] - 2s 515us/step - loss: 0.2003 - acc: 0.9216 - val_loss: 0.4322 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.81400\n",
      "Epoch 21/100\n",
      "4500/4500 [==============================] - 2s 530us/step - loss: 0.1840 - acc: 0.9302 - val_loss: 0.4404 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.81400\n",
      "Epoch 22/100\n",
      "4500/4500 [==============================] - 2s 516us/step - loss: 0.1617 - acc: 0.9464 - val_loss: 0.4495 - val_acc: 0.7920\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.81400\n",
      "Epoch 23/100\n",
      "4500/4500 [==============================] - 2s 530us/step - loss: 0.1502 - acc: 0.9489 - val_loss: 0.4537 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.81400\n",
      "Epoch 24/100\n",
      "4500/4500 [==============================] - 2s 519us/step - loss: 0.1348 - acc: 0.9556 - val_loss: 0.4758 - val_acc: 0.7920\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.81400\n",
      "Epoch 25/100\n",
      "4500/4500 [==============================] - 2s 513us/step - loss: 0.1194 - acc: 0.9560 - val_loss: 0.4835 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.81400\n",
      "Epoch 26/100\n",
      "4500/4500 [==============================] - 2s 534us/step - loss: 0.1096 - acc: 0.9678 - val_loss: 0.4871 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.81400\n",
      "Epoch 27/100\n",
      "4500/4500 [==============================] - 2s 512us/step - loss: 0.1011 - acc: 0.9693 - val_loss: 0.4998 - val_acc: 0.7920\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.81400\n",
      "Epoch 28/100\n",
      "4500/4500 [==============================] - 2s 529us/step - loss: 0.0944 - acc: 0.9687 - val_loss: 0.5218 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.81400\n",
      "Epoch 29/100\n",
      "4500/4500 [==============================] - 2s 522us/step - loss: 0.0822 - acc: 0.9762 - val_loss: 0.5143 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.81400\n",
      "Epoch 30/100\n",
      "4500/4500 [==============================] - 2s 513us/step - loss: 0.0780 - acc: 0.9782 - val_loss: 0.5268 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.81400\n",
      "Epoch 31/100\n",
      "4500/4500 [==============================] - 2s 520us/step - loss: 0.0683 - acc: 0.9833 - val_loss: 0.5540 - val_acc: 0.7920\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.81400\n",
      "Epoch 32/100\n",
      "4500/4500 [==============================] - 2s 515us/step - loss: 0.0593 - acc: 0.9851 - val_loss: 0.5655 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.81400\n",
      "Epoch 33/100\n",
      "4500/4500 [==============================] - 2s 512us/step - loss: 0.0559 - acc: 0.9827 - val_loss: 0.5690 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.81400\n",
      "Epoch 34/100\n",
      "4500/4500 [==============================] - 2s 528us/step - loss: 0.0499 - acc: 0.9862 - val_loss: 0.5826 - val_acc: 0.7980\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.81400\n",
      "Epoch 35/100\n",
      "4500/4500 [==============================] - 2s 511us/step - loss: 0.0486 - acc: 0.9876 - val_loss: 0.5687 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.81400\n",
      "Epoch 36/100\n",
      "4500/4500 [==============================] - 2s 533us/step - loss: 0.0407 - acc: 0.9907 - val_loss: 0.6279 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.81400\n",
      "Epoch 37/100\n",
      "4500/4500 [==============================] - 2s 513us/step - loss: 0.0361 - acc: 0.9920 - val_loss: 0.6297 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.81400\n",
      "Epoch 38/100\n",
      "4500/4500 [==============================] - 2s 510us/step - loss: 0.0341 - acc: 0.9916 - val_loss: 0.6322 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.81400\n",
      "Epoch 39/100\n",
      "4500/4500 [==============================] - 2s 526us/step - loss: 0.0324 - acc: 0.9931 - val_loss: 0.6425 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.81400\n",
      "Epoch 40/100\n",
      "4500/4500 [==============================] - 2s 513us/step - loss: 0.0322 - acc: 0.9938 - val_loss: 0.6345 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.81400\n",
      "Epoch 41/100\n",
      "4500/4500 [==============================] - 2s 519us/step - loss: 0.0241 - acc: 0.9964 - val_loss: 0.6576 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.81400\n",
      "Epoch 42/100\n",
      "4500/4500 [==============================] - 2s 521us/step - loss: 0.0226 - acc: 0.9967 - val_loss: 0.6767 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.81400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ec42eddd8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpointer = ModelCheckpoint('/tmp/lstm_model.h5', save_best_only=True, monitor='val_acc', verbose=1)\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=30)\n",
    "\n",
    "\n",
    "model.fit([X_train, X_emb_train], y_train, \n",
    "          callbacks=[checkpointer, early_stopper],\n",
    "          validation_data=([X_dev, X_emb_dev], y_dev), epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biGRU - Elmo+Embeddings -- \n",
      "\n",
      "\n",
      "Evaluación sobre dev\n",
      "500/500 [==============================] - 0s 265us/step\n",
      "Loss           : 0.4295\n",
      "Accuracy       : 0.8140\n",
      "Precision(1)   : 0.7919\n",
      "Precision(1)   : 0.8315\n",
      "Precision(avg) : 0.8117\n",
      "\n",
      "Recall(1)      : 0.7883\n",
      "Recall(0)      : 0.8345\n",
      "Recall(avg)    : 0.8114\n",
      "\n",
      "F1(1)          : 0.7901\n",
      "F1(0)          : 0.8330\n",
      "F1(avg)        : 0.8116\n",
      "\n",
      "Evaluación sobre test\n",
      "1600/1600 [==============================] - 0s 269us/step\n",
      "Loss           : 0.5484\n",
      "Accuracy       : 0.7444\n",
      "Precision(1)   : 0.6816\n",
      "Precision(1)   : 0.7921\n",
      "Precision(avg) : 0.7369\n",
      "\n",
      "Recall(1)      : 0.7136\n",
      "Recall(0)      : 0.7660\n",
      "Recall(avg)    : 0.7398\n",
      "\n",
      "F1(1)          : 0.6973\n",
      "F1(0)          : 0.7788\n",
      "F1(avg)        : 0.7380\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "print(\"biGRU - Elmo+Embeddings -- \\n\\n\")\n",
    "print(\"Evaluación sobre dev\")\n",
    "\n",
    "model.load_weights(checkpointer.filepath)\n",
    "\n",
    "print_evaluation(model, [X_dev, X_emb_dev], y_dev)\n",
    "print(\"\\nEvaluación sobre test\")\n",
    "\n",
    "print_evaluation(model, [X_test, X_emb_test], y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU + Global Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 40, 1024)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 40, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 40, 1324)     0           input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 40, 512)      2429952     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 40, 512)      0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 512)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            513         global_max_pooling1d_1[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 2,430,465\n",
      "Trainable params: 2,430,465\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Dropout, CuDNNLSTM, CuDNNGRU, Input, Concatenate, Bidirectional, GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "embedding_dim = 1024\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.00075,\n",
    "    \"decay\": 0.01,\n",
    "}\n",
    "\n",
    "elmo_input = Input(shape=X_train[0].shape)\n",
    "emb_input = Input(shape=X_emb_train[0].shape)\n",
    "\n",
    "x = Concatenate()([elmo_input, emb_input])\n",
    "x = Bidirectional(CuDNNGRU(256, return_sequences=True))(x)\n",
    "x = Dropout(0.60)(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=[elmo_input, emb_input], outputs=[output])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      "4500/4500 [==============================] - 4s 782us/step - loss: 0.5980 - acc: 0.7142 - val_loss: 0.5232 - val_acc: 0.7580\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.75800, saving model to /tmp/lstm_model.h5\n",
      "Epoch 2/100\n",
      "4500/4500 [==============================] - 2s 528us/step - loss: 0.3903 - acc: 0.8253 - val_loss: 0.4902 - val_acc: 0.7860\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.75800 to 0.78600, saving model to /tmp/lstm_model.h5\n",
      "Epoch 3/100\n",
      "4500/4500 [==============================] - 2s 524us/step - loss: 0.3221 - acc: 0.8667 - val_loss: 0.4804 - val_acc: 0.7900\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.78600 to 0.79000, saving model to /tmp/lstm_model.h5\n",
      "Epoch 4/100\n",
      "4500/4500 [==============================] - 2s 521us/step - loss: 0.2839 - acc: 0.8924 - val_loss: 0.4713 - val_acc: 0.8020\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.79000 to 0.80200, saving model to /tmp/lstm_model.h5\n",
      "Epoch 5/100\n",
      "4500/4500 [==============================] - 2s 536us/step - loss: 0.2522 - acc: 0.9082 - val_loss: 0.4625 - val_acc: 0.7980\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.80200\n",
      "Epoch 6/100\n",
      "4500/4500 [==============================] - 2s 524us/step - loss: 0.2262 - acc: 0.9229 - val_loss: 0.4580 - val_acc: 0.7980\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.80200\n",
      "Epoch 7/100\n",
      "4500/4500 [==============================] - 2s 540us/step - loss: 0.2080 - acc: 0.9347 - val_loss: 0.4561 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.80200 to 0.80400, saving model to /tmp/lstm_model.h5\n",
      "Epoch 8/100\n",
      "4500/4500 [==============================] - 2s 525us/step - loss: 0.1908 - acc: 0.9442 - val_loss: 0.4520 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.80400 to 0.80600, saving model to /tmp/lstm_model.h5\n",
      "Epoch 9/100\n",
      "4500/4500 [==============================] - 2s 519us/step - loss: 0.1764 - acc: 0.9536 - val_loss: 0.4510 - val_acc: 0.8000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.80600\n",
      "Epoch 10/100\n",
      "4500/4500 [==============================] - 2s 534us/step - loss: 0.1613 - acc: 0.9613 - val_loss: 0.4384 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.80600 to 0.81400, saving model to /tmp/lstm_model.h5\n",
      "Epoch 11/100\n",
      "4500/4500 [==============================] - 2s 523us/step - loss: 0.1536 - acc: 0.9627 - val_loss: 0.4383 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.81400 to 0.81800, saving model to /tmp/lstm_model.h5\n",
      "Epoch 12/100\n",
      "4500/4500 [==============================] - 2s 533us/step - loss: 0.1429 - acc: 0.9682 - val_loss: 0.4370 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.81800\n",
      "Epoch 13/100\n",
      "4500/4500 [==============================] - 2s 519us/step - loss: 0.1325 - acc: 0.9736 - val_loss: 0.4306 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.81800\n",
      "Epoch 14/100\n",
      "4500/4500 [==============================] - 2s 521us/step - loss: 0.1234 - acc: 0.9796 - val_loss: 0.4279 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.81800\n",
      "Epoch 15/100\n",
      "4500/4500 [==============================] - 2s 539us/step - loss: 0.1167 - acc: 0.9807 - val_loss: 0.4259 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.81800\n",
      "Epoch 16/100\n",
      "4500/4500 [==============================] - 2s 518us/step - loss: 0.1095 - acc: 0.9813 - val_loss: 0.4270 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.81800\n",
      "Epoch 17/100\n",
      "4500/4500 [==============================] - 2s 524us/step - loss: 0.1036 - acc: 0.9849 - val_loss: 0.4224 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.81800\n",
      "Epoch 18/100\n",
      "4500/4500 [==============================] - 2s 526us/step - loss: 0.0966 - acc: 0.9864 - val_loss: 0.4213 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.81800 to 0.82000, saving model to /tmp/lstm_model.h5\n",
      "Epoch 19/100\n",
      "4500/4500 [==============================] - 2s 511us/step - loss: 0.0899 - acc: 0.9882 - val_loss: 0.4193 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.82000\n",
      "Epoch 20/100\n",
      "4500/4500 [==============================] - 2s 536us/step - loss: 0.0849 - acc: 0.9907 - val_loss: 0.4188 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.82000\n",
      "Epoch 21/100\n",
      "4500/4500 [==============================] - 2s 515us/step - loss: 0.0799 - acc: 0.9909 - val_loss: 0.4150 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.82000\n",
      "Epoch 22/100\n",
      "4500/4500 [==============================] - 2s 524us/step - loss: 0.0754 - acc: 0.9913 - val_loss: 0.4130 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.82000\n",
      "Epoch 23/100\n",
      "4500/4500 [==============================] - 2s 531us/step - loss: 0.0694 - acc: 0.9933 - val_loss: 0.4108 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.82000\n",
      "Epoch 24/100\n",
      "4500/4500 [==============================] - 2s 522us/step - loss: 0.0646 - acc: 0.9938 - val_loss: 0.4103 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.82000\n",
      "Epoch 25/100\n",
      "4500/4500 [==============================] - 2s 523us/step - loss: 0.0604 - acc: 0.9933 - val_loss: 0.4070 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.82000 to 0.82200, saving model to /tmp/lstm_model.h5\n",
      "Epoch 26/100\n",
      "4500/4500 [==============================] - 2s 520us/step - loss: 0.0554 - acc: 0.9931 - val_loss: 0.4038 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.82200\n",
      "Epoch 27/100\n",
      "4500/4500 [==============================] - 2s 521us/step - loss: 0.0498 - acc: 0.9960 - val_loss: 0.4040 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.82200\n",
      "Epoch 28/100\n",
      "4500/4500 [==============================] - 2s 537us/step - loss: 0.0466 - acc: 0.9958 - val_loss: 0.4016 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.82200\n",
      "Epoch 29/100\n",
      "4500/4500 [==============================] - 2s 523us/step - loss: 0.0425 - acc: 0.9947 - val_loss: 0.4118 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.82200\n",
      "Epoch 30/100\n",
      "4500/4500 [==============================] - 2s 544us/step - loss: 0.0347 - acc: 0.9962 - val_loss: 0.4004 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.82200\n",
      "Epoch 31/100\n",
      "4500/4500 [==============================] - 2s 510us/step - loss: 0.0312 - acc: 0.9962 - val_loss: 0.4026 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.82200\n",
      "Epoch 32/100\n",
      "4500/4500 [==============================] - 2s 524us/step - loss: 0.0277 - acc: 0.9973 - val_loss: 0.4110 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.82200\n",
      "Epoch 33/100\n",
      "4500/4500 [==============================] - 2s 533us/step - loss: 0.0221 - acc: 0.9984 - val_loss: 0.4049 - val_acc: 0.8260\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.82200 to 0.82600, saving model to /tmp/lstm_model.h5\n",
      "Epoch 34/100\n",
      "4500/4500 [==============================] - 2s 520us/step - loss: 0.0194 - acc: 0.9982 - val_loss: 0.4118 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.82600\n",
      "Epoch 35/100\n",
      "4500/4500 [==============================] - 2s 535us/step - loss: 0.0166 - acc: 0.9991 - val_loss: 0.4103 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.82600\n",
      "Epoch 36/100\n",
      "4500/4500 [==============================] - 2s 527us/step - loss: 0.0134 - acc: 0.9993 - val_loss: 0.4126 - val_acc: 0.8280\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.82600 to 0.82800, saving model to /tmp/lstm_model.h5\n",
      "Epoch 37/100\n",
      "4500/4500 [==============================] - 2s 518us/step - loss: 0.0116 - acc: 0.9991 - val_loss: 0.4167 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.82800\n",
      "Epoch 38/100\n",
      "4500/4500 [==============================] - 2s 548us/step - loss: 0.0096 - acc: 0.9998 - val_loss: 0.4250 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.82800\n",
      "Epoch 39/100\n",
      "4500/4500 [==============================] - 2s 521us/step - loss: 0.0180 - acc: 0.9953 - val_loss: 0.4199 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.82800\n",
      "Epoch 40/100\n",
      "4500/4500 [==============================] - 2s 538us/step - loss: 0.0103 - acc: 0.9996 - val_loss: 0.4208 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.82800\n",
      "Epoch 41/100\n",
      "4500/4500 [==============================] - 2s 521us/step - loss: 0.0080 - acc: 0.9998 - val_loss: 0.4228 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.82800\n",
      "Epoch 42/100\n",
      "4500/4500 [==============================] - 2s 516us/step - loss: 0.0073 - acc: 0.9998 - val_loss: 0.4261 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.82800\n",
      "Epoch 43/100\n",
      "4500/4500 [==============================] - 2s 531us/step - loss: 0.0071 - acc: 0.9998 - val_loss: 0.4302 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.82800\n",
      "Epoch 44/100\n",
      "4500/4500 [==============================] - 2s 526us/step - loss: 0.0066 - acc: 0.9998 - val_loss: 0.4385 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.82800\n",
      "Epoch 45/100\n",
      "4500/4500 [==============================] - 2s 530us/step - loss: 0.0074 - acc: 0.9996 - val_loss: 0.4383 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.82800\n",
      "Epoch 46/100\n",
      "4500/4500 [==============================] - 2s 522us/step - loss: 0.0056 - acc: 0.9998 - val_loss: 0.4434 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.82800\n",
      "Epoch 47/100\n",
      "4500/4500 [==============================] - 2s 522us/step - loss: 0.0055 - acc: 0.9998 - val_loss: 0.4381 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.82800\n",
      "Epoch 48/100\n",
      "4500/4500 [==============================] - 2s 537us/step - loss: 0.0049 - acc: 0.9998 - val_loss: 0.4404 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.82800\n",
      "Epoch 49/100\n",
      "4500/4500 [==============================] - 2s 519us/step - loss: 0.0052 - acc: 0.9998 - val_loss: 0.4424 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.82800\n",
      "Epoch 50/100\n",
      "4500/4500 [==============================] - 2s 529us/step - loss: 0.0101 - acc: 0.9971 - val_loss: 0.4336 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.82800\n",
      "Epoch 51/100\n",
      "4500/4500 [==============================] - 2s 535us/step - loss: 0.0076 - acc: 0.9993 - val_loss: 0.4398 - val_acc: 0.8140\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.82800\n",
      "Epoch 52/100\n",
      "4500/4500 [==============================] - 2s 517us/step - loss: 0.0053 - acc: 0.9996 - val_loss: 0.4485 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.82800\n",
      "Epoch 53/100\n",
      "4500/4500 [==============================] - 2s 537us/step - loss: 0.0046 - acc: 0.9998 - val_loss: 0.4494 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.82800\n",
      "Epoch 54/100\n",
      "4500/4500 [==============================] - 2s 522us/step - loss: 0.0045 - acc: 0.9996 - val_loss: 0.4568 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.82800\n",
      "Epoch 55/100\n",
      "4500/4500 [==============================] - 2s 519us/step - loss: 0.0041 - acc: 0.9998 - val_loss: 0.4567 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.82800\n",
      "Epoch 56/100\n",
      "4500/4500 [==============================] - 2s 539us/step - loss: 0.0039 - acc: 0.9998 - val_loss: 0.4561 - val_acc: 0.8100\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.82800\n",
      "Epoch 57/100\n",
      "4500/4500 [==============================] - 2s 519us/step - loss: 0.0036 - acc: 0.9998 - val_loss: 0.4567 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.82800\n",
      "Epoch 58/100\n",
      "4500/4500 [==============================] - 2s 543us/step - loss: 0.0034 - acc: 0.9998 - val_loss: 0.4597 - val_acc: 0.8080\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.82800\n",
      "Epoch 59/100\n",
      "4500/4500 [==============================] - 2s 522us/step - loss: 0.0032 - acc: 0.9998 - val_loss: 0.4616 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.82800\n",
      "Epoch 60/100\n",
      "4500/4500 [==============================] - 2s 524us/step - loss: 0.0033 - acc: 0.9998 - val_loss: 0.4663 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.82800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7e684fbd30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpointer = ModelCheckpoint('/tmp/lstm_model.h5', save_best_only=True, monitor='val_acc', verbose=1)\n",
    "early_stopper = EarlyStopping(monitor='val_loss', patience=30)\n",
    "\n",
    "\n",
    "model.fit([X_train, X_emb_train], y_train, \n",
    "          callbacks=[checkpointer, early_stopper],\n",
    "          validation_data=([X_dev, X_emb_dev], y_dev), epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biGRU + MaxPool1D - Elmo+Embeddings -- \n",
      "\n",
      "\n",
      "Evaluación sobre dev\n",
      "500/500 [==============================] - 0s 271us/step\n",
      "Loss           : 0.4126\n",
      "Accuracy       : 0.8280\n",
      "Precision(1)   : 0.8238\n",
      "Precision(1)   : 0.8310\n",
      "Precision(avg) : 0.8274\n",
      "\n",
      "Recall(1)      : 0.7793\n",
      "Recall(0)      : 0.8669\n",
      "Recall(avg)    : 0.8231\n",
      "\n",
      "F1(1)          : 0.8009\n",
      "F1(0)          : 0.8486\n",
      "F1(avg)        : 0.8248\n",
      "\n",
      "Evaluación sobre test\n",
      "1600/1600 [==============================] - 0s 289us/step\n",
      "Loss           : 0.5544\n",
      "Accuracy       : 0.7388\n",
      "Precision(1)   : 0.6609\n",
      "Precision(1)   : 0.8078\n",
      "Precision(avg) : 0.7343\n",
      "\n",
      "Recall(1)      : 0.7530\n",
      "Recall(0)      : 0.7287\n",
      "Recall(avg)    : 0.7409\n",
      "\n",
      "F1(1)          : 0.7040\n",
      "F1(0)          : 0.7662\n",
      "F1(avg)        : 0.7351\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "print(\"biGRU + MaxPool1D - Elmo+Embeddings -- \\n\\n\")\n",
    "print(\"Evaluación sobre dev\")\n",
    "\n",
    "model.load_weights(checkpointer.filepath)\n",
    "\n",
    "print_evaluation(model, [X_dev, X_emb_dev], y_dev)\n",
    "print(\"\\nEvaluación sobre test\")\n",
    "\n",
    "print_evaluation(model, [X_test, X_emb_test], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Vamos a ver los tweets con mayores errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falsos negativos: 49\n",
      "Falsos positivos: 37\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_true</th>\n",
       "      <th>pred_false</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hs=1</th>\n",
       "      <td>173</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hs=0</th>\n",
       "      <td>37</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pred_true  pred_false\n",
       "real                       \n",
       "hs=1        173          49\n",
       "hs=0         37         241"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev[\"proba\"] = model.predict([X_dev, X_emb_dev])\n",
    "\n",
    "\n",
    "true_positives = df_dev[(df_dev[\"HS\"] == 1) & (df_dev[\"proba\"] >= 0.5)].copy()\n",
    "true_negatives = df_dev[(df_dev[\"HS\"] == 0) & (df_dev[\"proba\"] < 0.5)].copy()\n",
    "\n",
    "false_positives = df_dev[(df_dev[\"HS\"] == 0) & (df_dev[\"proba\"] > 0.5)].copy()\n",
    "false_positives.sort_values(\"proba\", ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "false_negatives = df_dev[(df_dev[\"HS\"] == 1) & (df_dev[\"proba\"] < 0.5)].copy()\n",
    "false_negatives.sort_values(\"proba\", ascending=True, inplace=True)\n",
    "\n",
    "conf_matrix = pd.DataFrame([\n",
    "    {\"real\":\"hs=1\", \"pred_true\": len(true_positives), \"pred_false\": len(false_negatives)},\n",
    "    {\"real\":\"hs=0\", \"pred_true\": len(false_positives), \"pred_false\": len(true_negatives)}\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "conf_matrix.set_index(\"real\", inplace=True)\n",
    "\n",
    "print(\"Falsos negativos: {}\".format(len(false_negatives)))\n",
    "print(\"Falsos positivos: {}\".format(len(false_positives)))\n",
    "\n",
    "conf_matrix[[\"pred_true\", \"pred_false\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proporción de Agresivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proporción de agresivos : 0.648\n",
      "Correlación AG - TR: 0.42040512740887925\n"
     ]
    }
   ],
   "source": [
    "print(\"Proporción de agresivos :\", sum(df_dev[\"AG\"] == 0) / len(df_dev))\n",
    "\n",
    "hs = df_dev[df_dev[\"HS\"] == 1]\n",
    "\n",
    "print(\"Correlación AG - TR:\", hs[\"AG\"].corr(hs[\"TR\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Falsos negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totales =  49\n",
      "No AG (16) AG (33)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>proba</th>\n",
       "      <th>HS</th>\n",
       "      <th>AG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24126</th>\n",
       "      <td>LA SOLUCIÓN ES SOLDARLE EL TIMÓN DE LA PATERA RUMBO DE DONDE SALIERON CON AGUA Y COMIDA. https://t.co/59pInBdgnR</td>\n",
       "      <td>0.022556</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22065</th>\n",
       "      <td>| lo único que dominas es mi polla en tu cara. https://t.co/qlYOd4zPwW</td>\n",
       "      <td>0.023897</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24590</th>\n",
       "      <td>SOMOS MUCHO MÁS DE LO QUE SUEÑAS SER PINCHÉ SUDACA CAGADO, y si no te gusta dime donde nos topamos pa reventarnos el hocico 1:1 @amarchesin1 https://t.co/vHHPBKS1AK</td>\n",
       "      <td>0.028093</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24925</th>\n",
       "      <td>La verdad que si, no veia la hora de llegar a los 5 mil amigos😂 ahora de a poco voy borrando esos indios indocumentados😂 https://t.co/Xw8q4GLM8v</td>\n",
       "      <td>0.048991</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24864</th>\n",
       "      <td>rosalia me podría decir sudaca i wouldn't give a fuck</td>\n",
       "      <td>0.050993</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20307</th>\n",
       "      <td>tu eres lesbi Ana xk no as probaddo mi polla</td>\n",
       "      <td>0.055418</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23688</th>\n",
       "      <td>Ninguna mujer es tan santa como para no aprender a coger bien, ni tan puta como para no aprender a amar.</td>\n",
       "      <td>0.082030</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22560</th>\n",
       "      <td>@CarlaEUrena @FelipeCalderon Callateeeee !! Puta gallina cacarea !! Ni idea tienes de nada .</td>\n",
       "      <td>0.087284</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22008</th>\n",
       "      <td>@Namurii @medicaster_xz @Yorha2d @KeloKlau_ Se merecía de puta y no quiere que le metan la verga, jajaja no mamen</td>\n",
       "      <td>0.088236</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21727</th>\n",
       "      <td>España crea su primer 'campo de refugiados' en la bahía de Algeciras https://t.co/l7Zmh2kvPN vía @elmundoes Gran noticia para el contribuyente español. Españoles a pagar, a pagar. Las pensiones pensad en las pensiones. Esto la sanidad como es gratis podemos dejar de pagarla no?</td>\n",
       "      <td>0.101227</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21841</th>\n",
       "      <td>La otra no noticia del día es la visita del sonriente Casado a Algeciras, a visitar en Ceuta y Algeciras, no a los guardias civiles heridos tras la violenta entrada de ilegales del otro día, sino a un grupo de subsaharianos que han entrado en nuestro país de forma igualmente ileg</td>\n",
       "      <td>0.122259</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22592</th>\n",
       "      <td>o sea tantos arabes q hay en venezuela y no pueden hacer un atentado decente q pasa ?????????</td>\n",
       "      <td>0.136349</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21948</th>\n",
       "      <td>@alaniscristalv Cállate, pa' interés cualquiera so perra</td>\n",
       "      <td>0.142756</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21072</th>\n",
       "      <td>Voy andando por la calle y me han dicho cosas en árabe ya que me estas acosando dime lo en español para que te entienda, asique como yo no les entiendo me he cagado en su puta madre en árabe para que ellos a mi si.</td>\n",
       "      <td>0.145931</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23415</th>\n",
       "      <td>Exacto! #puralata #feminazis #HombreAbusado https://t.co/zTEvaftxcd</td>\n",
       "      <td>0.153192</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20337</th>\n",
       "      <td>Ni armar un complot para matar un presidente saben... #Sudacas..</td>\n",
       "      <td>0.165254</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24401</th>\n",
       "      <td>No habrá que darles de lo 'nuestro'. Ellos se apropiarán de lo 'suyo'.  @Manuj_Hidalgo les dice ☝️ a quienes piensan q hay q jubilar a los mayores para q trabajen los jóvenes o q los q vienen d fuera te quitan el trabajo.  #Inmigración https://t.co/mzDhf2Fdqv vía @voz_opinion</td>\n",
       "      <td>0.168939</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24373</th>\n",
       "      <td>@ccamilitach no te mereces que te preste ninguna malla más por escribirlo así perra 😒jajajajaj</td>\n",
       "      <td>0.169246</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24352</th>\n",
       "      <td>¿Ves que tu coño me importe? Cómete mi polla, basura https://t.co/ht02KyYWZh</td>\n",
       "      <td>0.185124</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23922</th>\n",
       "      <td>Pobre moromierda. Pues a mi me parece mas digna y respetable la vida de una linda gallina que acabará en un KFC que ése semita culturizado en el país de la violación que acaban de  dejar libre https://t.co/lrL1K09aIb</td>\n",
       "      <td>0.206267</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21215</th>\n",
       "      <td>Que tal otro aporte anónimo son de la puta de mi esposa, así los dejó en navidad enseñando le los calzones al vecino rt y siguenos @TANGASUCIA @Tangasrobadas https://t.co/70ioUjveqY</td>\n",
       "      <td>0.207869</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22168</th>\n",
       "      <td>Mi novia es como mi polla. Sé que está ahí porque de vez en cuando se levanta. Una de mis pantalones y la otra de la cama.</td>\n",
       "      <td>0.209680</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21599</th>\n",
       "      <td>Como dicen… Si no tienes algo bueno que decir mejor callate y matate maldita perra arrogante o como era? https://t.co/7dGhOHg6Fs</td>\n",
       "      <td>0.215734</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20213</th>\n",
       "      <td>@britneyspears Perra te lo mereces, esa presentación de WORK BICTH estuvo a tu altura, por fin le hiciste justicia a la canción y al álbum Britney Jean al presentar la canción en un programa de TV luciendo hermosa, acuerpada y subliminal</td>\n",
       "      <td>0.220538</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23545</th>\n",
       "      <td>tú callate mamaguebo que eres rolo e' puta</td>\n",
       "      <td>0.224510</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24536</th>\n",
       "      <td>Eres una perra que no merece a sus amigos — ¿Quieres ser mi amigo/a? Tal vez a ti sí te merezca. https://t.co/NjEcj00Z9a</td>\n",
       "      <td>0.226269</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24822</th>\n",
       "      <td>@Relatofeminista 'A mí sólo me gustan las tías si son guarras'. Para mí está claro que si usa guarra para adjetivar el tipo de mujeres que le gusta no debe de asociarle las mismas connotaciones negativas que presuponéis. Tengo amigas que se autoadjetivan</td>\n",
       "      <td>0.240713</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21417</th>\n",
       "      <td>Q pasa en🇪🇸,Q tenemos sindicatos de manteros ,como se puede estar sindicado sin tener papeles ni pasaportes ni nada Q acrediten de donde son y quién son https://t.co/gzsAZUd8xA</td>\n",
       "      <td>0.247129</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21339</th>\n",
       "      <td>La #Inmigración, como sabemos, es mala, se ha demostrado en media #Europa. Pero no debemos caer en la tentación de salpicar nuestra #soberanía con extremismos, tenemos que defender lo nuestro, si, pero desde posiciones conservadoras y no radicales. Es lo mejor. RP</td>\n",
       "      <td>0.250485</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21152</th>\n",
       "      <td>@NatyBurgos15 @Horaciogenta @PaolaPa05 Cállate vos Black and bitch como la puta falsa abogada. Espero se pudra en la cárcel y devuelva todo. Dedícate a estudiar en serio. La droga es mala. Cristina puta</td>\n",
       "      <td>0.264617</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24488</th>\n",
       "      <td>Un indocumentado...habla y habla y no para y acaba hablando su ignorancia.Empezamos bien. https://t.co/SMRsqjtEBT</td>\n",
       "      <td>0.268349</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20711</th>\n",
       "      <td>@lopezdoriga eso es culpa de los gobiernos corruptos que tienen arreglos con esta escoria. Ojalá se muera la perra esa...</td>\n",
       "      <td>0.269006</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20265</th>\n",
       "      <td>Tema inmigración de Venezuela como estamos y que bebemos hacer ayer mire un programa con abogados pidiendo abrir un estudio con todos ellos pera trabajar en Argentina no es como mucho que antecedentes tienen que hacemos con los nuestros es un tema a mirar antes que nos estalle</td>\n",
       "      <td>0.272477</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23568</th>\n",
       "      <td>Para mi si querés llegar a latinoamerica no suma el discurso argento de nosotros somos descendientes d europ.. Nose un mexicano nos diria para loca somos tos sudakas..</td>\n",
       "      <td>0.278142</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23019</th>\n",
       "      <td>Tengo una sobrina que estaba re mal en química, tenía que dar una prueba global y fui a enseñarle y le fue la zorra, me siento tan bien</td>\n",
       "      <td>0.281681</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20175</th>\n",
       "      <td>Ay Dios cuando se entere Casado de que la ola de calor viene dada por la entrada masiva, sin papeles, de aire subsahariano</td>\n",
       "      <td>0.292687</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22953</th>\n",
       "      <td>Le puedes dar un pez a un hombre y comerá un día Como le des una caña a un subsahariano fabricará una lanza y atravesará a un guardia civil</td>\n",
       "      <td>0.299949</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22660</th>\n",
       "      <td>En esta vida llamada spice girls, yo sería Ginger spice...les explico: No sería la negra, ni la güera ridícula, ni la deportista plana, y mucho menos la posona. Básicamente sería Ginger, por qué es la única que tuvo un éxito como solista. Bendiciones</td>\n",
       "      <td>0.307678</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20019</th>\n",
       "      <td>\"Y NADIE SE HA PREGUNTADO LO QUE LE VA A COSTAR AL HOMBRE DEL GUANTAZO LA SITUACION..?!? PORQUE SEGURO ES, QUE EL MENDA MUSULMONO LE VA A PONER UNA DENUNCIA, QUE EL FALLO VA A SER \"\"CULPABLE\"\", QUE UNA PANDILLA DE MUSULMONOS LE VA A ESTAR ESPERANDO DELANTE DE LA PUERTA DE SU NEGOCIO https://t.co...</td>\n",
       "      <td>0.322538</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21535</th>\n",
       "      <td>#VOX critica duramente a ⁦@pablocasado_⁩ “Les da la bienvenida como si fuera un cartel del welcome de #Carmena “ ¡Expulsión inmediata de moromierdas!  https://t.co/HaySDibj2l</td>\n",
       "      <td>0.332042</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23247</th>\n",
       "      <td>Abro hilo recopilando todos los actos de violencia llevados a cabo por los inmigrantes sin papeles que tanto le gustan a la izquierda. Además, también recopilaré las paguitas que reciben a cambio de nada, puesto que pagan CERO impuestos. Empecemos. #UngaUngaArmy</td>\n",
       "      <td>0.332950</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24083</th>\n",
       "      <td>@alvhann La mujer conoce bien la debilidad del hombre, y cada día le sube más la falda y posteriormente se queja de acoso...</td>\n",
       "      <td>0.336261</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23928</th>\n",
       "      <td>Puta niña mierda, ya cállate a la chingada 😡😡🤯🤯🤯🤯</td>\n",
       "      <td>0.341926</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23341</th>\n",
       "      <td>Sudacas que votan a la derecha, que queres?</td>\n",
       "      <td>0.348049</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21469</th>\n",
       "      <td>Un jugador de mi equipo canta 'mi novia es sevillista y es una puta más' tras el derbi. Pide perdón y la gente le dice que no debe disculparse, que son cánticos normales de la rivalidad. Y luego nos sorprendemos de que pasen las cosas que pasan... https://t.co/MmjIZdi1Xq</td>\n",
       "      <td>0.374766</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21305</th>\n",
       "      <td>\"Tú explicas el patriarcado como \"\"simios negratas\"\" no vengas a exigir nada a las explicaciones de otras cuando se ha explicado desde hace años https://t.co/7VzZRnjNu1\"</td>\n",
       "      <td>0.414359</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22574</th>\n",
       "      <td>yo: quién tú eres tú bizcochitooo (cantando) mi mamá: a la tenés clara puta Jajajajaja me estallo</td>\n",
       "      <td>0.418250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23979</th>\n",
       "      <td>Si eres de los que piensas que una mujer es una mala puta por pensar diferente a ti Vota a ERC PdCAT o CUP</td>\n",
       "      <td>0.433492</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23390</th>\n",
       "      <td>Hola, tarta. Sé bien lo que quieres. Te ví en aquella película. Zorra, cerda, puta. No te mereces que me acueste contigo —Peter Griffin</td>\n",
       "      <td>0.477823</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                              text  \\\n",
       "id                                                                                                                                                                                                                                                                                                                   \n",
       "24126                                                                                                                                                                                             LA SOLUCIÓN ES SOLDARLE EL TIMÓN DE LA PATERA RUMBO DE DONDE SALIERON CON AGUA Y COMIDA. https://t.co/59pInBdgnR   \n",
       "22065                                                                                                                                                                                                                                       | lo único que dominas es mi polla en tu cara. https://t.co/qlYOd4zPwW   \n",
       "24590                                                                                                                                         SOMOS MUCHO MÁS DE LO QUE SUEÑAS SER PINCHÉ SUDACA CAGADO, y si no te gusta dime donde nos topamos pa reventarnos el hocico 1:1 @amarchesin1 https://t.co/vHHPBKS1AK   \n",
       "24925                                                                                                                                                             La verdad que si, no veia la hora de llegar a los 5 mil amigos😂 ahora de a poco voy borrando esos indios indocumentados😂 https://t.co/Xw8q4GLM8v   \n",
       "24864                                                                                                                                                                                                                                                        rosalia me podría decir sudaca i wouldn't give a fuck   \n",
       "20307                                                                                                                                                                                                                                                                 tu eres lesbi Ana xk no as probaddo mi polla   \n",
       "23688                                                                                                                                                                                                     Ninguna mujer es tan santa como para no aprender a coger bien, ni tan puta como para no aprender a amar.   \n",
       "22560                                                                                                                                                                                                                 @CarlaEUrena @FelipeCalderon Callateeeee !! Puta gallina cacarea !! Ni idea tienes de nada .   \n",
       "22008                                                                                                                                                                                            @Namurii @medicaster_xz @Yorha2d @KeloKlau_ Se merecía de puta y no quiere que le metan la verga, jajaja no mamen   \n",
       "21727                       España crea su primer 'campo de refugiados' en la bahía de Algeciras https://t.co/l7Zmh2kvPN vía @elmundoes Gran noticia para el contribuyente español. Españoles a pagar, a pagar. Las pensiones pensad en las pensiones. Esto la sanidad como es gratis podemos dejar de pagarla no?   \n",
       "21841                     La otra no noticia del día es la visita del sonriente Casado a Algeciras, a visitar en Ceuta y Algeciras, no a los guardias civiles heridos tras la violenta entrada de ilegales del otro día, sino a un grupo de subsaharianos que han entrado en nuestro país de forma igualmente ileg   \n",
       "22592                                                                                                                                                                                                                o sea tantos arabes q hay en venezuela y no pueden hacer un atentado decente q pasa ?????????   \n",
       "21948                                                                                                                                                                                                                                                     @alaniscristalv Cállate, pa' interés cualquiera so perra   \n",
       "21072                                                                                       Voy andando por la calle y me han dicho cosas en árabe ya que me estas acosando dime lo en español para que te entienda, asique como yo no les entiendo me he cagado en su puta madre en árabe para que ellos a mi si.   \n",
       "23415                                                                                                                                                                                                                                          Exacto! #puralata #feminazis #HombreAbusado https://t.co/zTEvaftxcd   \n",
       "20337                                                                                                                                                                                                                                             Ni armar un complot para matar un presidente saben... #Sudacas..   \n",
       "24401                         No habrá que darles de lo 'nuestro'. Ellos se apropiarán de lo 'suyo'.  @Manuj_Hidalgo les dice ☝️ a quienes piensan q hay q jubilar a los mayores para q trabajen los jóvenes o q los q vienen d fuera te quitan el trabajo.  #Inmigración https://t.co/mzDhf2Fdqv vía @voz_opinion   \n",
       "24373                                                                                                                                                                                                               @ccamilitach no te mereces que te preste ninguna malla más por escribirlo así perra 😒jajajajaj   \n",
       "24352                                                                                                                                                                                                                                 ¿Ves que tu coño me importe? Cómete mi polla, basura https://t.co/ht02KyYWZh   \n",
       "23922                                                                                     Pobre moromierda. Pues a mi me parece mas digna y respetable la vida de una linda gallina que acabará en un KFC que ése semita culturizado en el país de la violación que acaban de  dejar libre https://t.co/lrL1K09aIb   \n",
       "21215                                                                                                                        Que tal otro aporte anónimo son de la puta de mi esposa, así los dejó en navidad enseñando le los calzones al vecino rt y siguenos @TANGASUCIA @Tangasrobadas https://t.co/70ioUjveqY   \n",
       "22168                                                                                                                                                                                   Mi novia es como mi polla. Sé que está ahí porque de vez en cuando se levanta. Una de mis pantalones y la otra de la cama.   \n",
       "21599                                                                                                                                                                             Como dicen… Si no tienes algo bueno que decir mejor callate y matate maldita perra arrogante o como era? https://t.co/7dGhOHg6Fs   \n",
       "20213                                                                @britneyspears Perra te lo mereces, esa presentación de WORK BICTH estuvo a tu altura, por fin le hiciste justicia a la canción y al álbum Britney Jean al presentar la canción en un programa de TV luciendo hermosa, acuerpada y subliminal   \n",
       "23545                                                                                                                                                                                                                                                                   tú callate mamaguebo que eres rolo e' puta   \n",
       "24536                                                                                                                                                                                     Eres una perra que no merece a sus amigos — ¿Quieres ser mi amigo/a? Tal vez a ti sí te merezca. https://t.co/NjEcj00Z9a   \n",
       "24822                                              @Relatofeminista 'A mí sólo me gustan las tías si son guarras'. Para mí está claro que si usa guarra para adjetivar el tipo de mujeres que le gusta no debe de asociarle las mismas connotaciones negativas que presuponéis. Tengo amigas que se autoadjetivan    \n",
       "21417                                                                                                                             Q pasa en🇪🇸,Q tenemos sindicatos de manteros ,como se puede estar sindicado sin tener papeles ni pasaportes ni nada Q acrediten de donde son y quién son https://t.co/gzsAZUd8xA   \n",
       "21339                                     La #Inmigración, como sabemos, es mala, se ha demostrado en media #Europa. Pero no debemos caer en la tentación de salpicar nuestra #soberanía con extremismos, tenemos que defender lo nuestro, si, pero desde posiciones conservadoras y no radicales. Es lo mejor. RP   \n",
       "21152                                                                                                   @NatyBurgos15 @Horaciogenta @PaolaPa05 Cállate vos Black and bitch como la puta falsa abogada. Espero se pudra en la cárcel y devuelva todo. Dedícate a estudiar en serio. La droga es mala. Cristina puta   \n",
       "24488                                                                                                                                                                                            Un indocumentado...habla y habla y no para y acaba hablando su ignorancia.Empezamos bien. https://t.co/SMRsqjtEBT   \n",
       "20711                                                                                                                                                                                    @lopezdoriga eso es culpa de los gobiernos corruptos que tienen arreglos con esta escoria. Ojalá se muera la perra esa...   \n",
       "20265                        Tema inmigración de Venezuela como estamos y que bebemos hacer ayer mire un programa con abogados pidiendo abrir un estudio con todos ellos pera trabajar en Argentina no es como mucho que antecedentes tienen que hacemos con los nuestros es un tema a mirar antes que nos estalle   \n",
       "23568                                                                                                                                      Para mi si querés llegar a latinoamerica no suma el discurso argento de nosotros somos descendientes d europ.. Nose un mexicano nos diria para loca somos tos sudakas..   \n",
       "23019                                                                                                                                                                      Tengo una sobrina que estaba re mal en química, tenía que dar una prueba global y fui a enseñarle y le fue la zorra, me siento tan bien   \n",
       "20175                                                                                                                                                                                   Ay Dios cuando se entere Casado de que la ola de calor viene dada por la entrada masiva, sin papeles, de aire subsahariano   \n",
       "22953                                                                                                                                                                  Le puedes dar un pez a un hombre y comerá un día Como le des una caña a un subsahariano fabricará una lanza y atravesará a un guardia civil   \n",
       "22660                                                   En esta vida llamada spice girls, yo sería Ginger spice...les explico: No sería la negra, ni la güera ridícula, ni la deportista plana, y mucho menos la posona. Básicamente sería Ginger, por qué es la única que tuvo un éxito como solista. Bendiciones   \n",
       "20019  \"Y NADIE SE HA PREGUNTADO LO QUE LE VA A COSTAR AL HOMBRE DEL GUANTAZO LA SITUACION..?!? PORQUE SEGURO ES, QUE EL MENDA MUSULMONO LE VA A PONER UNA DENUNCIA, QUE EL FALLO VA A SER \"\"CULPABLE\"\", QUE UNA PANDILLA DE MUSULMONOS LE VA A ESTAR ESPERANDO DELANTE DE LA PUERTA DE SU NEGOCIO https://t.co...   \n",
       "21535                                                                                                                               #VOX critica duramente a ⁦@pablocasado_⁩ “Les da la bienvenida como si fuera un cartel del welcome de #Carmena “ ¡Expulsión inmediata de moromierdas!  https://t.co/HaySDibj2l   \n",
       "23247                                       Abro hilo recopilando todos los actos de violencia llevados a cabo por los inmigrantes sin papeles que tanto le gustan a la izquierda. Además, también recopilaré las paguitas que reciben a cambio de nada, puesto que pagan CERO impuestos. Empecemos. #UngaUngaArmy   \n",
       "24083                                                                                                                                                                                 @alvhann La mujer conoce bien la debilidad del hombre, y cada día le sube más la falda y posteriormente se queja de acoso...   \n",
       "23928                                                                                                                                                                                                                                                            Puta niña mierda, ya cállate a la chingada 😡😡🤯🤯🤯🤯   \n",
       "23341                                                                                                                                                                                                                                                                  Sudacas que votan a la derecha, que queres?   \n",
       "21469                              Un jugador de mi equipo canta 'mi novia es sevillista y es una puta más' tras el derbi. Pide perdón y la gente le dice que no debe disculparse, que son cánticos normales de la rivalidad. Y luego nos sorprendemos de que pasen las cosas que pasan... https://t.co/MmjIZdi1Xq   \n",
       "21305                                                                                                                                    \"Tú explicas el patriarcado como \"\"simios negratas\"\" no vengas a exigir nada a las explicaciones de otras cuando se ha explicado desde hace años https://t.co/7VzZRnjNu1\"   \n",
       "22574                                                                                                                                                                                                            yo: quién tú eres tú bizcochitooo (cantando) mi mamá: a la tenés clara puta Jajajajaja me estallo   \n",
       "23979                                                                                                                                                                                                   Si eres de los que piensas que una mujer es una mala puta por pensar diferente a ti Vota a ERC PdCAT o CUP   \n",
       "23390                                                                                                                                                                      Hola, tarta. Sé bien lo que quieres. Te ví en aquella película. Zorra, cerda, puta. No te mereces que me acueste contigo —Peter Griffin   \n",
       "\n",
       "          proba  HS  AG  \n",
       "id                       \n",
       "24126  0.022556   1   0  \n",
       "22065  0.023897   1   1  \n",
       "24590  0.028093   1   1  \n",
       "24925  0.048991   1   0  \n",
       "24864  0.050993   1   1  \n",
       "20307  0.055418   1   1  \n",
       "23688  0.082030   1   1  \n",
       "22560  0.087284   1   1  \n",
       "22008  0.088236   1   0  \n",
       "21727  0.101227   1   0  \n",
       "21841  0.122259   1   1  \n",
       "22592  0.136349   1   0  \n",
       "21948  0.142756   1   1  \n",
       "21072  0.145931   1   1  \n",
       "23415  0.153192   1   1  \n",
       "20337  0.165254   1   1  \n",
       "24401  0.168939   1   1  \n",
       "24373  0.169246   1   1  \n",
       "24352  0.185124   1   1  \n",
       "23922  0.206267   1   1  \n",
       "21215  0.207869   1   1  \n",
       "22168  0.209680   1   0  \n",
       "21599  0.215734   1   1  \n",
       "20213  0.220538   1   1  \n",
       "23545  0.224510   1   1  \n",
       "24536  0.226269   1   1  \n",
       "24822  0.240713   1   0  \n",
       "21417  0.247129   1   0  \n",
       "21339  0.250485   1   1  \n",
       "21152  0.264617   1   1  \n",
       "24488  0.268349   1   0  \n",
       "20711  0.269006   1   1  \n",
       "20265  0.272477   1   0  \n",
       "23568  0.278142   1   1  \n",
       "23019  0.281681   1   1  \n",
       "20175  0.292687   1   0  \n",
       "22953  0.299949   1   0  \n",
       "22660  0.307678   1   1  \n",
       "20019  0.322538   1   1  \n",
       "21535  0.332042   1   1  \n",
       "23247  0.332950   1   0  \n",
       "24083  0.336261   1   0  \n",
       "23928  0.341926   1   1  \n",
       "23341  0.348049   1   1  \n",
       "21469  0.374766   1   0  \n",
       "21305  0.414359   1   1  \n",
       "22574  0.418250   1   1  \n",
       "23979  0.433492   1   0  \n",
       "23390  0.477823   1   1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Totales = \", len(false_negatives))\n",
    "print(\"No AG ({}) AG ({})\".format(sum(false_negatives[\"AG\"] == 0), sum(false_negatives[\"AG\"] == 1)))\n",
    "false_negatives[[\"text\", \"proba\", \"HS\", \"AG\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Falsos Positivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totales =  37\n",
      "No AG (37) AG (0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>proba</th>\n",
       "      <th>HS</th>\n",
       "      <th>AG</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22457</th>\n",
       "      <td>De Colombia tocará emigrar en pateras, como hacen los africanos subsaharianos.</td>\n",
       "      <td>0.984295</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20376</th>\n",
       "      <td>Zorra tu abuela, imbécil.</td>\n",
       "      <td>0.953888</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21892</th>\n",
       "      <td>@saancisco Callate puta tu eres el traidor.</td>\n",
       "      <td>0.953812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21497</th>\n",
       "      <td>Talvez quede como zorra pero te lo mereces Socia😎</td>\n",
       "      <td>0.944706</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20148</th>\n",
       "      <td>Racismo es que hagas enfrijoladas con frijoles güeros y no negros.</td>\n",
       "      <td>0.942656</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24402</th>\n",
       "      <td>zorra eres tu que no te sientes hombre y no entiendes que toda mujer es bella</td>\n",
       "      <td>0.934405</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22014</th>\n",
       "      <td>Ni tú eres una zorra hambrienta ni yo un incorregible depredador.</td>\n",
       "      <td>0.926217</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22416</th>\n",
       "      <td>@Pepe_ElEzequiel Puta!? Puta con las que vos te metes Esa mujer es un ángel enviado por Jesucristo nuestro señor para darnos esa paz que buscamos sólo con verla</td>\n",
       "      <td>0.896160</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22115</th>\n",
       "      <td>Una chica no es una guarra, una zorra, una puta, una buscan por su manera de vestir.</td>\n",
       "      <td>0.886693</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24209</th>\n",
       "      <td>¿Ya les he contado que si fuera rica/millonaria sería toda una PERRA?, Yisus me está enseñando humildad... Y no he aprendido mucho ldvdd.</td>\n",
       "      <td>0.881671</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24172</th>\n",
       "      <td>Y los que no se denuncian... hagamos números. El acoso a las mujeres es diario. No todos los hombres son acosadores, pero todas las mujeres han sido acosadas. https://t.co/jza9llhqWA</td>\n",
       "      <td>0.874724</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21255</th>\n",
       "      <td>Y tú eres una maldita escoria con una repugnante inclinación por los hombres.. ¡Seres como tú me producen náuseas! https://t.co/TaHDcpZdnm</td>\n",
       "      <td>0.864138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22826</th>\n",
       "      <td>Me tienen arta con decirme puta, que no disfruten el sexo no es un drama mío ridículas</td>\n",
       "      <td>0.839998</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22517</th>\n",
       "      <td>-¿eres poblana? -soy colombiana -¿y por qué eres güera? en Colombia TODAS son morenas, fui a Colombia 3 veces. -ok, mentí 🙄 solo me pinto el cabello</td>\n",
       "      <td>0.839244</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22938</th>\n",
       "      <td>Masculinidad frágil es mi crush francés echándose un clavado al cenote solo porque un grupo de árabes le dijo pussy</td>\n",
       "      <td>0.828573</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20952</th>\n",
       "      <td>@NicolasMaduro Callate la jeta perra que de este año no pasas</td>\n",
       "      <td>0.824774</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24529</th>\n",
       "      <td>@desoir2525 Cállate rata MUDera.eres escoria.</td>\n",
       "      <td>0.817007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21920</th>\n",
       "      <td>Se ha muerto mi perra y no estoy para aguantar tonterías.</td>\n",
       "      <td>0.762498</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23086</th>\n",
       "      <td>@Brigitte_Grey Perra /\\ te enseñaré lo que es el nuevo orden mundial</td>\n",
       "      <td>0.760542</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24019</th>\n",
       "      <td>Acá nos huevean si nos encapuchamos pero en los paises árabes estariamos todas obligadas a encapucharnos 🤷‍♀️</td>\n",
       "      <td>0.750286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20210</th>\n",
       "      <td>@adrispears @7santy Tú eres la puta Beyoncé siempre, querido ❤</td>\n",
       "      <td>0.733948</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22848</th>\n",
       "      <td>Pinche bebé pendejo cállate el hocico, nomás te están cortando el pelo. Puta chillona</td>\n",
       "      <td>0.731091</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22575</th>\n",
       "      <td>Qué basura de vecinos, asquerosos, mezquinos e hijos de la gran puta que tienen que inventarse que el vecino hace ruido y van y llaman a la CNP para que se presenten aquí, en mi puta puerta, hace una media hora porque les apetece molestar al vecino de arriba.</td>\n",
       "      <td>0.702483</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23021</th>\n",
       "      <td>Creo que los colegios que impartan el Islam debe de ser el mismo numero de co!egiosque en los países árabes  imparten el cristianismo.Que no somos más tontos porque no ensayamos https://t.co/u9PV1noorb</td>\n",
       "      <td>0.693426</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23912</th>\n",
       "      <td>@RoyBarreras mira que tu eres bandido y criminal al igual que santos mas temprano que tarde pagaran sus crimenes escoria basura</td>\n",
       "      <td>0.684532</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21938</th>\n",
       "      <td>Me agregó un bicho random de Libya para hablar. El bicho me preguntó si me gusta viajar y me cuenta que él ha viajado por casi todos los países árabes, yo le digo que para allá no voy porq matan gays.</td>\n",
       "      <td>0.674498</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21077</th>\n",
       "      <td>Para cuando pones a Maduro en l frontera? Q se sepa el esta indocumentado ya que no tiene PdN legitima. L q mostro Tibitrampa es un fraude ya q la mama dicese venezolana, pero ella nunca se nacionalizo. Cuando cumples el encargo maleante? https://t.co/Xibew53U7l</td>\n",
       "      <td>0.672623</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21155</th>\n",
       "      <td>@diegocruzva @lila_tresmil @Relatofeminista Como coño se va a resistir la pobre chica con 3 tios violabdola DESGRACIDO !!!!!!!!!!!!!!!!!!!!!!!!!! No eres mejor que ellos para nada, verguenza te debería dar! Verguenza!!!!!!!¡!!!!!!!!</td>\n",
       "      <td>0.672022</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22889</th>\n",
       "      <td>@elajidetuvida .El acoso y la violación tiene connotaciones muy prifundas. El poder económico de estos acosadores tiene el mismo efecto que un cuchillo en el cuello por violadores. Los poderosos se aprovechan de momentos vulnerables de sus victimas,ahora</td>\n",
       "      <td>0.671183</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20935</th>\n",
       "      <td>@Ro_Valdess LA PUTA CALLATE QUE QUIERO TENER EL PELO NARANJAAA Y ME VA A QUEDAR HORRIBLE PORQUE SOY NEGRA</td>\n",
       "      <td>0.661808</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22815</th>\n",
       "      <td>Novia me hizo papas fritas esta mujer es todo lo que esta bien la puta madreeeeeeeeeee</td>\n",
       "      <td>0.658629</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22152</th>\n",
       "      <td>\"Ya que el \"\"residente\"\" del gobierno @sanchezcastejon no quiere ayudar a la @guardiacivil en laa fronteras, propongo hacer una colecta para que todo español que quiera y pueda, viaje a melilla y rodee la valla. Si nos invaden que sea con igualdad de fuerzas. #FelizSábado\"</td>\n",
       "      <td>0.640216</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24115</th>\n",
       "      <td>Chica, pero por qué  borras las fotos en Cholon, en yates? Si al final la frijolera  que todos los fines de semana revuelven y hasta estrujan  es la tuya y nadie debe opinar en eso... Tranquila deje sus fotos.</td>\n",
       "      <td>0.634816</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22846</th>\n",
       "      <td>Si a mi se me ocurre enseñar por twitter una conversación de whatsap con mi madre me manda a dormir a la caseta de mi perra.</td>\n",
       "      <td>0.627479</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22005</th>\n",
       "      <td>árabe para que la pussy lo baile 😈 @papichampok @_eckoyg</td>\n",
       "      <td>0.624119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20127</th>\n",
       "      <td>Una tía no es una guarra por hacer una foto de su culo y tú eres un puto nazi que no sé qué hace respirando. https://t.co/Frx1dv3VsN</td>\n",
       "      <td>0.598446</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23565</th>\n",
       "      <td>La solución no es darle papeles a todos. Es exigir a nuestro gobierno q no fabrique y venda armas a los gobiernos subsaharianos para que estos no masacren y acorralen a su población haciendo q quieran venir aquí x lo civil o criminal. Desesperación y supervivencia pura.</td>\n",
       "      <td>0.524922</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                    text  \\\n",
       "id                                                                                                                                                                                                                                                                                         \n",
       "22457                                                                                                                                                                                                     De Colombia tocará emigrar en pateras, como hacen los africanos subsaharianos.   \n",
       "20376                                                                                                                                                                                                                                                          Zorra tu abuela, imbécil.   \n",
       "21892                                                                                                                                                                                                                                        @saancisco Callate puta tu eres el traidor.   \n",
       "21497                                                                                                                                                                                                                                  Talvez quede como zorra pero te lo mereces Socia😎   \n",
       "20148                                                                                                                                                                                                                 Racismo es que hagas enfrijoladas con frijoles güeros y no negros.   \n",
       "24402                                                                                                                                                                                                      zorra eres tu que no te sientes hombre y no entiendes que toda mujer es bella   \n",
       "22014                                                                                                                                                                                                                  Ni tú eres una zorra hambrienta ni yo un incorregible depredador.   \n",
       "22416                                                                                                                   @Pepe_ElEzequiel Puta!? Puta con las que vos te metes Esa mujer es un ángel enviado por Jesucristo nuestro señor para darnos esa paz que buscamos sólo con verla   \n",
       "22115                                                                                                                                                                                               Una chica no es una guarra, una zorra, una puta, una buscan por su manera de vestir.   \n",
       "24209                                                                                                                                          ¿Ya les he contado que si fuera rica/millonaria sería toda una PERRA?, Yisus me está enseñando humildad... Y no he aprendido mucho ldvdd.   \n",
       "24172                                                                                             Y los que no se denuncian... hagamos números. El acoso a las mujeres es diario. No todos los hombres son acosadores, pero todas las mujeres han sido acosadas. https://t.co/jza9llhqWA   \n",
       "21255                                                                                                                                         Y tú eres una maldita escoria con una repugnante inclinación por los hombres.. ¡Seres como tú me producen náuseas! https://t.co/TaHDcpZdnm   \n",
       "22826                                                                                                                                                                                             Me tienen arta con decirme puta, que no disfruten el sexo no es un drama mío ridículas   \n",
       "22517                                                                                                                               -¿eres poblana? -soy colombiana -¿y por qué eres güera? en Colombia TODAS son morenas, fui a Colombia 3 veces. -ok, mentí 🙄 solo me pinto el cabello   \n",
       "22938                                                                                                                                                                Masculinidad frágil es mi crush francés echándose un clavado al cenote solo porque un grupo de árabes le dijo pussy   \n",
       "20952                                                                                                                                                                                                                      @NicolasMaduro Callate la jeta perra que de este año no pasas   \n",
       "24529                                                                                                                                                                                                                                      @desoir2525 Cállate rata MUDera.eres escoria.   \n",
       "21920                                                                                                                                                                                                                          Se ha muerto mi perra y no estoy para aguantar tonterías.   \n",
       "23086                                                                                                                                                                                                               @Brigitte_Grey Perra /\\ te enseñaré lo que es el nuevo orden mundial   \n",
       "24019                                                                                                                                                                      Acá nos huevean si nos encapuchamos pero en los paises árabes estariamos todas obligadas a encapucharnos 🤷‍♀️   \n",
       "20210                                                                                                                                                                                                                     @adrispears @7santy Tú eres la puta Beyoncé siempre, querido ❤   \n",
       "22848                                                                                                                                                                                              Pinche bebé pendejo cállate el hocico, nomás te están cortando el pelo. Puta chillona   \n",
       "22575                Qué basura de vecinos, asquerosos, mezquinos e hijos de la gran puta que tienen que inventarse que el vecino hace ruido y van y llaman a la CNP para que se presenten aquí, en mi puta puerta, hace una media hora porque les apetece molestar al vecino de arriba.   \n",
       "23021                                                                          Creo que los colegios que impartan el Islam debe de ser el mismo numero de co!egiosque en los países árabes  imparten el cristianismo.Que no somos más tontos porque no ensayamos https://t.co/u9PV1noorb   \n",
       "23912                                                                                                                                                    @RoyBarreras mira que tu eres bandido y criminal al igual que santos mas temprano que tarde pagaran sus crimenes escoria basura   \n",
       "21938                                                                           Me agregó un bicho random de Libya para hablar. El bicho me preguntó si me gusta viajar y me cuenta que él ha viajado por casi todos los países árabes, yo le digo que para allá no voy porq matan gays.   \n",
       "21077             Para cuando pones a Maduro en l frontera? Q se sepa el esta indocumentado ya que no tiene PdN legitima. L q mostro Tibitrampa es un fraude ya q la mama dicese venezolana, pero ella nunca se nacionalizo. Cuando cumples el encargo maleante? https://t.co/Xibew53U7l   \n",
       "21155                                           @diegocruzva @lila_tresmil @Relatofeminista Como coño se va a resistir la pobre chica con 3 tios violabdola DESGRACIDO !!!!!!!!!!!!!!!!!!!!!!!!!! No eres mejor que ellos para nada, verguenza te debería dar! Verguenza!!!!!!!¡!!!!!!!!   \n",
       "22889                    @elajidetuvida .El acoso y la violación tiene connotaciones muy prifundas. El poder económico de estos acosadores tiene el mismo efecto que un cuchillo en el cuello por violadores. Los poderosos se aprovechan de momentos vulnerables de sus victimas,ahora    \n",
       "20935                                                                                                                                                                          @Ro_Valdess LA PUTA CALLATE QUE QUIERO TENER EL PELO NARANJAAA Y ME VA A QUEDAR HORRIBLE PORQUE SOY NEGRA   \n",
       "22815                                                                                                                                                                                             Novia me hizo papas fritas esta mujer es todo lo que esta bien la puta madreeeeeeeeeee   \n",
       "22152  \"Ya que el \"\"residente\"\" del gobierno @sanchezcastejon no quiere ayudar a la @guardiacivil en laa fronteras, propongo hacer una colecta para que todo español que quiera y pueda, viaje a melilla y rodee la valla. Si nos invaden que sea con igualdad de fuerzas. #FelizSábado\"   \n",
       "24115                                                                  Chica, pero por qué  borras las fotos en Cholon, en yates? Si al final la frijolera  que todos los fines de semana revuelven y hasta estrujan  es la tuya y nadie debe opinar en eso... Tranquila deje sus fotos.   \n",
       "22846                                                                                                                                                       Si a mi se me ocurre enseñar por twitter una conversación de whatsap con mi madre me manda a dormir a la caseta de mi perra.   \n",
       "22005                                                                                                                                                                                                                           árabe para que la pussy lo baile 😈 @papichampok @_eckoyg   \n",
       "20127                                                                                                                                               Una tía no es una guarra por hacer una foto de su culo y tú eres un puto nazi que no sé qué hace respirando. https://t.co/Frx1dv3VsN   \n",
       "23565     La solución no es darle papeles a todos. Es exigir a nuestro gobierno q no fabrique y venda armas a los gobiernos subsaharianos para que estos no masacren y acorralen a su población haciendo q quieran venir aquí x lo civil o criminal. Desesperación y supervivencia pura.   \n",
       "\n",
       "          proba  HS  AG  \n",
       "id                       \n",
       "22457  0.984295   0   0  \n",
       "20376  0.953888   0   0  \n",
       "21892  0.953812   0   0  \n",
       "21497  0.944706   0   0  \n",
       "20148  0.942656   0   0  \n",
       "24402  0.934405   0   0  \n",
       "22014  0.926217   0   0  \n",
       "22416  0.896160   0   0  \n",
       "22115  0.886693   0   0  \n",
       "24209  0.881671   0   0  \n",
       "24172  0.874724   0   0  \n",
       "21255  0.864138   0   0  \n",
       "22826  0.839998   0   0  \n",
       "22517  0.839244   0   0  \n",
       "22938  0.828573   0   0  \n",
       "20952  0.824774   0   0  \n",
       "24529  0.817007   0   0  \n",
       "21920  0.762498   0   0  \n",
       "23086  0.760542   0   0  \n",
       "24019  0.750286   0   0  \n",
       "20210  0.733948   0   0  \n",
       "22848  0.731091   0   0  \n",
       "22575  0.702483   0   0  \n",
       "23021  0.693426   0   0  \n",
       "23912  0.684532   0   0  \n",
       "21938  0.674498   0   0  \n",
       "21077  0.672623   0   0  \n",
       "21155  0.672022   0   0  \n",
       "22889  0.671183   0   0  \n",
       "20935  0.661808   0   0  \n",
       "22815  0.658629   0   0  \n",
       "22152  0.640216   0   0  \n",
       "24115  0.634816   0   0  \n",
       "22846  0.627479   0   0  \n",
       "22005  0.624119   0   0  \n",
       "20127  0.598446   0   0  \n",
       "23565  0.524922   0   0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 300)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Totales = \", len(false_positives))\n",
    "print(\"No AG ({}) AG ({})\".format(sum(false_positives[\"AG\"] == 0), sum(false_positives[\"AG\"] == 1)))\n",
    "false_positives[[\"text\", \"proba\", \"HS\", \"AG\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM  + ElMO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instancias de entrenamiento: 4500\n",
      "Instancias de desarrollo: 500\n",
      "Instancias de test: 1600\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import torch\n",
    "\n",
    "seed = \n",
    "\n",
    "torch.manual_seed(2019)\n",
    "np.random.seed(2019)\n",
    "tf.random.set_random_seed(2019)\n",
    "random.seed(2019)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "df_dev = pd.read_table(\"../../../data/es/dev_es.tsv\", index_col=\"id\", quoting=csv.QUOTE_NONE)\n",
    "df_train = pd.read_table(\"../../../data/es/train_es.tsv\", index_col=\"id\", quoting=csv.QUOTE_NONE)\n",
    "df_test = pd.read_table(\"../../../data/es/reference_es.tsv\", header=None, \n",
    "                        names=[\"text\", \"HS\", \"TR\", \"AG\"], quoting=csv.QUOTE_NONE)\n",
    "\n",
    "\n",
    "text_train, y_train = df_train[\"text\"], df_train[\"HS\"]\n",
    "text_dev, y_dev = df_dev[\"text\"], df_dev[\"HS\"]\n",
    "text_test, y_test = df_test[\"text\"], df_test[\"HS\"]\n",
    "\n",
    "print(\"Instancias de entrenamiento: {}\".format(len(df_train)))\n",
    "print(\"Instancias de desarrollo: {}\".format(len(df_dev)))\n",
    "print(\"Instancias de test: {}\".format(len(df_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tengo que hacer dos cosas:\n",
    "\n",
    "- Primero, convertir los tweets a secuencias de texto\n",
    "- Luego, paddear las secuencias a cierta longitud (Keras necesita esto para poder paralelizar c√°lculo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "max_length = 30\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    if len(tokens) >= max_length:\n",
    "        tokens = tokens[:max_length]\n",
    "    else:\n",
    "        tokens = tokens + [''] * (max_length - len(tokens))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "text_train = [preprocess_tweet(tweet) for tweet in df_train[\"text\"].values]\n",
    "text_dev = [preprocess_tweet(tweet) for tweet in df_dev[\"text\"].values]\n",
    "text_test = [preprocess_tweet(tweet) for tweet in df_test[\"text\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from elmoformanylangs import Embedder\n",
    "\n",
    "e = Embedder(\"../../../models/elmo/es/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carguemos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['easyjet', 'quiere', 'duplicar', 'el', 'n√∫mero', 'de', 'mujeres', 'piloto', \"'\", 'ver√°s', 't√∫', 'para', 'aparcar', 'el', 'avi√≥n', '..', 'http://t.co/46NuLkm09x', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(text_train[0])\n",
    "\n",
    "\n",
    "X_train = np.array(e.sents2elmo(text_train))\n",
    "X_dev = np.array(e.sents2elmo(text_dev))\n",
    "X_test = np.array(e.sents2elmo(text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4500, 30, 1024), (500, 30, 1024), (1600, 30, 1024), (4500,), (500,), (1600,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_dev.shape, X_test.shape, y_train.shape, y_dev.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnngru_4 (CuDNNGRU)       (None, 256)               984576    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,050,625\n",
      "Trainable params: 1,050,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "4500/4500 [==============================] - 3s 591us/step - loss: 0.8773 - acc: 0.5462 - val_loss: 0.6861 - val_acc: 0.6120\n",
      "Epoch 2/30\n",
      "4500/4500 [==============================] - 2s 349us/step - loss: 0.7320 - acc: 0.5491 - val_loss: 0.6837 - val_acc: 0.5560\n",
      "Epoch 3/30\n",
      "4500/4500 [==============================] - 2s 356us/step - loss: 0.6973 - acc: 0.5684 - val_loss: 0.6848 - val_acc: 0.5560\n",
      "Epoch 4/30\n",
      "4500/4500 [==============================] - 2s 355us/step - loss: 0.6786 - acc: 0.5916 - val_loss: 0.6798 - val_acc: 0.5620\n",
      "Epoch 5/30\n",
      "4500/4500 [==============================] - 2s 353us/step - loss: 0.6649 - acc: 0.6011 - val_loss: 0.6741 - val_acc: 0.6160\n",
      "Epoch 6/30\n",
      "4500/4500 [==============================] - 2s 355us/step - loss: 0.6569 - acc: 0.6111 - val_loss: 0.6672 - val_acc: 0.6640\n",
      "Epoch 7/30\n",
      "4500/4500 [==============================] - 2s 354us/step - loss: 0.6389 - acc: 0.6407 - val_loss: 0.6530 - val_acc: 0.6980\n",
      "Epoch 8/30\n",
      "4500/4500 [==============================] - 2s 354us/step - loss: 0.6184 - acc: 0.6598 - val_loss: 0.6277 - val_acc: 0.7020\n",
      "Epoch 9/30\n",
      "4500/4500 [==============================] - 2s 356us/step - loss: 0.5889 - acc: 0.6958 - val_loss: 0.5893 - val_acc: 0.7320\n",
      "Epoch 10/30\n",
      "4500/4500 [==============================] - 2s 355us/step - loss: 0.5621 - acc: 0.7098 - val_loss: 0.5526 - val_acc: 0.7500\n",
      "Epoch 11/30\n",
      "4500/4500 [==============================] - 2s 356us/step - loss: 0.5292 - acc: 0.7376 - val_loss: 0.5195 - val_acc: 0.7600\n",
      "Epoch 12/30\n",
      "4500/4500 [==============================] - 2s 358us/step - loss: 0.5135 - acc: 0.7507 - val_loss: 0.4946 - val_acc: 0.7720\n",
      "Epoch 13/30\n",
      "4500/4500 [==============================] - 2s 355us/step - loss: 0.4809 - acc: 0.7780 - val_loss: 0.4751 - val_acc: 0.7800\n",
      "Epoch 14/30\n",
      "4500/4500 [==============================] - 2s 354us/step - loss: 0.4758 - acc: 0.7831 - val_loss: 0.4636 - val_acc: 0.7920\n",
      "Epoch 15/30\n",
      "4500/4500 [==============================] - 2s 357us/step - loss: 0.4363 - acc: 0.8044 - val_loss: 0.4511 - val_acc: 0.7820\n",
      "Epoch 16/30\n",
      "4500/4500 [==============================] - 2s 351us/step - loss: 0.4373 - acc: 0.8060 - val_loss: 0.4467 - val_acc: 0.7820\n",
      "Epoch 17/30\n",
      "4500/4500 [==============================] - 2s 355us/step - loss: 0.4177 - acc: 0.8284 - val_loss: 0.4414 - val_acc: 0.7880\n",
      "Epoch 18/30\n",
      "4500/4500 [==============================] - 2s 353us/step - loss: 0.3916 - acc: 0.8380 - val_loss: 0.4330 - val_acc: 0.8020\n",
      "Epoch 19/30\n",
      "4500/4500 [==============================] - 2s 356us/step - loss: 0.3878 - acc: 0.8404 - val_loss: 0.4426 - val_acc: 0.7880\n",
      "Epoch 20/30\n",
      "4500/4500 [==============================] - 2s 357us/step - loss: 0.3648 - acc: 0.8564 - val_loss: 0.4317 - val_acc: 0.7900\n",
      "Epoch 21/30\n",
      "4500/4500 [==============================] - 2s 357us/step - loss: 0.3614 - acc: 0.8584 - val_loss: 0.4266 - val_acc: 0.7860\n",
      "Epoch 22/30\n",
      "4500/4500 [==============================] - 2s 355us/step - loss: 0.3373 - acc: 0.8716 - val_loss: 0.4225 - val_acc: 0.7920\n",
      "Epoch 23/30\n",
      "4500/4500 [==============================] - 2s 358us/step - loss: 0.3145 - acc: 0.8773 - val_loss: 0.4215 - val_acc: 0.8020\n",
      "Epoch 24/30\n",
      "4500/4500 [==============================] - 2s 352us/step - loss: 0.3013 - acc: 0.8867 - val_loss: 0.4309 - val_acc: 0.8000\n",
      "Epoch 25/30\n",
      "4500/4500 [==============================] - 2s 358us/step - loss: 0.2806 - acc: 0.9038 - val_loss: 0.4414 - val_acc: 0.8020\n",
      "Epoch 26/30\n",
      "4500/4500 [==============================] - 2s 357us/step - loss: 0.2649 - acc: 0.9091 - val_loss: 0.4234 - val_acc: 0.8040\n",
      "Epoch 27/30\n",
      "4500/4500 [==============================] - 2s 357us/step - loss: 0.2530 - acc: 0.9102 - val_loss: 0.4526 - val_acc: 0.7960\n",
      "Epoch 28/30\n",
      "4500/4500 [==============================] - 2s 354us/step - loss: 0.2318 - acc: 0.9276 - val_loss: 0.4442 - val_acc: 0.8100\n",
      "Epoch 29/30\n",
      "4500/4500 [==============================] - 2s 358us/step - loss: 0.2074 - acc: 0.9320 - val_loss: 0.4515 - val_acc: 0.8100\n",
      "Epoch 30/30\n",
      "4500/4500 [==============================] - 2s 352us/step - loss: 0.1989 - acc: 0.9464 - val_loss: 0.4675 - val_acc: 0.8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6fe04577b8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, CuDNNLSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "embedding_dim = 1024\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.002,\n",
    "    \"decay\": 0.01\n",
    "}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(CuDNNGRU(256, input_shape=(max_length, embedding_dim)))\n",
    "model.add(Dropout(0.80))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.55))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=30, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU-LSTM -- \n",
      "\n",
      "\n",
      "Evaluaci√≥n sobre dev\n",
      "500/500 [==============================] - 0s 183us/step\n",
      "Loss           : 0.4675\n",
      "Accuracy       : 0.8080\n",
      "Precision(1)   : 0.7864\n",
      "Precision(1)   : 0.8250\n",
      "Precision(avg) : 0.8057\n",
      "\n",
      "Recall(1)      : 0.7793\n",
      "Recall(0)      : 0.8309\n",
      "Recall(avg)    : 0.8051\n",
      "\n",
      "F1(1)          : 0.7828\n",
      "F1(0)          : 0.8280\n",
      "F1(avg)        : 0.8054\n",
      "\n",
      "\n",
      "Evaluaci√≥n sobre test\n",
      "1600/1600 [==============================] - 0s 156us/step\n",
      "Loss           : 0.7042\n",
      "Accuracy       : 0.7288\n",
      "Precision(1)   : 0.6544\n",
      "Precision(1)   : 0.7915\n",
      "Precision(avg) : 0.7229\n",
      "\n",
      "Recall(1)      : 0.7258\n",
      "Recall(0)      : 0.7309\n",
      "Recall(avg)    : 0.7283\n",
      "\n",
      "F1(1)          : 0.6882\n",
      "F1(0)          : 0.7600\n",
      "F1(avg)        : 0.7241\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "print(\"GRU-LSTM -- \\n\\n\")\n",
    "print(\"Evaluaci√≥n sobre dev\")\n",
    "print_evaluation(model, X_dev, y_dev)\n",
    "print(\"\\n\\nEvaluaci√≥n sobre test\")\n",
    "print_evaluation(model, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "4500/4500 [==============================] - 3s 745us/step - loss: 0.8857 - acc: 0.5413 - val_loss: 0.6754 - val_acc: 0.5780\n",
      "Epoch 2/25\n",
      "4500/4500 [==============================] - 2s 434us/step - loss: 0.7046 - acc: 0.5896 - val_loss: 0.6659 - val_acc: 0.5820\n",
      "Epoch 3/25\n",
      "4500/4500 [==============================] - 2s 433us/step - loss: 0.6617 - acc: 0.6324 - val_loss: 0.6553 - val_acc: 0.6300\n",
      "Epoch 4/25\n",
      "4500/4500 [==============================] - 2s 443us/step - loss: 0.6209 - acc: 0.6591 - val_loss: 0.6208 - val_acc: 0.7220\n",
      "Epoch 5/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.5852 - acc: 0.6969 - val_loss: 0.5880 - val_acc: 0.7340\n",
      "Epoch 6/25\n",
      "4500/4500 [==============================] - 2s 436us/step - loss: 0.5510 - acc: 0.7229 - val_loss: 0.5588 - val_acc: 0.7680\n",
      "Epoch 7/25\n",
      "4500/4500 [==============================] - 2s 434us/step - loss: 0.5136 - acc: 0.7493 - val_loss: 0.5409 - val_acc: 0.7780\n",
      "Epoch 8/25\n",
      "4500/4500 [==============================] - 2s 436us/step - loss: 0.4996 - acc: 0.7624 - val_loss: 0.5173 - val_acc: 0.8060\n",
      "Epoch 9/25\n",
      "4500/4500 [==============================] - 2s 440us/step - loss: 0.4794 - acc: 0.7809 - val_loss: 0.4948 - val_acc: 0.8040\n",
      "Epoch 10/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.4547 - acc: 0.7864 - val_loss: 0.4886 - val_acc: 0.8060\n",
      "Epoch 11/25\n",
      "4500/4500 [==============================] - 2s 443us/step - loss: 0.4560 - acc: 0.7880 - val_loss: 0.4852 - val_acc: 0.8040\n",
      "Epoch 12/25\n",
      "4500/4500 [==============================] - 2s 439us/step - loss: 0.4336 - acc: 0.8051 - val_loss: 0.4766 - val_acc: 0.7940\n",
      "Epoch 13/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.4294 - acc: 0.8067 - val_loss: 0.4799 - val_acc: 0.7880\n",
      "Epoch 14/25\n",
      "4500/4500 [==============================] - 2s 439us/step - loss: 0.4146 - acc: 0.8187 - val_loss: 0.4671 - val_acc: 0.8160\n",
      "Epoch 15/25\n",
      "4500/4500 [==============================] - 2s 435us/step - loss: 0.3959 - acc: 0.8253 - val_loss: 0.4564 - val_acc: 0.8160\n",
      "Epoch 16/25\n",
      "4500/4500 [==============================] - 2s 439us/step - loss: 0.3778 - acc: 0.8380 - val_loss: 0.4497 - val_acc: 0.8140\n",
      "Epoch 17/25\n",
      "4500/4500 [==============================] - 2s 440us/step - loss: 0.3764 - acc: 0.8393 - val_loss: 0.4497 - val_acc: 0.8100\n",
      "Epoch 18/25\n",
      "4500/4500 [==============================] - 2s 435us/step - loss: 0.3655 - acc: 0.8431 - val_loss: 0.4449 - val_acc: 0.8080\n",
      "Epoch 19/25\n",
      "4500/4500 [==============================] - 2s 439us/step - loss: 0.3541 - acc: 0.8524 - val_loss: 0.4386 - val_acc: 0.8100\n",
      "Epoch 20/25\n",
      "4500/4500 [==============================] - 2s 442us/step - loss: 0.3411 - acc: 0.8582 - val_loss: 0.4369 - val_acc: 0.8100\n",
      "Epoch 21/25\n",
      "4500/4500 [==============================] - 2s 446us/step - loss: 0.3440 - acc: 0.8522 - val_loss: 0.4304 - val_acc: 0.8100\n",
      "Epoch 22/25\n",
      "4500/4500 [==============================] - 2s 440us/step - loss: 0.3203 - acc: 0.8680 - val_loss: 0.4336 - val_acc: 0.8100\n",
      "Epoch 23/25\n",
      "4500/4500 [==============================] - 2s 434us/step - loss: 0.3098 - acc: 0.8744 - val_loss: 0.4287 - val_acc: 0.8140\n",
      "Epoch 24/25\n",
      "4500/4500 [==============================] - 2s 441us/step - loss: 0.3034 - acc: 0.8773 - val_loss: 0.4330 - val_acc: 0.8060\n",
      "Epoch 25/25\n",
      "4500/4500 [==============================] - 2s 437us/step - loss: 0.2910 - acc: 0.8798 - val_loss: 0.4307 - val_acc: 0.8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6fcb5946a0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import CuDNNGRU, Dropout, LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.001,\n",
    "    \"decay\": 0.01\n",
    "}\n",
    "\n",
    "embedding_dim = 1024\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(CuDNNGRU(256), input_shape=(max_length, embedding_dim)))\n",
    "model.add(Dropout(0.80))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.55))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train.values, validation_data=(X_dev, y_dev), epochs=25, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biGRU-ELMO \n",
      "\n",
      "\n",
      "Evaluaci√≥n sobre dev\n",
      "500/500 [==============================] - 0s 229us/step\n",
      "Loss           : 0.4307\n",
      "Accuracy       : 0.8080\n",
      "Precision(1)   : 0.8088\n",
      "Precision(1)   : 0.8074\n",
      "Precision(avg) : 0.8081\n",
      "\n",
      "Recall(1)      : 0.7432\n",
      "Recall(0)      : 0.8597\n",
      "Recall(avg)    : 0.8015\n",
      "\n",
      "F1(1)          : 0.7746\n",
      "F1(0)          : 0.8328\n",
      "F1(avg)        : 0.8037\n",
      "\n",
      "\n",
      "Evaluaci√≥n sobre test\n",
      "1600/1600 [==============================] - 0s 201us/step\n",
      "Loss           : 0.5458\n",
      "Accuracy       : 0.7312\n",
      "Precision(1)   : 0.6764\n",
      "Precision(1)   : 0.7690\n",
      "Precision(avg) : 0.7227\n",
      "\n",
      "Recall(1)      : 0.6682\n",
      "Recall(0)      : 0.7755\n",
      "Recall(avg)    : 0.7219\n",
      "\n",
      "F1(1)          : 0.6723\n",
      "F1(0)          : 0.7722\n",
      "F1(avg)        : 0.7223\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "print(\"biGRU-ELMO \\n\\n\")\n",
    "print(\"Evaluaci√≥n sobre dev\")\n",
    "print_evaluation(model, X_dev, y_dev)\n",
    "print(\"\\n\\nEvaluaci√≥n sobre test\")\n",
    "print_evaluation(model, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "4500/4500 [==============================] - 3s 664us/step - loss: 0.7385 - acc: 0.5640 - val_loss: 0.6666 - val_acc: 0.5880\n",
      "Epoch 2/25\n",
      "4500/4500 [==============================] - 2s 460us/step - loss: 0.6634 - acc: 0.6178 - val_loss: 0.6468 - val_acc: 0.6320\n",
      "Epoch 3/25\n",
      "4500/4500 [==============================] - 2s 462us/step - loss: 0.6425 - acc: 0.6409 - val_loss: 0.6236 - val_acc: 0.6560\n",
      "Epoch 4/25\n",
      "4500/4500 [==============================] - 2s 461us/step - loss: 0.6086 - acc: 0.6742 - val_loss: 0.5996 - val_acc: 0.6860\n",
      "Epoch 5/25\n",
      "4500/4500 [==============================] - 2s 456us/step - loss: 0.5883 - acc: 0.7013 - val_loss: 0.5751 - val_acc: 0.6960\n",
      "Epoch 6/25\n",
      "4500/4500 [==============================] - 2s 467us/step - loss: 0.5638 - acc: 0.7251 - val_loss: 0.5552 - val_acc: 0.7300\n",
      "Epoch 7/25\n",
      "4500/4500 [==============================] - 2s 456us/step - loss: 0.5381 - acc: 0.7351 - val_loss: 0.5345 - val_acc: 0.7480\n",
      "Epoch 8/25\n",
      "4500/4500 [==============================] - 2s 466us/step - loss: 0.5249 - acc: 0.7496 - val_loss: 0.5183 - val_acc: 0.7440\n",
      "Epoch 9/25\n",
      "4500/4500 [==============================] - 2s 463us/step - loss: 0.5074 - acc: 0.7596 - val_loss: 0.5040 - val_acc: 0.7680\n",
      "Epoch 10/25\n",
      "4500/4500 [==============================] - 2s 464us/step - loss: 0.4931 - acc: 0.7744 - val_loss: 0.4915 - val_acc: 0.7700\n",
      "Epoch 11/25\n",
      "4500/4500 [==============================] - 2s 460us/step - loss: 0.4742 - acc: 0.7782 - val_loss: 0.4866 - val_acc: 0.7780\n",
      "Epoch 12/25\n",
      "4500/4500 [==============================] - 2s 467us/step - loss: 0.4665 - acc: 0.7798 - val_loss: 0.4827 - val_acc: 0.7780\n",
      "Epoch 13/25\n",
      "4500/4500 [==============================] - 2s 460us/step - loss: 0.4514 - acc: 0.7851 - val_loss: 0.4748 - val_acc: 0.7780\n",
      "Epoch 14/25\n",
      "4500/4500 [==============================] - 2s 462us/step - loss: 0.4371 - acc: 0.7987 - val_loss: 0.4704 - val_acc: 0.7940\n",
      "Epoch 15/25\n",
      "4500/4500 [==============================] - 2s 463us/step - loss: 0.4258 - acc: 0.8089 - val_loss: 0.4598 - val_acc: 0.7960\n",
      "Epoch 16/25\n",
      "4500/4500 [==============================] - 2s 462us/step - loss: 0.4253 - acc: 0.8078 - val_loss: 0.4590 - val_acc: 0.7960\n",
      "Epoch 17/25\n",
      "4500/4500 [==============================] - 2s 461us/step - loss: 0.4170 - acc: 0.8156 - val_loss: 0.4711 - val_acc: 0.7980\n",
      "Epoch 18/25\n",
      "4500/4500 [==============================] - 2s 462us/step - loss: 0.4085 - acc: 0.8189 - val_loss: 0.4610 - val_acc: 0.7800\n",
      "Epoch 19/25\n",
      "4500/4500 [==============================] - 2s 460us/step - loss: 0.4000 - acc: 0.8251 - val_loss: 0.4503 - val_acc: 0.7820\n",
      "Epoch 20/25\n",
      "4500/4500 [==============================] - 2s 457us/step - loss: 0.3868 - acc: 0.8253 - val_loss: 0.4474 - val_acc: 0.7860\n",
      "Epoch 21/25\n",
      "4500/4500 [==============================] - 2s 457us/step - loss: 0.3848 - acc: 0.8322 - val_loss: 0.4453 - val_acc: 0.7940\n",
      "Epoch 22/25\n",
      "4500/4500 [==============================] - 2s 460us/step - loss: 0.3697 - acc: 0.8364 - val_loss: 0.4458 - val_acc: 0.7900\n",
      "Epoch 23/25\n",
      "4500/4500 [==============================] - 2s 468us/step - loss: 0.3641 - acc: 0.8433 - val_loss: 0.4462 - val_acc: 0.7960\n",
      "Epoch 24/25\n",
      "4500/4500 [==============================] - 2s 460us/step - loss: 0.3655 - acc: 0.8424 - val_loss: 0.4445 - val_acc: 0.7940\n",
      "Epoch 25/25\n",
      "4500/4500 [==============================] - 2s 462us/step - loss: 0.3529 - acc: 0.8520 - val_loss: 0.4463 - val_acc: 0.7940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1a6842c080>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import CuDNNGRU, Dropout, LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.0005,\n",
    "    \"decay\": 0.01\n",
    "}\n",
    "\n",
    "embedding_dim = 1024\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(CuDNNLSTM(256), input_shape=(max_length, embedding_dim)))\n",
    "model.add(Dropout(0.80))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.55))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train.values, validation_data=(X_dev, y_dev), epochs=25, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "4500/4500 [==============================] - 2s 468us/step - loss: 0.3495 - acc: 0.8500 - val_loss: 0.4565 - val_acc: 0.7960\n",
      "Epoch 2/10\n",
      "4500/4500 [==============================] - 2s 457us/step - loss: 0.3445 - acc: 0.8556 - val_loss: 0.4466 - val_acc: 0.7900\n",
      "Epoch 3/10\n",
      "4500/4500 [==============================] - 2s 462us/step - loss: 0.3314 - acc: 0.8596 - val_loss: 0.4486 - val_acc: 0.7920\n",
      "Epoch 4/10\n",
      "4500/4500 [==============================] - 2s 460us/step - loss: 0.3311 - acc: 0.8636 - val_loss: 0.4530 - val_acc: 0.7980\n",
      "Epoch 5/10\n",
      "4500/4500 [==============================] - 2s 464us/step - loss: 0.3253 - acc: 0.8609 - val_loss: 0.4565 - val_acc: 0.7900\n",
      "Epoch 6/10\n",
      "4500/4500 [==============================] - 2s 460us/step - loss: 0.3129 - acc: 0.8653 - val_loss: 0.4532 - val_acc: 0.7880\n",
      "Epoch 7/10\n",
      "4500/4500 [==============================] - 2s 456us/step - loss: 0.3186 - acc: 0.8680 - val_loss: 0.4589 - val_acc: 0.7900\n",
      "Epoch 8/10\n",
      "4500/4500 [==============================] - 2s 465us/step - loss: 0.3029 - acc: 0.8798 - val_loss: 0.4522 - val_acc: 0.7900\n",
      "Epoch 9/10\n",
      "4500/4500 [==============================] - 2s 452us/step - loss: 0.3048 - acc: 0.8747 - val_loss: 0.4548 - val_acc: 0.7860\n",
      "Epoch 10/10\n",
      "4500/4500 [==============================] - 2s 462us/step - loss: 0.3013 - acc: 0.8798 - val_loss: 0.4617 - val_acc: 0.7880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1d65783748>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train.values, validation_data=(X_dev, y_dev), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluaci√≥n sobre dev\n",
      "500/500 [==============================] - 0s 211us/step\n",
      "Loss           : 0.4617\n",
      "Accuracy       : 0.7880\n",
      "Precision(1)   : 0.7762\n",
      "Precision(1)   : 0.7966\n",
      "Precision(avg) : 0.7864\n",
      "\n",
      "Recall(1)      : 0.7342\n",
      "Recall(0)      : 0.8309\n",
      "Recall(avg)    : 0.7826\n",
      "\n",
      "F1(1)          : 0.7546\n",
      "F1(0)          : 0.8134\n",
      "F1(avg)        : 0.7840\n",
      "\n",
      "\n",
      "Evaluaci√≥n sobre test\n",
      "1600/1600 [==============================] - 0s 203us/step\n",
      "Loss           : 0.6203\n",
      "Accuracy       : 0.7244\n",
      "Precision(1)   : 0.6613\n",
      "Precision(1)   : 0.7709\n",
      "Precision(avg) : 0.7161\n",
      "\n",
      "Recall(1)      : 0.6803\n",
      "Recall(0)      : 0.7553\n",
      "Recall(avg)    : 0.7178\n",
      "\n",
      "F1(1)          : 0.6706\n",
      "F1(0)          : 0.7630\n",
      "F1(avg)        : 0.7168\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "\n",
    "print(\"Evaluaci√≥n sobre dev\")\n",
    "print_evaluation(model, X_dev, y_dev)\n",
    "print(\"\\n\\nEvaluaci√≥n sobre test\")\n",
    "print_evaluation(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional GRU sin densa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "4500/4500 [==============================] - 4s 823us/step - loss: 0.8033 - acc: 0.5889 - val_loss: 0.6358 - val_acc: 0.6680\n",
      "Epoch 2/25\n",
      "4500/4500 [==============================] - 2s 439us/step - loss: 0.6520 - acc: 0.6633 - val_loss: 0.5888 - val_acc: 0.6960\n",
      "Epoch 3/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.5620 - acc: 0.7253 - val_loss: 0.5371 - val_acc: 0.7340\n",
      "Epoch 4/25\n",
      "4500/4500 [==============================] - 2s 433us/step - loss: 0.5035 - acc: 0.7580 - val_loss: 0.4944 - val_acc: 0.7560\n",
      "Epoch 5/25\n",
      "4500/4500 [==============================] - 2s 436us/step - loss: 0.4556 - acc: 0.7840 - val_loss: 0.4726 - val_acc: 0.7860\n",
      "Epoch 6/25\n",
      "4500/4500 [==============================] - 2s 432us/step - loss: 0.4297 - acc: 0.8042 - val_loss: 0.4579 - val_acc: 0.7900\n",
      "Epoch 7/25\n",
      "4500/4500 [==============================] - 2s 434us/step - loss: 0.4081 - acc: 0.8160 - val_loss: 0.4557 - val_acc: 0.7920\n",
      "Epoch 8/25\n",
      "4500/4500 [==============================] - 2s 432us/step - loss: 0.3879 - acc: 0.8293 - val_loss: 0.4463 - val_acc: 0.8120\n",
      "Epoch 9/25\n",
      "4500/4500 [==============================] - 2s 430us/step - loss: 0.3668 - acc: 0.8393 - val_loss: 0.4409 - val_acc: 0.8040\n",
      "Epoch 10/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.3523 - acc: 0.8484 - val_loss: 0.4370 - val_acc: 0.8080\n",
      "Epoch 11/25\n",
      "4500/4500 [==============================] - 2s 437us/step - loss: 0.3398 - acc: 0.8507 - val_loss: 0.4540 - val_acc: 0.8040\n",
      "Epoch 12/25\n",
      "4500/4500 [==============================] - 2s 431us/step - loss: 0.3246 - acc: 0.8613 - val_loss: 0.4532 - val_acc: 0.7820\n",
      "Epoch 13/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.3178 - acc: 0.8638 - val_loss: 0.4500 - val_acc: 0.7860\n",
      "Epoch 14/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.3042 - acc: 0.8749 - val_loss: 0.4535 - val_acc: 0.7960\n",
      "Epoch 15/25\n",
      "4500/4500 [==============================] - 2s 439us/step - loss: 0.2943 - acc: 0.8736 - val_loss: 0.4450 - val_acc: 0.7900\n",
      "Epoch 16/25\n",
      "4500/4500 [==============================] - 2s 431us/step - loss: 0.2866 - acc: 0.8802 - val_loss: 0.4407 - val_acc: 0.8060\n",
      "Epoch 17/25\n",
      "4500/4500 [==============================] - 2s 436us/step - loss: 0.2767 - acc: 0.8867 - val_loss: 0.4527 - val_acc: 0.7980\n",
      "Epoch 18/25\n",
      "4500/4500 [==============================] - 2s 435us/step - loss: 0.2672 - acc: 0.8918 - val_loss: 0.4558 - val_acc: 0.7940\n",
      "Epoch 19/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.2590 - acc: 0.8933 - val_loss: 0.4493 - val_acc: 0.8020\n",
      "Epoch 20/25\n",
      "4500/4500 [==============================] - 2s 437us/step - loss: 0.2431 - acc: 0.9004 - val_loss: 0.4493 - val_acc: 0.8000\n",
      "Epoch 21/25\n",
      "4500/4500 [==============================] - 2s 437us/step - loss: 0.2435 - acc: 0.9033 - val_loss: 0.4526 - val_acc: 0.8040\n",
      "Epoch 22/25\n",
      "4500/4500 [==============================] - 2s 437us/step - loss: 0.2365 - acc: 0.9018 - val_loss: 0.4602 - val_acc: 0.7940\n",
      "Epoch 23/25\n",
      "4500/4500 [==============================] - 2s 440us/step - loss: 0.2256 - acc: 0.9149 - val_loss: 0.4682 - val_acc: 0.7920\n",
      "Epoch 24/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.2192 - acc: 0.9147 - val_loss: 0.4871 - val_acc: 0.7920\n",
      "Epoch 25/25\n",
      "4500/4500 [==============================] - 2s 432us/step - loss: 0.2113 - acc: 0.9202 - val_loss: 0.4689 - val_acc: 0.7920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f181a5bb7f0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import CuDNNGRU, Dropout, LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.0005,\n",
    "    \"decay\": 0.01\n",
    "}\n",
    "\n",
    "embedding_dim = 1024\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(CuDNNGRU(256), input_shape=(max_length, embedding_dim)))\n",
    "model.add(Dropout(0.80))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train.values, validation_data=(X_dev, y_dev), epochs=25, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluaci√≥n sobre dev\n",
      "500/500 [==============================] - 0s 227us/step\n",
      "Loss           : 0.4689\n",
      "Accuracy       : 0.7920\n",
      "Precision(1)   : 0.7682\n",
      "Precision(1)   : 0.8107\n",
      "Precision(avg) : 0.7894\n",
      "\n",
      "Recall(1)      : 0.7613\n",
      "Recall(0)      : 0.8165\n",
      "Recall(avg)    : 0.7889\n",
      "\n",
      "F1(1)          : 0.7647\n",
      "F1(0)          : 0.8136\n",
      "F1(avg)        : 0.7892\n",
      "\n",
      "\n",
      "Evaluaci√≥n sobre test\n",
      "1600/1600 [==============================] - 0s 201us/step\n",
      "Loss           : 0.6708\n",
      "Accuracy       : 0.7275\n",
      "Precision(1)   : 0.6530\n",
      "Precision(1)   : 0.7903\n",
      "Precision(avg) : 0.7217\n",
      "\n",
      "Recall(1)      : 0.7242\n",
      "Recall(0)      : 0.7298\n",
      "Recall(avg)    : 0.7270\n",
      "\n",
      "F1(1)          : 0.6868\n",
      "F1(0)          : 0.7588\n",
      "F1(avg)        : 0.7228\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "\n",
    "print(\"Evaluaci√≥n sobre dev\")\n",
    "print_evaluation(model, X_dev, y_dev)\n",
    "print(\"\\n\\nEvaluaci√≥n sobre test\")\n",
    "print_evaluation(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Vamos a ver los tweets con mayores errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_true</th>\n",
       "      <th>pred_false</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hs=1</th>\n",
       "      <td>175</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hs=0</th>\n",
       "      <td>53</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pred_true  pred_false\n",
       "real                       \n",
       "hs=1        175          47\n",
       "hs=0         53         225"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev[\"proba\"] = model.predict_proba(X_dev)\n",
    "\n",
    "\n",
    "true_positives = df_dev[(df_dev[\"HS\"] == 1) & (df_dev[\"proba\"] >= 0.5)].copy()\n",
    "true_negatives = df_dev[(df_dev[\"HS\"] == 0) & (df_dev[\"proba\"] < 0.5)].copy()\n",
    "\n",
    "false_positives = df_dev[(df_dev[\"HS\"] == 0) & (df_dev[\"proba\"] > 0.5)].copy()\n",
    "false_positives.sort_values(\"proba\", ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "false_negatives = df_dev[(df_dev[\"HS\"] == 1) & (df_dev[\"proba\"] < 0.5)].copy()\n",
    "false_negatives.sort_values(\"proba\", ascending=True, inplace=True)\n",
    "\n",
    "conf_matrix = pd.DataFrame([\n",
    "    {\"real\":\"hs=1\", \"pred_true\": len(true_positives), \"pred_false\": len(false_negatives)},\n",
    "    {\"real\":\"hs=0\", \"pred_true\": len(false_positives), \"pred_false\": len(true_negatives)}\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "conf_matrix.set_index(\"real\", inplace=True)\n",
    "\n",
    "conf_matrix[[\"pred_true\", \"pred_false\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Falsos Negativos\n",
    "\n",
    "Veamos los 20 falsos negativos en los cuales nuestro modelo se equivoca m√°s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21535</th>\n",
       "      <td>#VOX critica duramente a ‚Å¶@pablocasado_‚Å© ‚ÄúLes da la bienvenida como si fuera un cartel del welcome de #Carmena ‚Äú ¬°Expulsi√≥n inmediata de moromierdas!  https://t.co/HaySDibj2l</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.043542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21152</th>\n",
       "      <td>@NatyBurgos15 @Horaciogenta @PaolaPa05 C√°llate vos Black and bitch como la puta falsa abogada. Espero se pudra en la c√°rcel y devuelva todo. Ded√≠cate a estudiar en serio. La droga es mala. Cristin...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.053808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22560</th>\n",
       "      <td>@CarlaEUrena @FelipeCalderon Callateeeee !! Puta gallina cacarea !! Ni idea tienes de nada .</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.080199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23415</th>\n",
       "      <td>Exacto! #puralata #feminazis #HombreAbusado https://t.co/zTEvaftxcd</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.084722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21215</th>\n",
       "      <td>Que tal otro aporte an√≥nimo son de la puta de mi esposa, as√≠ los dej√≥ en navidad ense√±ando le los calzones al vecino rt y siguenos @TANGASUCIA @Tangasrobadas https://t.co/70ioUjveqY</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.092792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24925</th>\n",
       "      <td>La verdad que si, no veia la hora de llegar a los 5 mil amigosüòÇ ahora de a poco voy borrando esos indios indocumentadosüòÇ https://t.co/Xw8q4GLM8v</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.144993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24401</th>\n",
       "      <td>No habr√° que darles de lo 'nuestro'. Ellos se apropiar√°n de lo 'suyo'.  @Manuj_Hidalgo les dice ‚òùÔ∏è a quienes piensan q hay q jubilar a los mayores para q trabajen los j√≥venes o q los q vienen d fu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.151047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21072</th>\n",
       "      <td>Voy andando por la calle y me han dicho cosas en √°rabe ya que me estas acosando dime lo en espa√±ol para que te entienda, asique como yo no les entiendo me he cagado en su puta madre en √°rabe para ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.152465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24590</th>\n",
       "      <td>SOMOS MUCHO M√ÅS DE LO QUE SUE√ëAS SER PINCH√â SUDACA CAGADO, y si no te gusta dime donde nos topamos pa reventarnos el hocico 1:1 @amarchesin1 https://t.co/vHHPBKS1AK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.157015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22008</th>\n",
       "      <td>@Namurii @medicaster_xz @Yorha2d @KeloKlau_ Se merec√≠a de puta y no quiere que le metan la verga, jajaja no mamen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24488</th>\n",
       "      <td>Un indocumentado...habla y habla y no para y acaba hablando su ignorancia.Empezamos bien. https://t.co/SMRsqjtEBT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22168</th>\n",
       "      <td>Mi novia es como mi polla. S√© que est√° ah√≠ porque de vez en cuando se levanta. Una de mis pantalones y la otra de la cama.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.185569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24864</th>\n",
       "      <td>rosalia me podr√≠a decir sudaca i wouldn't give a fuck</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24481</th>\n",
       "      <td>En que se parece superman a una mujer lista: En que los dos son personajes de ciencia-ficci√≥n.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.193847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24431</th>\n",
       "      <td>Yo nunca le he dicho puta a una mujer pero socia t√∫ eres senda petardo üòÇüòÇ estas navidades no voy a comprar pirotecnias ya contigo vasta üòÇüòÇüòÇ</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.194956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24746</th>\n",
       "      <td>@rU_1970 muestra como teni esa vagina de puta caliente llena de moco wacha culia hija de la perra traga semen 146:146</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.199260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21887</th>\n",
       "      <td>@A3Noticias Esta tiene que ser una 'fake new' porque las mujeres son siempre las v√≠ctimas, no las hijas de puta maltratadoras. Eso o el chaval es un machista que se lo ha merecido (machete al mach...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.222684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21727</th>\n",
       "      <td>Espa√±a crea su primer 'campo de refugiados' en la bah√≠a de Algeciras https://t.co/l7Zmh2kvPN v√≠a @elmundoes Gran noticia para el contribuyente espa√±ol. Espa√±oles a pagar, a pagar. Las pensiones pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.224704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20337</th>\n",
       "      <td>Ni armar un complot para matar un presidente saben... #Sudacas..</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.236325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21339</th>\n",
       "      <td>La #Inmigraci√≥n, como sabemos, es mala, se ha demostrado en media #Europa. Pero no debemos caer en la tentaci√≥n de salpicar nuestra #soberan√≠a con extremismos, tenemos que defender lo nuestro, si,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.236573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                          text  \\\n",
       "id                                                                                                                                                                                                               \n",
       "21535                           #VOX critica duramente a ‚Å¶@pablocasado_‚Å© ‚ÄúLes da la bienvenida como si fuera un cartel del welcome de #Carmena ‚Äú ¬°Expulsi√≥n inmediata de moromierdas!  https://t.co/HaySDibj2l   \n",
       "21152  @NatyBurgos15 @Horaciogenta @PaolaPa05 C√°llate vos Black and bitch como la puta falsa abogada. Espero se pudra en la c√°rcel y devuelva todo. Ded√≠cate a estudiar en serio. La droga es mala. Cristin...   \n",
       "22560                                                                                                             @CarlaEUrena @FelipeCalderon Callateeeee !! Puta gallina cacarea !! Ni idea tienes de nada .   \n",
       "23415                                                                                                                                      Exacto! #puralata #feminazis #HombreAbusado https://t.co/zTEvaftxcd   \n",
       "21215                    Que tal otro aporte an√≥nimo son de la puta de mi esposa, as√≠ los dej√≥ en navidad ense√±ando le los calzones al vecino rt y siguenos @TANGASUCIA @Tangasrobadas https://t.co/70ioUjveqY   \n",
       "24925                                                         La verdad que si, no veia la hora de llegar a los 5 mil amigosüòÇ ahora de a poco voy borrando esos indios indocumentadosüòÇ https://t.co/Xw8q4GLM8v   \n",
       "24401  No habr√° que darles de lo 'nuestro'. Ellos se apropiar√°n de lo 'suyo'.  @Manuj_Hidalgo les dice ‚òùÔ∏è a quienes piensan q hay q jubilar a los mayores para q trabajen los j√≥venes o q los q vienen d fu...   \n",
       "21072  Voy andando por la calle y me han dicho cosas en √°rabe ya que me estas acosando dime lo en espa√±ol para que te entienda, asique como yo no les entiendo me he cagado en su puta madre en √°rabe para ...   \n",
       "24590                                     SOMOS MUCHO M√ÅS DE LO QUE SUE√ëAS SER PINCH√â SUDACA CAGADO, y si no te gusta dime donde nos topamos pa reventarnos el hocico 1:1 @amarchesin1 https://t.co/vHHPBKS1AK   \n",
       "22008                                                                                        @Namurii @medicaster_xz @Yorha2d @KeloKlau_ Se merec√≠a de puta y no quiere que le metan la verga, jajaja no mamen   \n",
       "24488                                                                                        Un indocumentado...habla y habla y no para y acaba hablando su ignorancia.Empezamos bien. https://t.co/SMRsqjtEBT   \n",
       "22168                                                                               Mi novia es como mi polla. S√© que est√° ah√≠ porque de vez en cuando se levanta. Una de mis pantalones y la otra de la cama.   \n",
       "24864                                                                                                                                                    rosalia me podr√≠a decir sudaca i wouldn't give a fuck   \n",
       "24481                                                                                                           En que se parece superman a una mujer lista: En que los dos son personajes de ciencia-ficci√≥n.   \n",
       "24431                                                              Yo nunca le he dicho puta a una mujer pero socia t√∫ eres senda petardo üòÇüòÇ estas navidades no voy a comprar pirotecnias ya contigo vasta üòÇüòÇüòÇ   \n",
       "24746                                                                                    @rU_1970 muestra como teni esa vagina de puta caliente llena de moco wacha culia hija de la perra traga semen 146:146   \n",
       "21887  @A3Noticias Esta tiene que ser una 'fake new' porque las mujeres son siempre las v√≠ctimas, no las hijas de puta maltratadoras. Eso o el chaval es un machista que se lo ha merecido (machete al mach...   \n",
       "21727  Espa√±a crea su primer 'campo de refugiados' en la bah√≠a de Algeciras https://t.co/l7Zmh2kvPN v√≠a @elmundoes Gran noticia para el contribuyente espa√±ol. Espa√±oles a pagar, a pagar. Las pensiones pe...   \n",
       "20337                                                                                                                                         Ni armar un complot para matar un presidente saben... #Sudacas..   \n",
       "21339  La #Inmigraci√≥n, como sabemos, es mala, se ha demostrado en media #Europa. Pero no debemos caer en la tentaci√≥n de salpicar nuestra #soberan√≠a con extremismos, tenemos que defender lo nuestro, si,...   \n",
       "\n",
       "       HS  TR  AG     proba  \n",
       "id                           \n",
       "21535   1   0   1  0.043542  \n",
       "21152   1   1   1  0.053808  \n",
       "22560   1   1   1  0.080199  \n",
       "23415   1   0   1  0.084722  \n",
       "21215   1   1   1  0.092792  \n",
       "24925   1   0   0  0.144993  \n",
       "24401   1   0   1  0.151047  \n",
       "21072   1   0   1  0.152465  \n",
       "24590   1   1   1  0.157015  \n",
       "22008   1   1   0  0.168723  \n",
       "24488   1   0   0  0.183900  \n",
       "22168   1   1   0  0.185569  \n",
       "24864   1   1   1  0.186539  \n",
       "24481   1   0   0  0.193847  \n",
       "24431   1   1   0  0.194956  \n",
       "24746   1   1   1  0.199260  \n",
       "21887   1   0   1  0.222684  \n",
       "21727   1   0   0  0.224704  \n",
       "20337   1   0   1  0.236325  \n",
       "21339   1   0   1  0.236573  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives.iloc[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Qu√© onda la longitud de la secuencia?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 734., 1109., 1019.,  655.,  386.,  315.,  210.,   57.,   13.,\n",
       "           2.]),\n",
       " array([ 2., 10., 18., 26., 34., 42., 50., 58., 66., 74., 82.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokens_dev = df_dev[\"text\"].apply(lambda t: len(tokenizer.tokenize(t)))\n",
    "tokens_train = df_train[\"text\"].apply(lambda t: len(tokenizer.tokenize(t)))\n",
    "\n",
    "plt.hist(tokens_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22517</th>\n",
       "      <td>-¬øeres poblana? -soy colombiana -¬øy por qu√© eres g√ºera? en Colombia TODAS son morenas, fui a Colombia 3 veces. -ok, ment√≠ üôÑ solo me pinto el cabello</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.971048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22416</th>\n",
       "      <td>@Pepe_ElEzequiel Puta!? Puta con las que vos te metes Esa mujer es un √°ngel enviado por Jesucristo nuestro se√±or para darnos esa paz que buscamos s√≥lo con verla</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.967200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21892</th>\n",
       "      <td>@saancisco Callate puta tu eres el traidor.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.963039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21497</th>\n",
       "      <td>Talvez quede como zorra pero te lo mereces Sociaüòé</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.947185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24172</th>\n",
       "      <td>Y los que no se denuncian... hagamos n√∫meros. El acoso a las mujeres es diario. No todos los hombres son acosadores, pero todas las mujeres han sido acosadas. https://t.co/jza9llhqWA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.945128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21255</th>\n",
       "      <td>Y t√∫ eres una maldita escoria con una repugnante inclinaci√≥n por los hombres.. ¬°Seres como t√∫ me producen n√°useas! https://t.co/TaHDcpZdnm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24159</th>\n",
       "      <td>@LeonDeJapon Jajajajajaja tu eres uno, diego jajajajajaja mentira perra perra perrraaaaaaaaaa de mierda jajaja ‚ô°</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.931898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24402</th>\n",
       "      <td>zorra eres tu que no te sientes hombre y no entiendes que toda mujer es bella</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.925799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21671</th>\n",
       "      <td>@danlntifada @fatinascidi @manolivizoso @Zurine3 @Jvillaver2 https://t.co/HG4ljNlwR3, co√±o entonces tu eres de lo que te guias por el antes de cristo y despues de cristo,cuando llegamos los Espa√±o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.921038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22115</th>\n",
       "      <td>Una chica no es una guarra, una zorra, una puta, una buscan por su manera de vestir.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.915731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22014</th>\n",
       "      <td>Ni t√∫ eres una zorra hambrienta ni yo un incorregible depredador.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.910154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20376</th>\n",
       "      <td>Zorra tu abuela, imb√©cil.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.890432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21050</th>\n",
       "      <td>@GvyMC @Wismichu Callate puta rataüòÇüòÇ ya estas llorando??</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.877848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22848</th>\n",
       "      <td>Pinche beb√© pendejo c√°llate el hocico, nom√°s te est√°n cortando el pelo. Puta chillona</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.856129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22938</th>\n",
       "      <td>Masculinidad fr√°gil es mi crush franc√©s ech√°ndose un clavado al cenote solo porque un grupo de √°rabes le dijo pussy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.855542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23320</th>\n",
       "      <td>@HenriFalconLara No digas nada que t√∫ eres c√≥mplice de lo que est√° pasando... y recuerda seguir√°s siendo la perra m√°s perra de Venezuela</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.854512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21966</th>\n",
       "      <td>esc√∫chame ya me joder√≠a jajajajajajajajajajajajajaja si pa ti ense√±ar es ser una guarra yo soy biennnn guarra, menos mal que no me respeto que si no... https://t.co/yt5fHp99b7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.848084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24209</th>\n",
       "      <td>¬øYa les he contado que si fuera rica/millonaria ser√≠a toda una PERRA?, Yisus me est√° ense√±ando humildad... Y no he aprendido mucho ldvdd.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.813778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22846</th>\n",
       "      <td>Si a mi se me ocurre ense√±ar por twitter una conversaci√≥n de whatsap con mi madre me manda a dormir a la caseta de mi perra.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.808360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22457</th>\n",
       "      <td>De Colombia tocar√° emigrar en pateras, como hacen los africanos subsaharianos.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.795402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                          text  \\\n",
       "id                                                                                                                                                                                                               \n",
       "22517                                                     -¬øeres poblana? -soy colombiana -¬øy por qu√© eres g√ºera? en Colombia TODAS son morenas, fui a Colombia 3 veces. -ok, ment√≠ üôÑ solo me pinto el cabello   \n",
       "22416                                         @Pepe_ElEzequiel Puta!? Puta con las que vos te metes Esa mujer es un √°ngel enviado por Jesucristo nuestro se√±or para darnos esa paz que buscamos s√≥lo con verla   \n",
       "21892                                                                                                                                                              @saancisco Callate puta tu eres el traidor.   \n",
       "21497                                                                                                                                                        Talvez quede como zorra pero te lo mereces Sociaüòé   \n",
       "24172                   Y los que no se denuncian... hagamos n√∫meros. El acoso a las mujeres es diario. No todos los hombres son acosadores, pero todas las mujeres han sido acosadas. https://t.co/jza9llhqWA   \n",
       "21255                                                               Y t√∫ eres una maldita escoria con una repugnante inclinaci√≥n por los hombres.. ¬°Seres como t√∫ me producen n√°useas! https://t.co/TaHDcpZdnm   \n",
       "24159                                                                                         @LeonDeJapon Jajajajajaja tu eres uno, diego jajajajajaja mentira perra perra perrraaaaaaaaaa de mierda jajaja ‚ô°   \n",
       "24402                                                                                                                            zorra eres tu que no te sientes hombre y no entiendes que toda mujer es bella   \n",
       "21671  @danlntifada @fatinascidi @manolivizoso @Zurine3 @Jvillaver2 https://t.co/HG4ljNlwR3, co√±o entonces tu eres de lo que te guias por el antes de cristo y despues de cristo,cuando llegamos los Espa√±o...   \n",
       "22115                                                                                                                     Una chica no es una guarra, una zorra, una puta, una buscan por su manera de vestir.   \n",
       "22014                                                                                                                                        Ni t√∫ eres una zorra hambrienta ni yo un incorregible depredador.   \n",
       "20376                                                                                                                                                                                Zorra tu abuela, imb√©cil.   \n",
       "21050                                                                                                                                                 @GvyMC @Wismichu Callate puta rataüòÇüòÇ ya estas llorando??   \n",
       "22848                                                                                                                    Pinche beb√© pendejo c√°llate el hocico, nom√°s te est√°n cortando el pelo. Puta chillona   \n",
       "22938                                                                                      Masculinidad fr√°gil es mi crush franc√©s ech√°ndose un clavado al cenote solo porque un grupo de √°rabes le dijo pussy   \n",
       "23320                                                                 @HenriFalconLara No digas nada que t√∫ eres c√≥mplice de lo que est√° pasando... y recuerda seguir√°s siendo la perra m√°s perra de Venezuela   \n",
       "21966                          esc√∫chame ya me joder√≠a jajajajajajajajajajajajajaja si pa ti ense√±ar es ser una guarra yo soy biennnn guarra, menos mal que no me respeto que si no... https://t.co/yt5fHp99b7   \n",
       "24209                                                                ¬øYa les he contado que si fuera rica/millonaria ser√≠a toda una PERRA?, Yisus me est√° ense√±ando humildad... Y no he aprendido mucho ldvdd.   \n",
       "22846                                                                             Si a mi se me ocurre ense√±ar por twitter una conversaci√≥n de whatsap con mi madre me manda a dormir a la caseta de mi perra.   \n",
       "22457                                                                                                                           De Colombia tocar√° emigrar en pateras, como hacen los africanos subsaharianos.   \n",
       "\n",
       "       HS  TR  AG     proba  \n",
       "id                           \n",
       "22517   0   0   0  0.971048  \n",
       "22416   0   0   0  0.967200  \n",
       "21892   0   0   0  0.963039  \n",
       "21497   0   0   0  0.947185  \n",
       "24172   0   0   0  0.945128  \n",
       "21255   0   0   0  0.943286  \n",
       "24159   0   0   0  0.931898  \n",
       "24402   0   0   0  0.925799  \n",
       "21671   0   0   0  0.921038  \n",
       "22115   0   0   0  0.915731  \n",
       "22014   0   0   0  0.910154  \n",
       "20376   0   0   0  0.890432  \n",
       "21050   0   0   0  0.877848  \n",
       "22848   0   0   0  0.856129  \n",
       "22938   0   0   0  0.855542  \n",
       "23320   0   0   0  0.854512  \n",
       "21966   0   0   0  0.848084  \n",
       "24209   0   0   0  0.813778  \n",
       "22846   0   0   0  0.808360  \n",
       "22457   0   0   0  0.795402  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 200)\n",
    "false_positives.iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['easyjet',\n",
       " 'quiere',\n",
       " 'duplicar',\n",
       " 'el',\n",
       " 'n√∫mero',\n",
       " 'de',\n",
       " 'mujeres',\n",
       " 'piloto',\n",
       " \"'\",\n",
       " 'ver√°s',\n",
       " 't√∫',\n",
       " 'para',\n",
       " 'aparcar',\n",
       " 'el',\n",
       " 'avi√≥n',\n",
       " '..',\n",
       " 'http://t.co/46NuLkm09x',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

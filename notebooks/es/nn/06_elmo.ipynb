{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM  + ElMO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instancias de entrenamiento: 4500\n",
      "Instancias de desarrollo: 500\n",
      "Instancias de test: 1600\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import torch\n",
    "\n",
    "seed = \n",
    "\n",
    "torch.manual_seed(2019)\n",
    "np.random.seed(2019)\n",
    "tf.random.set_random_seed(2019)\n",
    "random.seed(2019)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "df_dev = pd.read_table(\"../../../data/es/dev_es.tsv\", index_col=\"id\", quoting=csv.QUOTE_NONE)\n",
    "df_train = pd.read_table(\"../../../data/es/train_es.tsv\", index_col=\"id\", quoting=csv.QUOTE_NONE)\n",
    "df_test = pd.read_table(\"../../../data/es/reference_es.tsv\", header=None, \n",
    "                        names=[\"text\", \"HS\", \"TR\", \"AG\"], quoting=csv.QUOTE_NONE)\n",
    "\n",
    "\n",
    "text_train, y_train = df_train[\"text\"], df_train[\"HS\"]\n",
    "text_dev, y_dev = df_dev[\"text\"], df_dev[\"HS\"]\n",
    "text_test, y_test = df_test[\"text\"], df_test[\"HS\"]\n",
    "\n",
    "print(\"Instancias de entrenamiento: {}\".format(len(df_train)))\n",
    "print(\"Instancias de desarrollo: {}\".format(len(df_dev)))\n",
    "print(\"Instancias de test: {}\".format(len(df_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tengo que hacer dos cosas:\n",
    "\n",
    "- Primero, convertir los tweets a secuencias de texto\n",
    "- Luego, paddear las secuencias a cierta longitud (Keras necesita esto para poder paralelizar cálculo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "max_length = 30\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    if len(tokens) >= max_length:\n",
    "        tokens = tokens[:max_length]\n",
    "    else:\n",
    "        tokens = tokens + [''] * (max_length - len(tokens))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "text_train = [preprocess_tweet(tweet) for tweet in df_train[\"text\"].values]\n",
    "text_dev = [preprocess_tweet(tweet) for tweet in df_dev[\"text\"].values]\n",
    "text_test = [preprocess_tweet(tweet) for tweet in df_test[\"text\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from elmoformanylangs import Embedder\n",
    "\n",
    "e = Embedder(\"../../../models/elmo/es/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carguemos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['easyjet', 'quiere', 'duplicar', 'el', 'número', 'de', 'mujeres', 'piloto', \"'\", 'verás', 'tú', 'para', 'aparcar', 'el', 'avión', '..', 'http://t.co/46NuLkm09x', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(text_train[0])\n",
    "\n",
    "\n",
    "X_train = np.array(e.sents2elmo(text_train))\n",
    "X_dev = np.array(e.sents2elmo(text_dev))\n",
    "X_test = np.array(e.sents2elmo(text_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4500, 30, 1024), (500, 30, 1024), (1600, 30, 1024), (4500,), (500,), (1600,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_dev.shape, X_test.shape, y_train.shape, y_dev.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnngru_4 (CuDNNGRU)       (None, 256)               984576    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,050,625\n",
      "Trainable params: 1,050,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "4500/4500 [==============================] - 3s 591us/step - loss: 0.8773 - acc: 0.5462 - val_loss: 0.6861 - val_acc: 0.6120\n",
      "Epoch 2/30\n",
      "4500/4500 [==============================] - 2s 349us/step - loss: 0.7320 - acc: 0.5491 - val_loss: 0.6837 - val_acc: 0.5560\n",
      "Epoch 3/30\n",
      "4500/4500 [==============================] - 2s 356us/step - loss: 0.6973 - acc: 0.5684 - val_loss: 0.6848 - val_acc: 0.5560\n",
      "Epoch 4/30\n",
      "4500/4500 [==============================] - 2s 355us/step - loss: 0.6786 - acc: 0.5916 - val_loss: 0.6798 - val_acc: 0.5620\n",
      "Epoch 5/30\n",
      "4500/4500 [==============================] - 2s 353us/step - loss: 0.6649 - acc: 0.6011 - val_loss: 0.6741 - val_acc: 0.6160\n",
      "Epoch 6/30\n",
      "4500/4500 [==============================] - 2s 355us/step - loss: 0.6569 - acc: 0.6111 - val_loss: 0.6672 - val_acc: 0.6640\n",
      "Epoch 7/30\n",
      "4500/4500 [==============================] - 2s 354us/step - loss: 0.6389 - acc: 0.6407 - val_loss: 0.6530 - val_acc: 0.6980\n",
      "Epoch 8/30\n",
      "4500/4500 [==============================] - 2s 354us/step - loss: 0.6184 - acc: 0.6598 - val_loss: 0.6277 - val_acc: 0.7020\n",
      "Epoch 9/30\n",
      "4500/4500 [==============================] - 2s 356us/step - loss: 0.5889 - acc: 0.6958 - val_loss: 0.5893 - val_acc: 0.7320\n",
      "Epoch 10/30\n",
      "4500/4500 [==============================] - 2s 355us/step - loss: 0.5621 - acc: 0.7098 - val_loss: 0.5526 - val_acc: 0.7500\n",
      "Epoch 11/30\n",
      "4500/4500 [==============================] - 2s 356us/step - loss: 0.5292 - acc: 0.7376 - val_loss: 0.5195 - val_acc: 0.7600\n",
      "Epoch 12/30\n",
      "4500/4500 [==============================] - 2s 358us/step - loss: 0.5135 - acc: 0.7507 - val_loss: 0.4946 - val_acc: 0.7720\n",
      "Epoch 13/30\n",
      "4500/4500 [==============================] - 2s 355us/step - loss: 0.4809 - acc: 0.7780 - val_loss: 0.4751 - val_acc: 0.7800\n",
      "Epoch 14/30\n",
      "4500/4500 [==============================] - 2s 354us/step - loss: 0.4758 - acc: 0.7831 - val_loss: 0.4636 - val_acc: 0.7920\n",
      "Epoch 15/30\n",
      "4500/4500 [==============================] - 2s 357us/step - loss: 0.4363 - acc: 0.8044 - val_loss: 0.4511 - val_acc: 0.7820\n",
      "Epoch 16/30\n",
      "4500/4500 [==============================] - 2s 351us/step - loss: 0.4373 - acc: 0.8060 - val_loss: 0.4467 - val_acc: 0.7820\n",
      "Epoch 17/30\n",
      "4500/4500 [==============================] - 2s 355us/step - loss: 0.4177 - acc: 0.8284 - val_loss: 0.4414 - val_acc: 0.7880\n",
      "Epoch 18/30\n",
      "4500/4500 [==============================] - 2s 353us/step - loss: 0.3916 - acc: 0.8380 - val_loss: 0.4330 - val_acc: 0.8020\n",
      "Epoch 19/30\n",
      "4500/4500 [==============================] - 2s 356us/step - loss: 0.3878 - acc: 0.8404 - val_loss: 0.4426 - val_acc: 0.7880\n",
      "Epoch 20/30\n",
      "4500/4500 [==============================] - 2s 357us/step - loss: 0.3648 - acc: 0.8564 - val_loss: 0.4317 - val_acc: 0.7900\n",
      "Epoch 21/30\n",
      "4500/4500 [==============================] - 2s 357us/step - loss: 0.3614 - acc: 0.8584 - val_loss: 0.4266 - val_acc: 0.7860\n",
      "Epoch 22/30\n",
      "4500/4500 [==============================] - 2s 355us/step - loss: 0.3373 - acc: 0.8716 - val_loss: 0.4225 - val_acc: 0.7920\n",
      "Epoch 23/30\n",
      "4500/4500 [==============================] - 2s 358us/step - loss: 0.3145 - acc: 0.8773 - val_loss: 0.4215 - val_acc: 0.8020\n",
      "Epoch 24/30\n",
      "4500/4500 [==============================] - 2s 352us/step - loss: 0.3013 - acc: 0.8867 - val_loss: 0.4309 - val_acc: 0.8000\n",
      "Epoch 25/30\n",
      "4500/4500 [==============================] - 2s 358us/step - loss: 0.2806 - acc: 0.9038 - val_loss: 0.4414 - val_acc: 0.8020\n",
      "Epoch 26/30\n",
      "4500/4500 [==============================] - 2s 357us/step - loss: 0.2649 - acc: 0.9091 - val_loss: 0.4234 - val_acc: 0.8040\n",
      "Epoch 27/30\n",
      "4500/4500 [==============================] - 2s 357us/step - loss: 0.2530 - acc: 0.9102 - val_loss: 0.4526 - val_acc: 0.7960\n",
      "Epoch 28/30\n",
      "4500/4500 [==============================] - 2s 354us/step - loss: 0.2318 - acc: 0.9276 - val_loss: 0.4442 - val_acc: 0.8100\n",
      "Epoch 29/30\n",
      "4500/4500 [==============================] - 2s 358us/step - loss: 0.2074 - acc: 0.9320 - val_loss: 0.4515 - val_acc: 0.8100\n",
      "Epoch 30/30\n",
      "4500/4500 [==============================] - 2s 352us/step - loss: 0.1989 - acc: 0.9464 - val_loss: 0.4675 - val_acc: 0.8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6fe04577b8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, CuDNNLSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "embedding_dim = 1024\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.002,\n",
    "    \"decay\": 0.01\n",
    "}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(CuDNNGRU(256, input_shape=(max_length, embedding_dim)))\n",
    "model.add(Dropout(0.80))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.55))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=30, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU-LSTM -- \n",
      "\n",
      "\n",
      "Evaluación sobre dev\n",
      "500/500 [==============================] - 0s 183us/step\n",
      "Loss           : 0.4675\n",
      "Accuracy       : 0.8080\n",
      "Precision(1)   : 0.7864\n",
      "Precision(1)   : 0.8250\n",
      "Precision(avg) : 0.8057\n",
      "\n",
      "Recall(1)      : 0.7793\n",
      "Recall(0)      : 0.8309\n",
      "Recall(avg)    : 0.8051\n",
      "\n",
      "F1(1)          : 0.7828\n",
      "F1(0)          : 0.8280\n",
      "F1(avg)        : 0.8054\n",
      "\n",
      "\n",
      "Evaluación sobre test\n",
      "1600/1600 [==============================] - 0s 156us/step\n",
      "Loss           : 0.7042\n",
      "Accuracy       : 0.7288\n",
      "Precision(1)   : 0.6544\n",
      "Precision(1)   : 0.7915\n",
      "Precision(avg) : 0.7229\n",
      "\n",
      "Recall(1)      : 0.7258\n",
      "Recall(0)      : 0.7309\n",
      "Recall(avg)    : 0.7283\n",
      "\n",
      "F1(1)          : 0.6882\n",
      "F1(0)          : 0.7600\n",
      "F1(avg)        : 0.7241\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "print(\"GRU-LSTM -- \\n\\n\")\n",
    "print(\"Evaluación sobre dev\")\n",
    "print_evaluation(model, X_dev, y_dev)\n",
    "print(\"\\n\\nEvaluación sobre test\")\n",
    "print_evaluation(model, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "4500/4500 [==============================] - 3s 745us/step - loss: 0.8857 - acc: 0.5413 - val_loss: 0.6754 - val_acc: 0.5780\n",
      "Epoch 2/25\n",
      "4500/4500 [==============================] - 2s 434us/step - loss: 0.7046 - acc: 0.5896 - val_loss: 0.6659 - val_acc: 0.5820\n",
      "Epoch 3/25\n",
      "4500/4500 [==============================] - 2s 433us/step - loss: 0.6617 - acc: 0.6324 - val_loss: 0.6553 - val_acc: 0.6300\n",
      "Epoch 4/25\n",
      "4500/4500 [==============================] - 2s 443us/step - loss: 0.6209 - acc: 0.6591 - val_loss: 0.6208 - val_acc: 0.7220\n",
      "Epoch 5/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.5852 - acc: 0.6969 - val_loss: 0.5880 - val_acc: 0.7340\n",
      "Epoch 6/25\n",
      "4500/4500 [==============================] - 2s 436us/step - loss: 0.5510 - acc: 0.7229 - val_loss: 0.5588 - val_acc: 0.7680\n",
      "Epoch 7/25\n",
      "4500/4500 [==============================] - 2s 434us/step - loss: 0.5136 - acc: 0.7493 - val_loss: 0.5409 - val_acc: 0.7780\n",
      "Epoch 8/25\n",
      "4500/4500 [==============================] - 2s 436us/step - loss: 0.4996 - acc: 0.7624 - val_loss: 0.5173 - val_acc: 0.8060\n",
      "Epoch 9/25\n",
      "4500/4500 [==============================] - 2s 440us/step - loss: 0.4794 - acc: 0.7809 - val_loss: 0.4948 - val_acc: 0.8040\n",
      "Epoch 10/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.4547 - acc: 0.7864 - val_loss: 0.4886 - val_acc: 0.8060\n",
      "Epoch 11/25\n",
      "4500/4500 [==============================] - 2s 443us/step - loss: 0.4560 - acc: 0.7880 - val_loss: 0.4852 - val_acc: 0.8040\n",
      "Epoch 12/25\n",
      "4500/4500 [==============================] - 2s 439us/step - loss: 0.4336 - acc: 0.8051 - val_loss: 0.4766 - val_acc: 0.7940\n",
      "Epoch 13/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.4294 - acc: 0.8067 - val_loss: 0.4799 - val_acc: 0.7880\n",
      "Epoch 14/25\n",
      "4500/4500 [==============================] - 2s 439us/step - loss: 0.4146 - acc: 0.8187 - val_loss: 0.4671 - val_acc: 0.8160\n",
      "Epoch 15/25\n",
      "4500/4500 [==============================] - 2s 435us/step - loss: 0.3959 - acc: 0.8253 - val_loss: 0.4564 - val_acc: 0.8160\n",
      "Epoch 16/25\n",
      "4500/4500 [==============================] - 2s 439us/step - loss: 0.3778 - acc: 0.8380 - val_loss: 0.4497 - val_acc: 0.8140\n",
      "Epoch 17/25\n",
      "4500/4500 [==============================] - 2s 440us/step - loss: 0.3764 - acc: 0.8393 - val_loss: 0.4497 - val_acc: 0.8100\n",
      "Epoch 18/25\n",
      "4500/4500 [==============================] - 2s 435us/step - loss: 0.3655 - acc: 0.8431 - val_loss: 0.4449 - val_acc: 0.8080\n",
      "Epoch 19/25\n",
      "4500/4500 [==============================] - 2s 439us/step - loss: 0.3541 - acc: 0.8524 - val_loss: 0.4386 - val_acc: 0.8100\n",
      "Epoch 20/25\n",
      "4500/4500 [==============================] - 2s 442us/step - loss: 0.3411 - acc: 0.8582 - val_loss: 0.4369 - val_acc: 0.8100\n",
      "Epoch 21/25\n",
      "4500/4500 [==============================] - 2s 446us/step - loss: 0.3440 - acc: 0.8522 - val_loss: 0.4304 - val_acc: 0.8100\n",
      "Epoch 22/25\n",
      "4500/4500 [==============================] - 2s 440us/step - loss: 0.3203 - acc: 0.8680 - val_loss: 0.4336 - val_acc: 0.8100\n",
      "Epoch 23/25\n",
      "4500/4500 [==============================] - 2s 434us/step - loss: 0.3098 - acc: 0.8744 - val_loss: 0.4287 - val_acc: 0.8140\n",
      "Epoch 24/25\n",
      "4500/4500 [==============================] - 2s 441us/step - loss: 0.3034 - acc: 0.8773 - val_loss: 0.4330 - val_acc: 0.8060\n",
      "Epoch 25/25\n",
      "4500/4500 [==============================] - 2s 437us/step - loss: 0.2910 - acc: 0.8798 - val_loss: 0.4307 - val_acc: 0.8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6fcb5946a0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import CuDNNGRU, Dropout, LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.001,\n",
    "    \"decay\": 0.01\n",
    "}\n",
    "\n",
    "embedding_dim = 1024\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(CuDNNGRU(256), input_shape=(max_length, embedding_dim)))\n",
    "model.add(Dropout(0.80))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.55))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train.values, validation_data=(X_dev, y_dev), epochs=25, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biGRU-ELMO \n",
      "\n",
      "\n",
      "Evaluación sobre dev\n",
      "500/500 [==============================] - 0s 229us/step\n",
      "Loss           : 0.4307\n",
      "Accuracy       : 0.8080\n",
      "Precision(1)   : 0.8088\n",
      "Precision(1)   : 0.8074\n",
      "Precision(avg) : 0.8081\n",
      "\n",
      "Recall(1)      : 0.7432\n",
      "Recall(0)      : 0.8597\n",
      "Recall(avg)    : 0.8015\n",
      "\n",
      "F1(1)          : 0.7746\n",
      "F1(0)          : 0.8328\n",
      "F1(avg)        : 0.8037\n",
      "\n",
      "\n",
      "Evaluación sobre test\n",
      "1600/1600 [==============================] - 0s 201us/step\n",
      "Loss           : 0.5458\n",
      "Accuracy       : 0.7312\n",
      "Precision(1)   : 0.6764\n",
      "Precision(1)   : 0.7690\n",
      "Precision(avg) : 0.7227\n",
      "\n",
      "Recall(1)      : 0.6682\n",
      "Recall(0)      : 0.7755\n",
      "Recall(avg)    : 0.7219\n",
      "\n",
      "F1(1)          : 0.6723\n",
      "F1(0)          : 0.7722\n",
      "F1(avg)        : 0.7223\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "print(\"biGRU-ELMO \\n\\n\")\n",
    "print(\"Evaluación sobre dev\")\n",
    "print_evaluation(model, X_dev, y_dev)\n",
    "print(\"\\n\\nEvaluación sobre test\")\n",
    "print_evaluation(model, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "4500/4500 [==============================] - 3s 664us/step - loss: 0.7385 - acc: 0.5640 - val_loss: 0.6666 - val_acc: 0.5880\n",
      "Epoch 2/25\n",
      "4500/4500 [==============================] - 2s 460us/step - loss: 0.6634 - acc: 0.6178 - val_loss: 0.6468 - val_acc: 0.6320\n",
      "Epoch 3/25\n",
      "4500/4500 [==============================] - 2s 462us/step - loss: 0.6425 - acc: 0.6409 - val_loss: 0.6236 - val_acc: 0.6560\n",
      "Epoch 4/25\n",
      "4500/4500 [==============================] - 2s 461us/step - loss: 0.6086 - acc: 0.6742 - val_loss: 0.5996 - val_acc: 0.6860\n",
      "Epoch 5/25\n",
      "4500/4500 [==============================] - 2s 456us/step - loss: 0.5883 - acc: 0.7013 - val_loss: 0.5751 - val_acc: 0.6960\n",
      "Epoch 6/25\n",
      "4500/4500 [==============================] - 2s 467us/step - loss: 0.5638 - acc: 0.7251 - val_loss: 0.5552 - val_acc: 0.7300\n",
      "Epoch 7/25\n",
      "4500/4500 [==============================] - 2s 456us/step - loss: 0.5381 - acc: 0.7351 - val_loss: 0.5345 - val_acc: 0.7480\n",
      "Epoch 8/25\n",
      "4500/4500 [==============================] - 2s 466us/step - loss: 0.5249 - acc: 0.7496 - val_loss: 0.5183 - val_acc: 0.7440\n",
      "Epoch 9/25\n",
      "4500/4500 [==============================] - 2s 463us/step - loss: 0.5074 - acc: 0.7596 - val_loss: 0.5040 - val_acc: 0.7680\n",
      "Epoch 10/25\n",
      "4500/4500 [==============================] - 2s 464us/step - loss: 0.4931 - acc: 0.7744 - val_loss: 0.4915 - val_acc: 0.7700\n",
      "Epoch 11/25\n",
      "4500/4500 [==============================] - 2s 460us/step - loss: 0.4742 - acc: 0.7782 - val_loss: 0.4866 - val_acc: 0.7780\n",
      "Epoch 12/25\n",
      "4500/4500 [==============================] - 2s 467us/step - loss: 0.4665 - acc: 0.7798 - val_loss: 0.4827 - val_acc: 0.7780\n",
      "Epoch 13/25\n",
      "4500/4500 [==============================] - 2s 460us/step - loss: 0.4514 - acc: 0.7851 - val_loss: 0.4748 - val_acc: 0.7780\n",
      "Epoch 14/25\n",
      "4500/4500 [==============================] - 2s 462us/step - loss: 0.4371 - acc: 0.7987 - val_loss: 0.4704 - val_acc: 0.7940\n",
      "Epoch 15/25\n",
      "4500/4500 [==============================] - 2s 463us/step - loss: 0.4258 - acc: 0.8089 - val_loss: 0.4598 - val_acc: 0.7960\n",
      "Epoch 16/25\n",
      "4500/4500 [==============================] - 2s 462us/step - loss: 0.4253 - acc: 0.8078 - val_loss: 0.4590 - val_acc: 0.7960\n",
      "Epoch 17/25\n",
      "4500/4500 [==============================] - 2s 461us/step - loss: 0.4170 - acc: 0.8156 - val_loss: 0.4711 - val_acc: 0.7980\n",
      "Epoch 18/25\n",
      "4500/4500 [==============================] - 2s 462us/step - loss: 0.4085 - acc: 0.8189 - val_loss: 0.4610 - val_acc: 0.7800\n",
      "Epoch 19/25\n",
      "4500/4500 [==============================] - 2s 460us/step - loss: 0.4000 - acc: 0.8251 - val_loss: 0.4503 - val_acc: 0.7820\n",
      "Epoch 20/25\n",
      "4500/4500 [==============================] - 2s 457us/step - loss: 0.3868 - acc: 0.8253 - val_loss: 0.4474 - val_acc: 0.7860\n",
      "Epoch 21/25\n",
      "4500/4500 [==============================] - 2s 457us/step - loss: 0.3848 - acc: 0.8322 - val_loss: 0.4453 - val_acc: 0.7940\n",
      "Epoch 22/25\n",
      "4500/4500 [==============================] - 2s 460us/step - loss: 0.3697 - acc: 0.8364 - val_loss: 0.4458 - val_acc: 0.7900\n",
      "Epoch 23/25\n",
      "4500/4500 [==============================] - 2s 468us/step - loss: 0.3641 - acc: 0.8433 - val_loss: 0.4462 - val_acc: 0.7960\n",
      "Epoch 24/25\n",
      "4500/4500 [==============================] - 2s 460us/step - loss: 0.3655 - acc: 0.8424 - val_loss: 0.4445 - val_acc: 0.7940\n",
      "Epoch 25/25\n",
      "4500/4500 [==============================] - 2s 462us/step - loss: 0.3529 - acc: 0.8520 - val_loss: 0.4463 - val_acc: 0.7940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1a6842c080>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import CuDNNGRU, Dropout, LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.0005,\n",
    "    \"decay\": 0.01\n",
    "}\n",
    "\n",
    "embedding_dim = 1024\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(CuDNNLSTM(256), input_shape=(max_length, embedding_dim)))\n",
    "model.add(Dropout(0.80))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.55))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train.values, validation_data=(X_dev, y_dev), epochs=25, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "4500/4500 [==============================] - 2s 468us/step - loss: 0.3495 - acc: 0.8500 - val_loss: 0.4565 - val_acc: 0.7960\n",
      "Epoch 2/10\n",
      "4500/4500 [==============================] - 2s 457us/step - loss: 0.3445 - acc: 0.8556 - val_loss: 0.4466 - val_acc: 0.7900\n",
      "Epoch 3/10\n",
      "4500/4500 [==============================] - 2s 462us/step - loss: 0.3314 - acc: 0.8596 - val_loss: 0.4486 - val_acc: 0.7920\n",
      "Epoch 4/10\n",
      "4500/4500 [==============================] - 2s 460us/step - loss: 0.3311 - acc: 0.8636 - val_loss: 0.4530 - val_acc: 0.7980\n",
      "Epoch 5/10\n",
      "4500/4500 [==============================] - 2s 464us/step - loss: 0.3253 - acc: 0.8609 - val_loss: 0.4565 - val_acc: 0.7900\n",
      "Epoch 6/10\n",
      "4500/4500 [==============================] - 2s 460us/step - loss: 0.3129 - acc: 0.8653 - val_loss: 0.4532 - val_acc: 0.7880\n",
      "Epoch 7/10\n",
      "4500/4500 [==============================] - 2s 456us/step - loss: 0.3186 - acc: 0.8680 - val_loss: 0.4589 - val_acc: 0.7900\n",
      "Epoch 8/10\n",
      "4500/4500 [==============================] - 2s 465us/step - loss: 0.3029 - acc: 0.8798 - val_loss: 0.4522 - val_acc: 0.7900\n",
      "Epoch 9/10\n",
      "4500/4500 [==============================] - 2s 452us/step - loss: 0.3048 - acc: 0.8747 - val_loss: 0.4548 - val_acc: 0.7860\n",
      "Epoch 10/10\n",
      "4500/4500 [==============================] - 2s 462us/step - loss: 0.3013 - acc: 0.8798 - val_loss: 0.4617 - val_acc: 0.7880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1d65783748>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train.values, validation_data=(X_dev, y_dev), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluación sobre dev\n",
      "500/500 [==============================] - 0s 211us/step\n",
      "Loss           : 0.4617\n",
      "Accuracy       : 0.7880\n",
      "Precision(1)   : 0.7762\n",
      "Precision(1)   : 0.7966\n",
      "Precision(avg) : 0.7864\n",
      "\n",
      "Recall(1)      : 0.7342\n",
      "Recall(0)      : 0.8309\n",
      "Recall(avg)    : 0.7826\n",
      "\n",
      "F1(1)          : 0.7546\n",
      "F1(0)          : 0.8134\n",
      "F1(avg)        : 0.7840\n",
      "\n",
      "\n",
      "Evaluación sobre test\n",
      "1600/1600 [==============================] - 0s 203us/step\n",
      "Loss           : 0.6203\n",
      "Accuracy       : 0.7244\n",
      "Precision(1)   : 0.6613\n",
      "Precision(1)   : 0.7709\n",
      "Precision(avg) : 0.7161\n",
      "\n",
      "Recall(1)      : 0.6803\n",
      "Recall(0)      : 0.7553\n",
      "Recall(avg)    : 0.7178\n",
      "\n",
      "F1(1)          : 0.6706\n",
      "F1(0)          : 0.7630\n",
      "F1(avg)        : 0.7168\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "\n",
    "print(\"Evaluación sobre dev\")\n",
    "print_evaluation(model, X_dev, y_dev)\n",
    "print(\"\\n\\nEvaluación sobre test\")\n",
    "print_evaluation(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional GRU sin densa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "4500/4500 [==============================] - 4s 823us/step - loss: 0.8033 - acc: 0.5889 - val_loss: 0.6358 - val_acc: 0.6680\n",
      "Epoch 2/25\n",
      "4500/4500 [==============================] - 2s 439us/step - loss: 0.6520 - acc: 0.6633 - val_loss: 0.5888 - val_acc: 0.6960\n",
      "Epoch 3/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.5620 - acc: 0.7253 - val_loss: 0.5371 - val_acc: 0.7340\n",
      "Epoch 4/25\n",
      "4500/4500 [==============================] - 2s 433us/step - loss: 0.5035 - acc: 0.7580 - val_loss: 0.4944 - val_acc: 0.7560\n",
      "Epoch 5/25\n",
      "4500/4500 [==============================] - 2s 436us/step - loss: 0.4556 - acc: 0.7840 - val_loss: 0.4726 - val_acc: 0.7860\n",
      "Epoch 6/25\n",
      "4500/4500 [==============================] - 2s 432us/step - loss: 0.4297 - acc: 0.8042 - val_loss: 0.4579 - val_acc: 0.7900\n",
      "Epoch 7/25\n",
      "4500/4500 [==============================] - 2s 434us/step - loss: 0.4081 - acc: 0.8160 - val_loss: 0.4557 - val_acc: 0.7920\n",
      "Epoch 8/25\n",
      "4500/4500 [==============================] - 2s 432us/step - loss: 0.3879 - acc: 0.8293 - val_loss: 0.4463 - val_acc: 0.8120\n",
      "Epoch 9/25\n",
      "4500/4500 [==============================] - 2s 430us/step - loss: 0.3668 - acc: 0.8393 - val_loss: 0.4409 - val_acc: 0.8040\n",
      "Epoch 10/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.3523 - acc: 0.8484 - val_loss: 0.4370 - val_acc: 0.8080\n",
      "Epoch 11/25\n",
      "4500/4500 [==============================] - 2s 437us/step - loss: 0.3398 - acc: 0.8507 - val_loss: 0.4540 - val_acc: 0.8040\n",
      "Epoch 12/25\n",
      "4500/4500 [==============================] - 2s 431us/step - loss: 0.3246 - acc: 0.8613 - val_loss: 0.4532 - val_acc: 0.7820\n",
      "Epoch 13/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.3178 - acc: 0.8638 - val_loss: 0.4500 - val_acc: 0.7860\n",
      "Epoch 14/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.3042 - acc: 0.8749 - val_loss: 0.4535 - val_acc: 0.7960\n",
      "Epoch 15/25\n",
      "4500/4500 [==============================] - 2s 439us/step - loss: 0.2943 - acc: 0.8736 - val_loss: 0.4450 - val_acc: 0.7900\n",
      "Epoch 16/25\n",
      "4500/4500 [==============================] - 2s 431us/step - loss: 0.2866 - acc: 0.8802 - val_loss: 0.4407 - val_acc: 0.8060\n",
      "Epoch 17/25\n",
      "4500/4500 [==============================] - 2s 436us/step - loss: 0.2767 - acc: 0.8867 - val_loss: 0.4527 - val_acc: 0.7980\n",
      "Epoch 18/25\n",
      "4500/4500 [==============================] - 2s 435us/step - loss: 0.2672 - acc: 0.8918 - val_loss: 0.4558 - val_acc: 0.7940\n",
      "Epoch 19/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.2590 - acc: 0.8933 - val_loss: 0.4493 - val_acc: 0.8020\n",
      "Epoch 20/25\n",
      "4500/4500 [==============================] - 2s 437us/step - loss: 0.2431 - acc: 0.9004 - val_loss: 0.4493 - val_acc: 0.8000\n",
      "Epoch 21/25\n",
      "4500/4500 [==============================] - 2s 437us/step - loss: 0.2435 - acc: 0.9033 - val_loss: 0.4526 - val_acc: 0.8040\n",
      "Epoch 22/25\n",
      "4500/4500 [==============================] - 2s 437us/step - loss: 0.2365 - acc: 0.9018 - val_loss: 0.4602 - val_acc: 0.7940\n",
      "Epoch 23/25\n",
      "4500/4500 [==============================] - 2s 440us/step - loss: 0.2256 - acc: 0.9149 - val_loss: 0.4682 - val_acc: 0.7920\n",
      "Epoch 24/25\n",
      "4500/4500 [==============================] - 2s 438us/step - loss: 0.2192 - acc: 0.9147 - val_loss: 0.4871 - val_acc: 0.7920\n",
      "Epoch 25/25\n",
      "4500/4500 [==============================] - 2s 432us/step - loss: 0.2113 - acc: 0.9202 - val_loss: 0.4689 - val_acc: 0.7920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f181a5bb7f0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import CuDNNGRU, Dropout, LSTM, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.0005,\n",
    "    \"decay\": 0.01\n",
    "}\n",
    "\n",
    "embedding_dim = 1024\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(CuDNNGRU(256), input_shape=(max_length, embedding_dim)))\n",
    "model.add(Dropout(0.80))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train.values, validation_data=(X_dev, y_dev), epochs=25, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluación sobre dev\n",
      "500/500 [==============================] - 0s 227us/step\n",
      "Loss           : 0.4689\n",
      "Accuracy       : 0.7920\n",
      "Precision(1)   : 0.7682\n",
      "Precision(1)   : 0.8107\n",
      "Precision(avg) : 0.7894\n",
      "\n",
      "Recall(1)      : 0.7613\n",
      "Recall(0)      : 0.8165\n",
      "Recall(avg)    : 0.7889\n",
      "\n",
      "F1(1)          : 0.7647\n",
      "F1(0)          : 0.8136\n",
      "F1(avg)        : 0.7892\n",
      "\n",
      "\n",
      "Evaluación sobre test\n",
      "1600/1600 [==============================] - 0s 201us/step\n",
      "Loss           : 0.6708\n",
      "Accuracy       : 0.7275\n",
      "Precision(1)   : 0.6530\n",
      "Precision(1)   : 0.7903\n",
      "Precision(avg) : 0.7217\n",
      "\n",
      "Recall(1)      : 0.7242\n",
      "Recall(0)      : 0.7298\n",
      "Recall(avg)    : 0.7270\n",
      "\n",
      "F1(1)          : 0.6868\n",
      "F1(0)          : 0.7588\n",
      "F1(avg)        : 0.7228\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "\n",
    "print(\"Evaluación sobre dev\")\n",
    "print_evaluation(model, X_dev, y_dev)\n",
    "print(\"\\n\\nEvaluación sobre test\")\n",
    "print_evaluation(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Vamos a ver los tweets con mayores errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_true</th>\n",
       "      <th>pred_false</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>real</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hs=1</th>\n",
       "      <td>175</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hs=0</th>\n",
       "      <td>53</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pred_true  pred_false\n",
       "real                       \n",
       "hs=1        175          47\n",
       "hs=0         53         225"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev[\"proba\"] = model.predict_proba(X_dev)\n",
    "\n",
    "\n",
    "true_positives = df_dev[(df_dev[\"HS\"] == 1) & (df_dev[\"proba\"] >= 0.5)].copy()\n",
    "true_negatives = df_dev[(df_dev[\"HS\"] == 0) & (df_dev[\"proba\"] < 0.5)].copy()\n",
    "\n",
    "false_positives = df_dev[(df_dev[\"HS\"] == 0) & (df_dev[\"proba\"] > 0.5)].copy()\n",
    "false_positives.sort_values(\"proba\", ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "false_negatives = df_dev[(df_dev[\"HS\"] == 1) & (df_dev[\"proba\"] < 0.5)].copy()\n",
    "false_negatives.sort_values(\"proba\", ascending=True, inplace=True)\n",
    "\n",
    "conf_matrix = pd.DataFrame([\n",
    "    {\"real\":\"hs=1\", \"pred_true\": len(true_positives), \"pred_false\": len(false_negatives)},\n",
    "    {\"real\":\"hs=0\", \"pred_true\": len(false_positives), \"pred_false\": len(true_negatives)}\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "conf_matrix.set_index(\"real\", inplace=True)\n",
    "\n",
    "conf_matrix[[\"pred_true\", \"pred_false\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Falsos Negativos\n",
    "\n",
    "Veamos los 20 falsos negativos en los cuales nuestro modelo se equivoca más"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21535</th>\n",
       "      <td>#VOX critica duramente a ⁦@pablocasado_⁩ “Les da la bienvenida como si fuera un cartel del welcome de #Carmena “ ¡Expulsión inmediata de moromierdas!  https://t.co/HaySDibj2l</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.043542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21152</th>\n",
       "      <td>@NatyBurgos15 @Horaciogenta @PaolaPa05 Cállate vos Black and bitch como la puta falsa abogada. Espero se pudra en la cárcel y devuelva todo. Dedícate a estudiar en serio. La droga es mala. Cristin...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.053808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22560</th>\n",
       "      <td>@CarlaEUrena @FelipeCalderon Callateeeee !! Puta gallina cacarea !! Ni idea tienes de nada .</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.080199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23415</th>\n",
       "      <td>Exacto! #puralata #feminazis #HombreAbusado https://t.co/zTEvaftxcd</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.084722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21215</th>\n",
       "      <td>Que tal otro aporte anónimo son de la puta de mi esposa, así los dejó en navidad enseñando le los calzones al vecino rt y siguenos @TANGASUCIA @Tangasrobadas https://t.co/70ioUjveqY</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.092792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24925</th>\n",
       "      <td>La verdad que si, no veia la hora de llegar a los 5 mil amigos😂 ahora de a poco voy borrando esos indios indocumentados😂 https://t.co/Xw8q4GLM8v</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.144993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24401</th>\n",
       "      <td>No habrá que darles de lo 'nuestro'. Ellos se apropiarán de lo 'suyo'.  @Manuj_Hidalgo les dice ☝️ a quienes piensan q hay q jubilar a los mayores para q trabajen los jóvenes o q los q vienen d fu...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.151047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21072</th>\n",
       "      <td>Voy andando por la calle y me han dicho cosas en árabe ya que me estas acosando dime lo en español para que te entienda, asique como yo no les entiendo me he cagado en su puta madre en árabe para ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.152465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24590</th>\n",
       "      <td>SOMOS MUCHO MÁS DE LO QUE SUEÑAS SER PINCHÉ SUDACA CAGADO, y si no te gusta dime donde nos topamos pa reventarnos el hocico 1:1 @amarchesin1 https://t.co/vHHPBKS1AK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.157015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22008</th>\n",
       "      <td>@Namurii @medicaster_xz @Yorha2d @KeloKlau_ Se merecía de puta y no quiere que le metan la verga, jajaja no mamen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24488</th>\n",
       "      <td>Un indocumentado...habla y habla y no para y acaba hablando su ignorancia.Empezamos bien. https://t.co/SMRsqjtEBT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22168</th>\n",
       "      <td>Mi novia es como mi polla. Sé que está ahí porque de vez en cuando se levanta. Una de mis pantalones y la otra de la cama.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.185569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24864</th>\n",
       "      <td>rosalia me podría decir sudaca i wouldn't give a fuck</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24481</th>\n",
       "      <td>En que se parece superman a una mujer lista: En que los dos son personajes de ciencia-ficción.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.193847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24431</th>\n",
       "      <td>Yo nunca le he dicho puta a una mujer pero socia tú eres senda petardo 😂😂 estas navidades no voy a comprar pirotecnias ya contigo vasta 😂😂😂</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.194956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24746</th>\n",
       "      <td>@rU_1970 muestra como teni esa vagina de puta caliente llena de moco wacha culia hija de la perra traga semen 146:146</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.199260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21887</th>\n",
       "      <td>@A3Noticias Esta tiene que ser una 'fake new' porque las mujeres son siempre las víctimas, no las hijas de puta maltratadoras. Eso o el chaval es un machista que se lo ha merecido (machete al mach...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.222684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21727</th>\n",
       "      <td>España crea su primer 'campo de refugiados' en la bahía de Algeciras https://t.co/l7Zmh2kvPN vía @elmundoes Gran noticia para el contribuyente español. Españoles a pagar, a pagar. Las pensiones pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.224704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20337</th>\n",
       "      <td>Ni armar un complot para matar un presidente saben... #Sudacas..</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.236325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21339</th>\n",
       "      <td>La #Inmigración, como sabemos, es mala, se ha demostrado en media #Europa. Pero no debemos caer en la tentación de salpicar nuestra #soberanía con extremismos, tenemos que defender lo nuestro, si,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.236573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                          text  \\\n",
       "id                                                                                                                                                                                                               \n",
       "21535                           #VOX critica duramente a ⁦@pablocasado_⁩ “Les da la bienvenida como si fuera un cartel del welcome de #Carmena “ ¡Expulsión inmediata de moromierdas!  https://t.co/HaySDibj2l   \n",
       "21152  @NatyBurgos15 @Horaciogenta @PaolaPa05 Cállate vos Black and bitch como la puta falsa abogada. Espero se pudra en la cárcel y devuelva todo. Dedícate a estudiar en serio. La droga es mala. Cristin...   \n",
       "22560                                                                                                             @CarlaEUrena @FelipeCalderon Callateeeee !! Puta gallina cacarea !! Ni idea tienes de nada .   \n",
       "23415                                                                                                                                      Exacto! #puralata #feminazis #HombreAbusado https://t.co/zTEvaftxcd   \n",
       "21215                    Que tal otro aporte anónimo son de la puta de mi esposa, así los dejó en navidad enseñando le los calzones al vecino rt y siguenos @TANGASUCIA @Tangasrobadas https://t.co/70ioUjveqY   \n",
       "24925                                                         La verdad que si, no veia la hora de llegar a los 5 mil amigos😂 ahora de a poco voy borrando esos indios indocumentados😂 https://t.co/Xw8q4GLM8v   \n",
       "24401  No habrá que darles de lo 'nuestro'. Ellos se apropiarán de lo 'suyo'.  @Manuj_Hidalgo les dice ☝️ a quienes piensan q hay q jubilar a los mayores para q trabajen los jóvenes o q los q vienen d fu...   \n",
       "21072  Voy andando por la calle y me han dicho cosas en árabe ya que me estas acosando dime lo en español para que te entienda, asique como yo no les entiendo me he cagado en su puta madre en árabe para ...   \n",
       "24590                                     SOMOS MUCHO MÁS DE LO QUE SUEÑAS SER PINCHÉ SUDACA CAGADO, y si no te gusta dime donde nos topamos pa reventarnos el hocico 1:1 @amarchesin1 https://t.co/vHHPBKS1AK   \n",
       "22008                                                                                        @Namurii @medicaster_xz @Yorha2d @KeloKlau_ Se merecía de puta y no quiere que le metan la verga, jajaja no mamen   \n",
       "24488                                                                                        Un indocumentado...habla y habla y no para y acaba hablando su ignorancia.Empezamos bien. https://t.co/SMRsqjtEBT   \n",
       "22168                                                                               Mi novia es como mi polla. Sé que está ahí porque de vez en cuando se levanta. Una de mis pantalones y la otra de la cama.   \n",
       "24864                                                                                                                                                    rosalia me podría decir sudaca i wouldn't give a fuck   \n",
       "24481                                                                                                           En que se parece superman a una mujer lista: En que los dos son personajes de ciencia-ficción.   \n",
       "24431                                                              Yo nunca le he dicho puta a una mujer pero socia tú eres senda petardo 😂😂 estas navidades no voy a comprar pirotecnias ya contigo vasta 😂😂😂   \n",
       "24746                                                                                    @rU_1970 muestra como teni esa vagina de puta caliente llena de moco wacha culia hija de la perra traga semen 146:146   \n",
       "21887  @A3Noticias Esta tiene que ser una 'fake new' porque las mujeres son siempre las víctimas, no las hijas de puta maltratadoras. Eso o el chaval es un machista que se lo ha merecido (machete al mach...   \n",
       "21727  España crea su primer 'campo de refugiados' en la bahía de Algeciras https://t.co/l7Zmh2kvPN vía @elmundoes Gran noticia para el contribuyente español. Españoles a pagar, a pagar. Las pensiones pe...   \n",
       "20337                                                                                                                                         Ni armar un complot para matar un presidente saben... #Sudacas..   \n",
       "21339  La #Inmigración, como sabemos, es mala, se ha demostrado en media #Europa. Pero no debemos caer en la tentación de salpicar nuestra #soberanía con extremismos, tenemos que defender lo nuestro, si,...   \n",
       "\n",
       "       HS  TR  AG     proba  \n",
       "id                           \n",
       "21535   1   0   1  0.043542  \n",
       "21152   1   1   1  0.053808  \n",
       "22560   1   1   1  0.080199  \n",
       "23415   1   0   1  0.084722  \n",
       "21215   1   1   1  0.092792  \n",
       "24925   1   0   0  0.144993  \n",
       "24401   1   0   1  0.151047  \n",
       "21072   1   0   1  0.152465  \n",
       "24590   1   1   1  0.157015  \n",
       "22008   1   1   0  0.168723  \n",
       "24488   1   0   0  0.183900  \n",
       "22168   1   1   0  0.185569  \n",
       "24864   1   1   1  0.186539  \n",
       "24481   1   0   0  0.193847  \n",
       "24431   1   1   0  0.194956  \n",
       "24746   1   1   1  0.199260  \n",
       "21887   1   0   1  0.222684  \n",
       "21727   1   0   0  0.224704  \n",
       "20337   1   0   1  0.236325  \n",
       "21339   1   0   1  0.236573  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_negatives.iloc[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Qué onda la longitud de la secuencia?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 734., 1109., 1019.,  655.,  386.,  315.,  210.,   57.,   13.,\n",
       "           2.]),\n",
       " array([ 2., 10., 18., 26., 34., 42., 50., 58., 66., 74., 82.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokens_dev = df_dev[\"text\"].apply(lambda t: len(tokenizer.tokenize(t)))\n",
    "tokens_train = df_train[\"text\"].apply(lambda t: len(tokenizer.tokenize(t)))\n",
    "\n",
    "plt.hist(tokens_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>HS</th>\n",
       "      <th>TR</th>\n",
       "      <th>AG</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22517</th>\n",
       "      <td>-¿eres poblana? -soy colombiana -¿y por qué eres güera? en Colombia TODAS son morenas, fui a Colombia 3 veces. -ok, mentí 🙄 solo me pinto el cabello</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.971048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22416</th>\n",
       "      <td>@Pepe_ElEzequiel Puta!? Puta con las que vos te metes Esa mujer es un ángel enviado por Jesucristo nuestro señor para darnos esa paz que buscamos sólo con verla</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.967200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21892</th>\n",
       "      <td>@saancisco Callate puta tu eres el traidor.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.963039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21497</th>\n",
       "      <td>Talvez quede como zorra pero te lo mereces Socia😎</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.947185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24172</th>\n",
       "      <td>Y los que no se denuncian... hagamos números. El acoso a las mujeres es diario. No todos los hombres son acosadores, pero todas las mujeres han sido acosadas. https://t.co/jza9llhqWA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.945128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21255</th>\n",
       "      <td>Y tú eres una maldita escoria con una repugnante inclinación por los hombres.. ¡Seres como tú me producen náuseas! https://t.co/TaHDcpZdnm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24159</th>\n",
       "      <td>@LeonDeJapon Jajajajajaja tu eres uno, diego jajajajajaja mentira perra perra perrraaaaaaaaaa de mierda jajaja ♡</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.931898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24402</th>\n",
       "      <td>zorra eres tu que no te sientes hombre y no entiendes que toda mujer es bella</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.925799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21671</th>\n",
       "      <td>@danlntifada @fatinascidi @manolivizoso @Zurine3 @Jvillaver2 https://t.co/HG4ljNlwR3, coño entonces tu eres de lo que te guias por el antes de cristo y despues de cristo,cuando llegamos los Españo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.921038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22115</th>\n",
       "      <td>Una chica no es una guarra, una zorra, una puta, una buscan por su manera de vestir.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.915731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22014</th>\n",
       "      <td>Ni tú eres una zorra hambrienta ni yo un incorregible depredador.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.910154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20376</th>\n",
       "      <td>Zorra tu abuela, imbécil.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.890432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21050</th>\n",
       "      <td>@GvyMC @Wismichu Callate puta rata😂😂 ya estas llorando??</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.877848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22848</th>\n",
       "      <td>Pinche bebé pendejo cállate el hocico, nomás te están cortando el pelo. Puta chillona</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.856129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22938</th>\n",
       "      <td>Masculinidad frágil es mi crush francés echándose un clavado al cenote solo porque un grupo de árabes le dijo pussy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.855542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23320</th>\n",
       "      <td>@HenriFalconLara No digas nada que tú eres cómplice de lo que está pasando... y recuerda seguirás siendo la perra más perra de Venezuela</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.854512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21966</th>\n",
       "      <td>escúchame ya me jodería jajajajajajajajajajajajajaja si pa ti enseñar es ser una guarra yo soy biennnn guarra, menos mal que no me respeto que si no... https://t.co/yt5fHp99b7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.848084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24209</th>\n",
       "      <td>¿Ya les he contado que si fuera rica/millonaria sería toda una PERRA?, Yisus me está enseñando humildad... Y no he aprendido mucho ldvdd.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.813778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22846</th>\n",
       "      <td>Si a mi se me ocurre enseñar por twitter una conversación de whatsap con mi madre me manda a dormir a la caseta de mi perra.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.808360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22457</th>\n",
       "      <td>De Colombia tocará emigrar en pateras, como hacen los africanos subsaharianos.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.795402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                          text  \\\n",
       "id                                                                                                                                                                                                               \n",
       "22517                                                     -¿eres poblana? -soy colombiana -¿y por qué eres güera? en Colombia TODAS son morenas, fui a Colombia 3 veces. -ok, mentí 🙄 solo me pinto el cabello   \n",
       "22416                                         @Pepe_ElEzequiel Puta!? Puta con las que vos te metes Esa mujer es un ángel enviado por Jesucristo nuestro señor para darnos esa paz que buscamos sólo con verla   \n",
       "21892                                                                                                                                                              @saancisco Callate puta tu eres el traidor.   \n",
       "21497                                                                                                                                                        Talvez quede como zorra pero te lo mereces Socia😎   \n",
       "24172                   Y los que no se denuncian... hagamos números. El acoso a las mujeres es diario. No todos los hombres son acosadores, pero todas las mujeres han sido acosadas. https://t.co/jza9llhqWA   \n",
       "21255                                                               Y tú eres una maldita escoria con una repugnante inclinación por los hombres.. ¡Seres como tú me producen náuseas! https://t.co/TaHDcpZdnm   \n",
       "24159                                                                                         @LeonDeJapon Jajajajajaja tu eres uno, diego jajajajajaja mentira perra perra perrraaaaaaaaaa de mierda jajaja ♡   \n",
       "24402                                                                                                                            zorra eres tu que no te sientes hombre y no entiendes que toda mujer es bella   \n",
       "21671  @danlntifada @fatinascidi @manolivizoso @Zurine3 @Jvillaver2 https://t.co/HG4ljNlwR3, coño entonces tu eres de lo que te guias por el antes de cristo y despues de cristo,cuando llegamos los Españo...   \n",
       "22115                                                                                                                     Una chica no es una guarra, una zorra, una puta, una buscan por su manera de vestir.   \n",
       "22014                                                                                                                                        Ni tú eres una zorra hambrienta ni yo un incorregible depredador.   \n",
       "20376                                                                                                                                                                                Zorra tu abuela, imbécil.   \n",
       "21050                                                                                                                                                 @GvyMC @Wismichu Callate puta rata😂😂 ya estas llorando??   \n",
       "22848                                                                                                                    Pinche bebé pendejo cállate el hocico, nomás te están cortando el pelo. Puta chillona   \n",
       "22938                                                                                      Masculinidad frágil es mi crush francés echándose un clavado al cenote solo porque un grupo de árabes le dijo pussy   \n",
       "23320                                                                 @HenriFalconLara No digas nada que tú eres cómplice de lo que está pasando... y recuerda seguirás siendo la perra más perra de Venezuela   \n",
       "21966                          escúchame ya me jodería jajajajajajajajajajajajajaja si pa ti enseñar es ser una guarra yo soy biennnn guarra, menos mal que no me respeto que si no... https://t.co/yt5fHp99b7   \n",
       "24209                                                                ¿Ya les he contado que si fuera rica/millonaria sería toda una PERRA?, Yisus me está enseñando humildad... Y no he aprendido mucho ldvdd.   \n",
       "22846                                                                             Si a mi se me ocurre enseñar por twitter una conversación de whatsap con mi madre me manda a dormir a la caseta de mi perra.   \n",
       "22457                                                                                                                           De Colombia tocará emigrar en pateras, como hacen los africanos subsaharianos.   \n",
       "\n",
       "       HS  TR  AG     proba  \n",
       "id                           \n",
       "22517   0   0   0  0.971048  \n",
       "22416   0   0   0  0.967200  \n",
       "21892   0   0   0  0.963039  \n",
       "21497   0   0   0  0.947185  \n",
       "24172   0   0   0  0.945128  \n",
       "21255   0   0   0  0.943286  \n",
       "24159   0   0   0  0.931898  \n",
       "24402   0   0   0  0.925799  \n",
       "21671   0   0   0  0.921038  \n",
       "22115   0   0   0  0.915731  \n",
       "22014   0   0   0  0.910154  \n",
       "20376   0   0   0  0.890432  \n",
       "21050   0   0   0  0.877848  \n",
       "22848   0   0   0  0.856129  \n",
       "22938   0   0   0  0.855542  \n",
       "23320   0   0   0  0.854512  \n",
       "21966   0   0   0  0.848084  \n",
       "24209   0   0   0  0.813778  \n",
       "22846   0   0   0  0.808360  \n",
       "22457   0   0   0  0.795402  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 200)\n",
    "false_positives.iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['easyjet',\n",
       " 'quiere',\n",
       " 'duplicar',\n",
       " 'el',\n",
       " 'número',\n",
       " 'de',\n",
       " 'mujeres',\n",
       " 'piloto',\n",
       " \"'\",\n",
       " 'verás',\n",
       " 'tú',\n",
       " 'para',\n",
       " 'aparcar',\n",
       " 'el',\n",
       " 'avión',\n",
       " '..',\n",
       " 'http://t.co/46NuLkm09x',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

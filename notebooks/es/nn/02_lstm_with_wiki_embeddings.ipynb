{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM con WIKI\n",
    "\n",
    "Modelo básico con los embeddings de fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instancias de entrenamiento: 4500\n",
      "Instancias de desarrollo: 500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "np.random.seed(2019)\n",
    "tf.random.set_random_seed(2019)\n",
    "random.seed(2019)\n",
    "\n",
    "df_dev = pd.read_table(\"../../../data/es/dev_es.tsv\", index_col=\"id\", quoting=csv.QUOTE_NONE)\n",
    "df_train = pd.read_table(\"../../../data/es/train_es.tsv\", index_col=\"id\", quoting=csv.QUOTE_NONE)\n",
    "df_test = pd.read_table(\"../../../data/es/reference_es.tsv\", header=None, \n",
    "                        names=[\"text\", \"HS\", \"TR\", \"AG\"], quoting=csv.QUOTE_NONE)\n",
    "\n",
    "\n",
    "text_train, y_train = df_train[\"text\"], df_train[\"HS\"]\n",
    "text_dev, y_dev = df_dev[\"text\"], df_dev[\"HS\"]\n",
    "text_test, y_test = df_test[\"text\"], df_test[\"HS\"]\n",
    "\n",
    "print(\"Instancias de entrenamiento: {}\".format(len(df_train)))\n",
    "print(\"Instancias de desarrollo: {}\".format(len(df_dev)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tengo que hacer dos cosas:\n",
    "\n",
    "- Primero, convertir los tweets a secuencias de texto\n",
    "- Luego, paddear las secuencias a cierta longitud (Keras necesita esto para poder paralelizar cálculo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "num_words = 200000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(text_train)\n",
    "X_dev = tokenizer.texts_to_sequences(text_dev)\n",
    "X_test = tokenizer.texts_to_sequences(text_test)\n",
    "max_length = 30\n",
    "\n",
    "X_train = pad_sequences(X_train, max_length)\n",
    "X_dev = pad_sequences(X_dev, max_length)\n",
    "X_test = pad_sequences(X_test, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carguemos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available embeddings:  ['/home/jmperez/WordVectors/UBA_w5_200.vec', '/home/jmperez/WordVectors/wiki.es.vec', '/home/jmperez/WordVectors/UBA_w5_300.vec']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "path_to_embeddings = os.path.expanduser(\"/home/jmperez/WordVectors/\")\n",
    "\n",
    "print(\"Available embeddings: \", glob(os.path.join(path_to_embeddings, \"*.vec\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Embeddings\n",
    "\n",
    "Me quedo sólo con los embeddings de unigramas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "word_to_vec = {}\n",
    "\n",
    "with open(os.path.join(path_to_embeddings, \"wiki.es.vec\")) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            vec = np.asarray(values[1:], dtype=\"float32\")\n",
    "            word_to_vec[word] = vec\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "embedding_size = len(word_to_vec[\"hola\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((num_words, embedding_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = word_to_vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use `var.assign(var * other)` if you want assignment to the variable value or `x = x * y` if you want a new python Tensor object.\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/20\n",
      "4500/4500 [==============================] - 2s 555us/step - loss: 0.6631 - acc: 0.6189 - val_loss: 0.6587 - val_acc: 0.5760\n",
      "Epoch 2/20\n",
      "4500/4500 [==============================] - 1s 281us/step - loss: 0.6056 - acc: 0.6887 - val_loss: 0.6052 - val_acc: 0.6640\n",
      "Epoch 3/20\n",
      "4500/4500 [==============================] - 1s 281us/step - loss: 0.5585 - acc: 0.7256 - val_loss: 0.5861 - val_acc: 0.7000\n",
      "Epoch 4/20\n",
      "4500/4500 [==============================] - 1s 283us/step - loss: 0.5382 - acc: 0.7453 - val_loss: 0.5757 - val_acc: 0.7100\n",
      "Epoch 5/20\n",
      "4500/4500 [==============================] - 1s 283us/step - loss: 0.5151 - acc: 0.7573 - val_loss: 0.5728 - val_acc: 0.7140\n",
      "Epoch 6/20\n",
      "4500/4500 [==============================] - 1s 278us/step - loss: 0.4979 - acc: 0.7656 - val_loss: 0.5526 - val_acc: 0.7240\n",
      "Epoch 7/20\n",
      "4500/4500 [==============================] - 1s 289us/step - loss: 0.4870 - acc: 0.7740 - val_loss: 0.5557 - val_acc: 0.7260\n",
      "Epoch 8/20\n",
      "4500/4500 [==============================] - 1s 285us/step - loss: 0.4797 - acc: 0.7742 - val_loss: 0.5495 - val_acc: 0.7320\n",
      "Epoch 9/20\n",
      "4500/4500 [==============================] - 1s 275us/step - loss: 0.4705 - acc: 0.7853 - val_loss: 0.5394 - val_acc: 0.7380\n",
      "Epoch 10/20\n",
      "4500/4500 [==============================] - 1s 283us/step - loss: 0.4624 - acc: 0.7918 - val_loss: 0.5355 - val_acc: 0.7380\n",
      "Epoch 11/20\n",
      "4500/4500 [==============================] - 1s 281us/step - loss: 0.4500 - acc: 0.7947 - val_loss: 0.5400 - val_acc: 0.7340\n",
      "Epoch 12/20\n",
      "4500/4500 [==============================] - 1s 285us/step - loss: 0.4443 - acc: 0.7942 - val_loss: 0.5367 - val_acc: 0.7440\n",
      "Epoch 13/20\n",
      "4500/4500 [==============================] - 1s 287us/step - loss: 0.4425 - acc: 0.8013 - val_loss: 0.5463 - val_acc: 0.7460\n",
      "Epoch 14/20\n",
      "4500/4500 [==============================] - 1s 282us/step - loss: 0.4346 - acc: 0.8022 - val_loss: 0.5401 - val_acc: 0.7340\n",
      "Epoch 15/20\n",
      "4500/4500 [==============================] - 1s 282us/step - loss: 0.4296 - acc: 0.8042 - val_loss: 0.5346 - val_acc: 0.7440\n",
      "Epoch 16/20\n",
      "4500/4500 [==============================] - 1s 280us/step - loss: 0.4304 - acc: 0.8100 - val_loss: 0.5374 - val_acc: 0.7400\n",
      "Epoch 17/20\n",
      "4500/4500 [==============================] - 1s 282us/step - loss: 0.4214 - acc: 0.8096 - val_loss: 0.5467 - val_acc: 0.7480\n",
      "Epoch 18/20\n",
      "4500/4500 [==============================] - 1s 282us/step - loss: 0.4156 - acc: 0.8202 - val_loss: 0.5394 - val_acc: 0.7400\n",
      "Epoch 19/20\n",
      "4500/4500 [==============================] - 1s 281us/step - loss: 0.4169 - acc: 0.8180 - val_loss: 0.5407 - val_acc: 0.7500\n",
      "Epoch 20/20\n",
      "4500/4500 [==============================] - 1s 278us/step - loss: 0.4128 - acc: 0.8209 - val_loss: 0.5427 - val_acc: 0.7480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbc960d4c88>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import CuDNNLSTM, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_size, input_length=max_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(CuDNNLSTM(100))\n",
    "model.add(Dropout(0.65))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.35))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "optimizer_args = {\n",
    "    \"lr\": 0.001,\n",
    "    \"decay\": 0.01\n",
    "}\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_dev, y_dev), \n",
    "          epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluación sobre dev\n",
      "500/500 [==============================] - 0s 101us/step\n",
      "Loss           : 0.5427\n",
      "Accuracy       : 0.7480\n",
      "Precision(1)   : 0.7143\n",
      "Precision(1)   : 0.7754\n",
      "Precision(avg) : 0.7448\n",
      "\n",
      "Recall(1)      : 0.7207\n",
      "Recall(0)      : 0.7698\n",
      "Recall(avg)    : 0.7453\n",
      "\n",
      "F1(1)          : 0.7175\n",
      "F1(0)          : 0.7726\n",
      "F1(avg)        : 0.7450\n",
      "\n",
      "\n",
      "Evaluación sobre test\n",
      "1600/1600 [==============================] - 0s 97us/step\n",
      "Loss           : 0.6044\n",
      "Accuracy       : 0.6956\n",
      "Precision(1)   : 0.6297\n",
      "Precision(1)   : 0.7428\n",
      "Precision(avg) : 0.6862\n",
      "\n",
      "Recall(1)      : 0.6364\n",
      "Recall(0)      : 0.7372\n",
      "Recall(avg)    : 0.6868\n",
      "\n",
      "F1(1)          : 0.6330\n",
      "F1(0)          : 0.7400\n",
      "F1(avg)        : 0.6865\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "\n",
    "print(\"Evaluación sobre dev\")\n",
    "print_evaluation(model, X_dev, y_dev)\n",
    "print(\"\\n\\nEvaluación sobre test\")\n",
    "print_evaluation(model, X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 30, 300)           60000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               321600    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               25728     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 60,347,457\n",
      "Trainable params: 347,457\n",
      "Non-trainable params: 60,000,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/25\n",
      "4500/4500 [==============================] - 2s 520us/step - loss: 0.6478 - acc: 0.6369 - val_loss: 0.6270 - val_acc: 0.6560\n",
      "Epoch 2/25\n",
      "4500/4500 [==============================] - 2s 376us/step - loss: 0.5697 - acc: 0.7122 - val_loss: 0.5950 - val_acc: 0.7040\n",
      "Epoch 3/25\n",
      "4500/4500 [==============================] - 2s 376us/step - loss: 0.5325 - acc: 0.7460 - val_loss: 0.5659 - val_acc: 0.7380\n",
      "Epoch 4/25\n",
      "4500/4500 [==============================] - 2s 395us/step - loss: 0.5027 - acc: 0.7647 - val_loss: 0.5594 - val_acc: 0.7360\n",
      "Epoch 5/25\n",
      "4500/4500 [==============================] - 2s 385us/step - loss: 0.4884 - acc: 0.7742 - val_loss: 0.5433 - val_acc: 0.7420\n",
      "Epoch 6/25\n",
      "4500/4500 [==============================] - 2s 388us/step - loss: 0.4717 - acc: 0.7824 - val_loss: 0.5391 - val_acc: 0.7460\n",
      "Epoch 7/25\n",
      "4500/4500 [==============================] - 2s 388us/step - loss: 0.4493 - acc: 0.7907 - val_loss: 0.5294 - val_acc: 0.7380\n",
      "Epoch 8/25\n",
      "4500/4500 [==============================] - 2s 391us/step - loss: 0.4444 - acc: 0.8007 - val_loss: 0.5213 - val_acc: 0.7500\n",
      "Epoch 9/25\n",
      "4500/4500 [==============================] - 2s 387us/step - loss: 0.4315 - acc: 0.8051 - val_loss: 0.5244 - val_acc: 0.7540\n",
      "Epoch 10/25\n",
      "4500/4500 [==============================] - 2s 385us/step - loss: 0.4210 - acc: 0.8107 - val_loss: 0.5187 - val_acc: 0.7600\n",
      "Epoch 11/25\n",
      "4500/4500 [==============================] - 2s 390us/step - loss: 0.4102 - acc: 0.8129 - val_loss: 0.5224 - val_acc: 0.7560\n",
      "Epoch 12/25\n",
      "4500/4500 [==============================] - 2s 389us/step - loss: 0.4031 - acc: 0.8244 - val_loss: 0.5199 - val_acc: 0.7540\n",
      "Epoch 13/25\n",
      "4500/4500 [==============================] - 2s 387us/step - loss: 0.3950 - acc: 0.8220 - val_loss: 0.5358 - val_acc: 0.7460\n",
      "Epoch 14/25\n",
      "4500/4500 [==============================] - 2s 385us/step - loss: 0.3905 - acc: 0.8320 - val_loss: 0.5162 - val_acc: 0.7720\n",
      "Epoch 15/25\n",
      "4500/4500 [==============================] - 2s 382us/step - loss: 0.3830 - acc: 0.8338 - val_loss: 0.5237 - val_acc: 0.7700\n",
      "Epoch 16/25\n",
      "4500/4500 [==============================] - 2s 386us/step - loss: 0.3782 - acc: 0.8380 - val_loss: 0.5175 - val_acc: 0.7740\n",
      "Epoch 17/25\n",
      "4500/4500 [==============================] - 2s 392us/step - loss: 0.3668 - acc: 0.8431 - val_loss: 0.5118 - val_acc: 0.7600\n",
      "Epoch 18/25\n",
      "4500/4500 [==============================] - 2s 389us/step - loss: 0.3669 - acc: 0.8440 - val_loss: 0.5126 - val_acc: 0.7720\n",
      "Epoch 19/25\n",
      "4500/4500 [==============================] - 2s 381us/step - loss: 0.3603 - acc: 0.8500 - val_loss: 0.5214 - val_acc: 0.7680\n",
      "Epoch 20/25\n",
      "4500/4500 [==============================] - 2s 390us/step - loss: 0.3532 - acc: 0.8498 - val_loss: 0.5196 - val_acc: 0.7700\n",
      "Epoch 21/25\n",
      "4500/4500 [==============================] - 2s 392us/step - loss: 0.3537 - acc: 0.8480 - val_loss: 0.5324 - val_acc: 0.7600\n",
      "Epoch 22/25\n",
      "4500/4500 [==============================] - 2s 388us/step - loss: 0.3518 - acc: 0.8538 - val_loss: 0.5211 - val_acc: 0.7700\n",
      "Epoch 23/25\n",
      "4500/4500 [==============================] - 2s 386us/step - loss: 0.3426 - acc: 0.8553 - val_loss: 0.5251 - val_acc: 0.7600\n",
      "Epoch 24/25\n",
      "4500/4500 [==============================] - 2s 383us/step - loss: 0.3335 - acc: 0.8584 - val_loss: 0.5290 - val_acc: 0.7760\n",
      "Epoch 25/25\n",
      "4500/4500 [==============================] - 2s 386us/step - loss: 0.3258 - acc: 0.8622 - val_loss: 0.5298 - val_acc: 0.7700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbaf40eb860>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "embedding_vector_length = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_size, input_length=max_length, \n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(Bidirectional(CuDNNLSTM(100)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=Adam(**optimizer_args), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=25, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluación sobre dev\n",
      "500/500 [==============================] - 0s 138us/step\n",
      "Loss           : 0.5298\n",
      "Accuracy       : 0.7700\n",
      "Precision(1)   : 0.7399\n",
      "Precision(1)   : 0.7942\n",
      "Precision(avg) : 0.7671\n",
      "\n",
      "Recall(1)      : 0.7432\n",
      "Recall(0)      : 0.7914\n",
      "Recall(avg)    : 0.7673\n",
      "\n",
      "F1(1)          : 0.7416\n",
      "F1(0)          : 0.7928\n",
      "F1(avg)        : 0.7672\n",
      "\n",
      "\n",
      "Evaluación sobre test\n",
      "1600/1600 [==============================] - 0s 137us/step\n",
      "Loss           : 0.6587\n",
      "Accuracy       : 0.6969\n",
      "Precision(1)   : 0.6296\n",
      "Precision(1)   : 0.7459\n",
      "Precision(avg) : 0.6878\n",
      "\n",
      "Recall(1)      : 0.6439\n",
      "Recall(0)      : 0.7340\n",
      "Recall(avg)    : 0.6890\n",
      "\n",
      "F1(1)          : 0.6367\n",
      "F1(0)          : 0.7399\n",
      "F1(avg)        : 0.6883\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "\n",
    "print(\"Evaluación sobre dev\")\n",
    "print_evaluation(model, X_dev, y_dev)\n",
    "print(\"\\n\\nEvaluación sobre test\")\n",
    "print_evaluation(model, X_test, y_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

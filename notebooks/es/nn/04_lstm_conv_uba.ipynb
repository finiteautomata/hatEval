{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM con embeddings\n",
    "\n",
    "Modelo básico con los embeddings de fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '../data/dev_es/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%ls ../data/dev_es/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_table(\"../../../data/es/train_es.tsv\", index_col=\"id\")\n",
    "df_dev = pd.read_table(\"../../../data/es/dev_es.tsv\", index_col=\"id\")\n",
    "\n",
    "text_train, y_train = df_train[\"text\"], df_train[\"HS\"]\n",
    "text_dev, y_dev = df_dev[\"text\"], df_dev[\"HS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tengo que hacer dos cosas:\n",
    "\n",
    "- Primero, convertir los tweets a secuencias de texto\n",
    "- Luego, paddear las secuencias a cierta longitud (Keras necesita esto para poder paralelizar cálculo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "num_words = 200000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(text_train)\n",
    "X_dev = tokenizer.texts_to_sequences(text_dev)\n",
    "\n",
    "max_length = 30\n",
    "\n",
    "X_train = pad_sequences(X_train, max_length)\n",
    "X_dev = pad_sequences(X_dev, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carguemos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available embeddings:  ['/home/jmperez/WordVectors/UBA_w5_200.vec', '/home/jmperez/WordVectors/wiki.es.vec', '/home/jmperez/WordVectors/UBA_w5_300.vec']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "path_to_embeddings = os.path.expanduser(\"/home/jmperez/WordVectors/\")\n",
    "\n",
    "print(\"Available embeddings: \", glob(os.path.join(path_to_embeddings, \"*.vec\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "\n",
      "Problema con la sig línea:\n",
      "['.', '.', '-0.22232', '0.0052569', '0.47066', '0.13836', '0.15991', '0.19504', '0.00067885', '0.020299']\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "\n",
      "Problema con la sig línea:\n",
      "['.', '...', '-0.11666', '-0.083768', '0.028919', '0.29973', '0.21017', '0.27808', '0.063251', '0.090223']\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "\n",
      "Problema con la sig línea:\n",
      "['.', '..', '-0.43752', '-0.0016885', '0.1533', '0.28071', '0.18051', '0.28698', '0.11806', '0.044891']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "word_to_vec = {}\n",
    "\n",
    "with open(os.path.join(path_to_embeddings, \"UBA_w5_300.vec\")) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            vec = np.asarray(values[1:], dtype=\"float32\")\n",
    "        except:\n",
    "            print((\"*\" * 80  + \"\\n\")*3)\n",
    "            print(\"Problema con la sig línea:\")\n",
    "            print(values[:10])\n",
    "            word = values[1]\n",
    "            vec = np.asarray(values[2:], dtype=\"float32\")\n",
    "        word_to_vec[word] = vec\n",
    "        \n",
    "embedding_size = len(word_to_vec[\"hola\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((num_words, embedding_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = word_to_vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout, Conv1D, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_size, input_length=max_length, \n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "4469/4469 [==============================] - 8s 2ms/step - loss: 0.6349 - acc: 0.6449 - val_loss: 0.5699 - val_acc: 0.7100\n",
      "Epoch 2/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.5316 - acc: 0.7342 - val_loss: 0.5439 - val_acc: 0.7300\n",
      "Epoch 3/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.4595 - acc: 0.7874 - val_loss: 0.5075 - val_acc: 0.7400\n",
      "Epoch 4/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.4123 - acc: 0.8085 - val_loss: 0.5116 - val_acc: 0.7800\n",
      "Epoch 5/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.3866 - acc: 0.8328 - val_loss: 0.4874 - val_acc: 0.7640\n",
      "Epoch 6/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.3281 - acc: 0.8575 - val_loss: 0.5325 - val_acc: 0.7740\n",
      "Epoch 7/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.2789 - acc: 0.8870 - val_loss: 0.5408 - val_acc: 0.7840\n",
      "Epoch 8/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.2348 - acc: 0.9056 - val_loss: 0.5269 - val_acc: 0.7640\n",
      "Epoch 9/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.1942 - acc: 0.9257 - val_loss: 0.6282 - val_acc: 0.7680\n",
      "Epoch 10/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.1651 - acc: 0.9324 - val_loss: 0.7420 - val_acc: 0.7700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6e1065c978>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 623us/step\n",
      "Loss        : 0.7420\n",
      "Accuracy    : 0.7700\n",
      "Precision   : 0.7831\n",
      "Recall      : 0.6667\n",
      "F1          : 0.7202\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "\n",
    "print_evaluation(model, X_dev, y_dev.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout, Conv1D, Flatten, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_size, input_length=max_length, \n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "4469/4469 [==============================] - 13s 3ms/step - loss: 0.6197 - acc: 0.6632 - val_loss: 0.5475 - val_acc: 0.7200\n",
      "Epoch 2/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.5058 - acc: 0.7588 - val_loss: 0.5986 - val_acc: 0.6680\n",
      "Epoch 3/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.4548 - acc: 0.7870 - val_loss: 0.5036 - val_acc: 0.7400\n",
      "Epoch 4/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.4001 - acc: 0.8241 - val_loss: 0.4640 - val_acc: 0.7660\n",
      "Epoch 5/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.3556 - acc: 0.8434 - val_loss: 0.4233 - val_acc: 0.7960\n",
      "Epoch 6/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.3197 - acc: 0.8633 - val_loss: 0.5232 - val_acc: 0.7580\n",
      "Epoch 7/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.2751 - acc: 0.8787 - val_loss: 0.4584 - val_acc: 0.8000\n",
      "Epoch 8/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.2349 - acc: 0.9015 - val_loss: 0.5139 - val_acc: 0.7820\n",
      "Epoch 9/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.1872 - acc: 0.9221 - val_loss: 0.5657 - val_acc: 0.7940\n",
      "Epoch 10/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.1452 - acc: 0.9479 - val_loss: 0.6086 - val_acc: 0.8020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6de3ffa860>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 1ms/step\n",
      "Loss        : 0.6086\n",
      "Accuracy    : 0.8020\n",
      "Precision   : 0.7552\n",
      "Recall      : 0.8198\n",
      "F1          : 0.7862\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "\n",
    "print_evaluation(model, X_dev, y_dev.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probemos GRU + Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 500 samples\n",
      "Epoch 1/20\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.6341 - acc: 0.6444 - val_loss: 0.5989 - val_acc: 0.6540\n",
      "Epoch 2/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.5250 - acc: 0.7375 - val_loss: 0.5991 - val_acc: 0.6820\n",
      "Epoch 3/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.4605 - acc: 0.7809 - val_loss: 0.6370 - val_acc: 0.6820\n",
      "Epoch 4/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.4063 - acc: 0.8156 - val_loss: 0.5146 - val_acc: 0.7800\n",
      "Epoch 5/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.3305 - acc: 0.8568 - val_loss: 0.4906 - val_acc: 0.7840\n",
      "Epoch 6/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.2764 - acc: 0.8850 - val_loss: 0.5166 - val_acc: 0.7820\n",
      "Epoch 7/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.2351 - acc: 0.9047 - val_loss: 0.5812 - val_acc: 0.7720\n",
      "Epoch 8/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.1856 - acc: 0.9244 - val_loss: 0.6492 - val_acc: 0.7820\n",
      "Epoch 9/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.1575 - acc: 0.9369 - val_loss: 0.7143 - val_acc: 0.7860\n",
      "Epoch 10/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.1179 - acc: 0.9550 - val_loss: 0.8319 - val_acc: 0.7680\n",
      "Epoch 11/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.0874 - acc: 0.9669 - val_loss: 0.9216 - val_acc: 0.7700\n",
      "Epoch 12/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.0811 - acc: 0.9682 - val_loss: 0.8581 - val_acc: 0.7680\n",
      "Epoch 13/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.0654 - acc: 0.9765 - val_loss: 0.8837 - val_acc: 0.7880\n",
      "Epoch 14/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.0619 - acc: 0.9767 - val_loss: 0.9666 - val_acc: 0.7600\n",
      "Epoch 15/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.0440 - acc: 0.9825 - val_loss: 1.0883 - val_acc: 0.7820\n",
      "Epoch 16/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.0400 - acc: 0.9861 - val_loss: 1.3059 - val_acc: 0.7820\n",
      "Epoch 17/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.0495 - acc: 0.9817 - val_loss: 1.3112 - val_acc: 0.7640\n",
      "Epoch 18/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.0246 - acc: 0.9917 - val_loss: 1.4510 - val_acc: 0.7520\n",
      "Epoch 19/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.0371 - acc: 0.9852 - val_loss: 1.3994 - val_acc: 0.7640\n",
      "Epoch 20/20\n",
      "4469/4469 [==============================] - 6s 1ms/step - loss: 0.0408 - acc: 0.9850 - val_loss: 1.3919 - val_acc: 0.7680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6d3c7257f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_size, input_length=max_length, \n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(GRU(100, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 501us/step\n",
      "Loss        : 1.3919\n",
      "Accuracy    : 0.7680\n",
      "Precision   : 0.7760\n",
      "Recall      : 0.6712\n",
      "F1          : 0.7198\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "\n",
    "print_evaluation(model, X_dev, y_dev.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Y si probamos Conv+GRU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 500 samples\n",
      "Epoch 1/20\n",
      "4469/4469 [==============================] - 5s 1ms/step - loss: 0.6373 - acc: 0.6435 - val_loss: 0.5859 - val_acc: 0.6960\n",
      "Epoch 2/20\n",
      "4469/4469 [==============================] - 4s 840us/step - loss: 0.5000 - acc: 0.7628 - val_loss: 0.5336 - val_acc: 0.7300\n",
      "Epoch 3/20\n",
      "4469/4469 [==============================] - 4s 863us/step - loss: 0.3874 - acc: 0.8360 - val_loss: 0.4763 - val_acc: 0.8020\n",
      "Epoch 4/20\n",
      "4469/4469 [==============================] - 4s 863us/step - loss: 0.2824 - acc: 0.8859 - val_loss: 0.5421 - val_acc: 0.7800\n",
      "Epoch 5/20\n",
      "4469/4469 [==============================] - 4s 835us/step - loss: 0.1955 - acc: 0.9268 - val_loss: 0.5875 - val_acc: 0.7460\n",
      "Epoch 6/20\n",
      "4469/4469 [==============================] - 4s 795us/step - loss: 0.1326 - acc: 0.9550 - val_loss: 0.6413 - val_acc: 0.7680\n",
      "Epoch 7/20\n",
      "4469/4469 [==============================] - 4s 838us/step - loss: 0.0822 - acc: 0.9734 - val_loss: 0.9684 - val_acc: 0.7100\n",
      "Epoch 8/20\n",
      "4469/4469 [==============================] - 4s 836us/step - loss: 0.0528 - acc: 0.9830 - val_loss: 0.9675 - val_acc: 0.7720\n",
      "Epoch 9/20\n",
      "4469/4469 [==============================] - 4s 795us/step - loss: 0.0457 - acc: 0.9837 - val_loss: 1.0006 - val_acc: 0.7920\n",
      "Epoch 10/20\n",
      "4469/4469 [==============================] - 4s 836us/step - loss: 0.0415 - acc: 0.9850 - val_loss: 0.9611 - val_acc: 0.7840\n",
      "Epoch 11/20\n",
      "4469/4469 [==============================] - 4s 842us/step - loss: 0.0536 - acc: 0.9817 - val_loss: 0.8239 - val_acc: 0.7740\n",
      "Epoch 12/20\n",
      "4469/4469 [==============================] - 4s 848us/step - loss: 0.0445 - acc: 0.9857 - val_loss: 1.1055 - val_acc: 0.7860\n",
      "Epoch 13/20\n",
      "4469/4469 [==============================] - 3s 775us/step - loss: 0.0359 - acc: 0.9899 - val_loss: 0.9656 - val_acc: 0.7860\n",
      "Epoch 14/20\n",
      "4469/4469 [==============================] - 3s 770us/step - loss: 0.0191 - acc: 0.9957 - val_loss: 1.4055 - val_acc: 0.7640\n",
      "Epoch 15/20\n",
      "4469/4469 [==============================] - 4s 785us/step - loss: 0.0536 - acc: 0.9823 - val_loss: 1.0963 - val_acc: 0.7820\n",
      "Epoch 16/20\n",
      "4469/4469 [==============================] - 4s 835us/step - loss: 0.0096 - acc: 0.9975 - val_loss: 1.4095 - val_acc: 0.7800\n",
      "Epoch 17/20\n",
      "4469/4469 [==============================] - 4s 821us/step - loss: 0.0048 - acc: 0.9991 - val_loss: 1.5470 - val_acc: 0.7900\n",
      "Epoch 18/20\n",
      "4469/4469 [==============================] - 4s 869us/step - loss: 7.2414e-04 - acc: 0.9998 - val_loss: 1.6427 - val_acc: 0.7880\n",
      "Epoch 19/20\n",
      "4469/4469 [==============================] - 4s 843us/step - loss: 0.0023 - acc: 0.9989 - val_loss: 1.5775 - val_acc: 0.7800\n",
      "Epoch 20/20\n",
      "4469/4469 [==============================] - 4s 839us/step - loss: 0.0475 - acc: 0.9846 - val_loss: 1.5117 - val_acc: 0.7600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6d398c5080>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import GRU, MaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_size, input_length=max_length, \n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(GRU(100, return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 307us/step\n",
      "Loss        : 1.5117\n",
      "Accuracy    : 0.7600\n",
      "Precision   : 0.6796\n",
      "Recall      : 0.8694\n",
      "F1          : 0.7628\n"
     ]
    }
   ],
   "source": [
    "from hate.utils import print_evaluation\n",
    "\n",
    "print_evaluation(model, X_dev, y_dev.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

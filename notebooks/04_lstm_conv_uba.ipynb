{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM con embeddings\n",
    "\n",
    "Modelo básico con los embeddings de fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_es.tsv  train_es.tsv\n"
     ]
    }
   ],
   "source": [
    "%ls ../data/dev_es/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_table(\"../data/dev_es/train_es.tsv\", index_col=\"id\")\n",
    "df_dev = pd.read_table(\"../data/dev_es/dev_es.tsv\", index_col=\"id\")\n",
    "\n",
    "text_train, y_train = df_train[\"text\"], df_train[\"HS\"]\n",
    "text_dev, y_dev = df_dev[\"text\"], df_dev[\"HS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tengo que hacer dos cosas:\n",
    "\n",
    "- Primero, convertir los tweets a secuencias de texto\n",
    "- Luego, paddear las secuencias a cierta longitud (Keras necesita esto para poder paralelizar cálculo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "num_words = 200000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(text_train)\n",
    "X_dev = tokenizer.texts_to_sequences(text_dev)\n",
    "\n",
    "max_length = 30\n",
    "\n",
    "X_train = pad_sequences(X_train, max_length)\n",
    "X_dev = pad_sequences(X_dev, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carguemos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available embeddings:  ['/home/jmperez/WordVectors/UBA_w5_200.vec', '/home/jmperez/WordVectors/wiki.es.vec', '/home/jmperez/WordVectors/UBA_w5_300.vec']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "path_to_embeddings = os.path.expanduser(\"/home/jmperez/WordVectors/\")\n",
    "\n",
    "print(\"Available embeddings: \", glob(os.path.join(path_to_embeddings, \"*.vec\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "\n",
      "Problema con la sig línea:\n",
      "['.', '.', '-0.22232', '0.0052569', '0.47066', '0.13836', '0.15991', '0.19504', '0.00067885', '0.020299']\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "\n",
      "Problema con la sig línea:\n",
      "['.', '...', '-0.11666', '-0.083768', '0.028919', '0.29973', '0.21017', '0.27808', '0.063251', '0.090223']\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "********************************************************************************\n",
      "\n",
      "Problema con la sig línea:\n",
      "['.', '..', '-0.43752', '-0.0016885', '0.1533', '0.28071', '0.18051', '0.28698', '0.11806', '0.044891']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "word_to_vec = {}\n",
    "\n",
    "with open(os.path.join(path_to_embeddings, \"UBA_w5_300.vec\")) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            vec = np.asarray(values[1:], dtype=\"float32\")\n",
    "        except:\n",
    "            print((\"*\" * 80  + \"\\n\")*3)\n",
    "            print(\"Problema con la sig línea:\")\n",
    "            print(values[:10])\n",
    "            word = values[1]\n",
    "            vec = np.asarray(values[2:], dtype=\"float32\")\n",
    "        word_to_vec[word] = vec\n",
    "        \n",
    "embedding_size = len(word_to_vec[\"hola\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((num_words, embedding_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = word_to_vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout, Conv1D, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_size, input_length=max_length, \n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "4469/4469 [==============================] - 8s 2ms/step - loss: 0.6165 - acc: 0.6682 - val_loss: 0.5564 - val_acc: 0.7320\n",
      "Epoch 2/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.5184 - acc: 0.7521 - val_loss: 0.5269 - val_acc: 0.7300\n",
      "Epoch 3/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.4538 - acc: 0.7953 - val_loss: 0.4821 - val_acc: 0.7740\n",
      "Epoch 4/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.4074 - acc: 0.8194 - val_loss: 0.4911 - val_acc: 0.7620\n",
      "Epoch 5/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.3619 - acc: 0.8414 - val_loss: 0.4961 - val_acc: 0.8000\n",
      "Epoch 6/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.3211 - acc: 0.8662 - val_loss: 0.4606 - val_acc: 0.7900\n",
      "Epoch 7/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.2740 - acc: 0.8787 - val_loss: 0.4824 - val_acc: 0.8020\n",
      "Epoch 8/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.2303 - acc: 0.9076 - val_loss: 0.6278 - val_acc: 0.7900\n",
      "Epoch 9/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.2007 - acc: 0.9192 - val_loss: 0.5338 - val_acc: 0.8080\n",
      "Epoch 10/10\n",
      "4469/4469 [==============================] - 7s 2ms/step - loss: 0.1600 - acc: 0.9362 - val_loss: 0.5957 - val_acc: 0.8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f15c079deb8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 616us/step\n",
      "Loss function: 0.596\n",
      "Accuracy: 80.80%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_dev, y_dev)\n",
    "\n",
    "print(\"Loss function: {:.3f}\".format(loss))\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout, Conv1D, Flatten, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_size, input_length=max_length, \n",
    "                    weights=[embedding_matrix], trainable=False))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4469 samples, validate on 500 samples\n",
      "Epoch 1/10\n",
      "4469/4469 [==============================] - 14s 3ms/step - loss: 0.6127 - acc: 0.6729 - val_loss: 0.5643 - val_acc: 0.6920\n",
      "Epoch 2/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.5030 - acc: 0.7568 - val_loss: 0.5316 - val_acc: 0.7280\n",
      "Epoch 3/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.4516 - acc: 0.7892 - val_loss: 0.4571 - val_acc: 0.7840\n",
      "Epoch 4/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.3918 - acc: 0.8230 - val_loss: 0.4489 - val_acc: 0.7840\n",
      "Epoch 5/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.3326 - acc: 0.8575 - val_loss: 0.4987 - val_acc: 0.7920\n",
      "Epoch 6/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.2838 - acc: 0.8823 - val_loss: 0.5279 - val_acc: 0.7740\n",
      "Epoch 7/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.2556 - acc: 0.8962 - val_loss: 0.6069 - val_acc: 0.7660\n",
      "Epoch 8/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.2108 - acc: 0.9163 - val_loss: 0.6495 - val_acc: 0.7720\n",
      "Epoch 9/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.1828 - acc: 0.9277 - val_loss: 0.5443 - val_acc: 0.7820\n",
      "Epoch 10/10\n",
      "4469/4469 [==============================] - 12s 3ms/step - loss: 0.1468 - acc: 0.9425 - val_loss: 0.7756 - val_acc: 0.7860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f14f05c4400>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 1ms/step\n",
      "Loss function: 0.776\n",
      "Accuracy: 78.60%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_dev, y_dev)\n",
    "\n",
    "print(\"Loss function: {:.3f}\".format(loss))\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
